{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A unified platform to manage high-throughput workflows across the HPC landscape. Balsam Documentation (branch: main) Legacy Balsam Documentation (branch: master) Run Balsam on any laptop, cluster, or supercomputer. $ pip install --pre balsam $ balsam login $ balsam site init my-site Python class-based declaration of Apps and execution lifecycles. from balsam.api import ApplicationDefinition class Hello ( ApplicationDefinition ): site = \"my-laptop\" command_template = \"echo hello {{ name }}\" def handle_timeout ( self ): self . job . state = \"RESTART_READY\" Seamless remote job management. # On any machine with internet access... from balsam.api import Job , BatchJob # Create Jobs: job = Job . objects . create ( site_name = \"my-laptop\" , app_id = \"Hello\" , workdir = \"test/say-hello\" , parameters = { \"name\" : \"world!\" }, ) # Or allocate resources: BatchJob . objects . create ( site_id = job . site_id , num_nodes = 1 , wall_time_min = 10 , job_mode = \"serial\" , project = \"local\" , queue = \"local\" , ) Dispatch Python Apps across heterogeneous resources from a single session. import numpy as np class MyApp ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , vec ): from mpi4py import MPI rank = MPI . COMM_WORLD . Get_rank () print ( \"Hello from rank\" , rank ) return np . linalg . norm ( vec ) jobs = [ MyApp . submit ( workdir = f \"test/ { i } \" , vec = np . random . rand ( 3 ), ranks_per_node = 4 , gpus_per_rank = 0 , ) for i in range ( 10 ) ] for job in Job . objects . as_completed ( jobs ): print ( job . workdir , job . result ()) Features \u00b6 Easy pip installation runs out-of-the-box on several HPC systems and is easily adaptable to others . Balsam Sites are remotely controlled by design: submit and monitor workflows from anywhere Run any existing application, with flexible execution environments and job lifecycle hooks High-throughput and fault-tolerant task execution on diverse resources Define data dependencies for any task: Balsam orchestrates the necessary data transfers Elastic queueing : auto-scale resources to the workload size Monitoring APIs : query recent task failures, node utilization, or throughput","title":"Home"},{"location":"#features","text":"Easy pip installation runs out-of-the-box on several HPC systems and is easily adaptable to others . Balsam Sites are remotely controlled by design: submit and monitor workflows from anywhere Run any existing application, with flexible execution environments and job lifecycle hooks High-throughput and fault-tolerant task execution on diverse resources Define data dependencies for any task: Balsam orchestrates the necessary data transfers Elastic queueing : auto-scale resources to the workload size Monitoring APIs : query recent task failures, node utilization, or throughput","title":"Features"},{"location":"LICENSE/","text":"BSD 3-Clause License Copyright (c) 2021, UChicago Argonne LLC All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of UChicago Argonne LLC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"community/publications/","text":"Publications \u00b6 Balsam is a community project developed at Argonne Leadership Computing Facility. If you find it useful for your computational science work, please include the following citation in your publications. M. Salim, T. D. Uram, J.T. Childers, P. Balaprakash, V. Vishwanath, M. Papka. Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive HPC Workflows . In Proceedings of the 8th Workshop on Python for High-Performance and Scientific Computing. ACM Press, 2018. Related Publications \u00b6 R. Vescovi, H. Li, J. Kinnison, M. Keceli, M. Salim, N. Kasthuri, T. D. Uram, N. Ferrier, Toward an Automated HPC Pipeline for Processing Large Scale Electron Microscopy Data, 2020 IEEE/ACM 2nd Annual Workshop on Extreme-scale Experiment-in-the-Loop Computing (XLOOP), 2020, pp. 16-22, doi: 10.1109/XLOOP51963.2020.00008. M. Salim, T. D. Uram, J. T. Childers, V. Vishwanath and M. Papka, Balsam: Near Real-Time Experimental Data Analysis on Supercomputers, 2019 IEEE/ACM 1st Annual Workshop on Large-scale Experiment-in-the-Loop Computing (XLOOP), 2019, pp. 26-31, doi: 10.1109/XLOOP49562.2019.00010. A. Brace, M. Salim, V. Subbiah, H. Ma, M. Emani, A. Trifa, A. R. Clyde, C. Adams, T. D. Uram, H. Yoo, A. Hock, J. Liu, V. Vishwanath, and A. Ramanathan. Stream-AI-MD: streaming AI-driven adaptive molecular simulations for heterogeneous computing platforms . Proceedings of the Platform for Advanced Scientific Computing Conference. Association for Computing Machinery, New York, NY, USA, Article 6, 1\u201313. DOI:https://doi.org/10.1145/3468267.3470578 A. Al-Saadi, D. H. Ahn, Y. Babuji, K. Chard, J. Corbett, M. Hategan, S. Herbein, S. Jha, D. Laney, A. Merzky, T. Munson, M. Salim, M. Titov, M. Turilli, T. D. Uram, J. M. Wozniak, ExaWorks: Workflows for Exascale, 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS), 2021, pp. 50-57, doi: 10.1109/WORKS54523.2021.00012. S. Hudson, J. Larson, J. -L. Navarro and S. M. Wild, libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations, in IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 4, pp. 977-988, 1 April 2022, doi: 10.1109/TPDS.2021.3082815. P. Balaprakash, R. Egele, M. Salim, S. Wild, V. Vishwanath, F. Xia, T. Brettin, and R. Stevens. Scalable reinforcement-learning-based neural architecture search for cancer deep learning research . In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '19). Association for Computing Machinery, New York, NY, USA, Article 37, 1\u201333. DOI:https://doi.org/10.1145/3295500.3356202 P. Balaprakash, M. Salim, T. D. Uram, V. Vishwanath and S. M. Wild, DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks, 2018 IEEE 25th International Conference on High Performance Computing (HiPC), 2018, pp. 42-51, doi: 10.1109/HiPC.2018.00014. M. Kostuk, T. D. Uram, T. Evans, D. M. Orlov, M. E. Papka, and D. Schissel, Automatic Between-Pulse Analysis of DIII-D Experimental Data Performed Remotely on a Supercomputer at Argonne Leadership Computing Facility . United States: N. p., 2018. Web. https://doi.org/10.1080/15361055.2017.1390388. J. T. Childers, T. D. Uram, D. Benjamin, T. J. LeCompte, and M. E. Papka. An Edge Service for Managing HPC Workflows . In Proceedings of the Fourth International Workshop on HPC User Support Tools (HUST'17). Association for Computing Machinery, New York, NY, USA, Article 1, 1\u20138. DOI:https://doi.org/10.1145/3152493.3152557 T. D. Uram, J. T. Childers, T. J. LeCompte, M. E. Papka, D. Benjamin, Achieving production-level use of HEP software at the Argonne Leadership Computing Facility . Journal of Physics: Conference Series. 664. 062063. 10.1088/1742-6596/664/6/062063.","title":"Publications"},{"location":"community/publications/#publications","text":"Balsam is a community project developed at Argonne Leadership Computing Facility. If you find it useful for your computational science work, please include the following citation in your publications. M. Salim, T. D. Uram, J.T. Childers, P. Balaprakash, V. Vishwanath, M. Papka. Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive HPC Workflows . In Proceedings of the 8th Workshop on Python for High-Performance and Scientific Computing. ACM Press, 2018.","title":"Publications"},{"location":"community/publications/#related-publications","text":"R. Vescovi, H. Li, J. Kinnison, M. Keceli, M. Salim, N. Kasthuri, T. D. Uram, N. Ferrier, Toward an Automated HPC Pipeline for Processing Large Scale Electron Microscopy Data, 2020 IEEE/ACM 2nd Annual Workshop on Extreme-scale Experiment-in-the-Loop Computing (XLOOP), 2020, pp. 16-22, doi: 10.1109/XLOOP51963.2020.00008. M. Salim, T. D. Uram, J. T. Childers, V. Vishwanath and M. Papka, Balsam: Near Real-Time Experimental Data Analysis on Supercomputers, 2019 IEEE/ACM 1st Annual Workshop on Large-scale Experiment-in-the-Loop Computing (XLOOP), 2019, pp. 26-31, doi: 10.1109/XLOOP49562.2019.00010. A. Brace, M. Salim, V. Subbiah, H. Ma, M. Emani, A. Trifa, A. R. Clyde, C. Adams, T. D. Uram, H. Yoo, A. Hock, J. Liu, V. Vishwanath, and A. Ramanathan. Stream-AI-MD: streaming AI-driven adaptive molecular simulations for heterogeneous computing platforms . Proceedings of the Platform for Advanced Scientific Computing Conference. Association for Computing Machinery, New York, NY, USA, Article 6, 1\u201313. DOI:https://doi.org/10.1145/3468267.3470578 A. Al-Saadi, D. H. Ahn, Y. Babuji, K. Chard, J. Corbett, M. Hategan, S. Herbein, S. Jha, D. Laney, A. Merzky, T. Munson, M. Salim, M. Titov, M. Turilli, T. D. Uram, J. M. Wozniak, ExaWorks: Workflows for Exascale, 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS), 2021, pp. 50-57, doi: 10.1109/WORKS54523.2021.00012. S. Hudson, J. Larson, J. -L. Navarro and S. M. Wild, libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations, in IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 4, pp. 977-988, 1 April 2022, doi: 10.1109/TPDS.2021.3082815. P. Balaprakash, R. Egele, M. Salim, S. Wild, V. Vishwanath, F. Xia, T. Brettin, and R. Stevens. Scalable reinforcement-learning-based neural architecture search for cancer deep learning research . In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '19). Association for Computing Machinery, New York, NY, USA, Article 37, 1\u201333. DOI:https://doi.org/10.1145/3295500.3356202 P. Balaprakash, M. Salim, T. D. Uram, V. Vishwanath and S. M. Wild, DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks, 2018 IEEE 25th International Conference on High Performance Computing (HiPC), 2018, pp. 42-51, doi: 10.1109/HiPC.2018.00014. M. Kostuk, T. D. Uram, T. Evans, D. M. Orlov, M. E. Papka, and D. Schissel, Automatic Between-Pulse Analysis of DIII-D Experimental Data Performed Remotely on a Supercomputer at Argonne Leadership Computing Facility . United States: N. p., 2018. Web. https://doi.org/10.1080/15361055.2017.1390388. J. T. Childers, T. D. Uram, D. Benjamin, T. J. LeCompte, and M. E. Papka. An Edge Service for Managing HPC Workflows . In Proceedings of the Fourth International Workshop on HPC User Support Tools (HUST'17). Association for Computing Machinery, New York, NY, USA, Article 1, 1\u20138. DOI:https://doi.org/10.1145/3152493.3152557 T. D. Uram, J. T. Childers, T. J. LeCompte, M. E. Papka, D. Benjamin, Achieving production-level use of HEP software at the Argonne Leadership Computing Facility . Journal of Physics: Conference Series. 664. 062063. 10.1088/1742-6596/664/6/062063.","title":"Related Publications"},{"location":"development/client/","text":"About the API Client \u00b6 In transitioning Balsam from a database-driven application to a multi-user, Web client-driven application, we have had to rethink how the Python API should look. Both internal Balsam components and user-written scripts need a way to manipulate and synchronize with the central state. In Balsam 0.x , users leverage direct access to the Django ORM and manipulate the database with simple APIs like: BalsamJob.objects.filter(state=\"FAILED\").delete() . Obviously, direct database access is not acceptable in a multi-user application. However, in cutting off access to the Django ORM, users would lose the familiar API (arguably one of Balsams' most important features) and have to drop down to writing and decoding JSON data for each request. The client architecture described below provides a solution to this problem with a Django ORM-inspired API. A familiar Python object model of the data, complete with models (e.g. Job ), managers ( Job.objects ), and Querysets ( Job.objects.filter(state=\"FAILED\").delete() ) is available. Instead of accessing a database, execution of these \"queries\" results in a REST API call. Internally, a RESTClient interface encapsulates the HTTP request and authentication logic and contains Resource components that map ordinary Python methods to API methods.","title":"The Python REST Client"},{"location":"development/client/#about-the-api-client","text":"In transitioning Balsam from a database-driven application to a multi-user, Web client-driven application, we have had to rethink how the Python API should look. Both internal Balsam components and user-written scripts need a way to manipulate and synchronize with the central state. In Balsam 0.x , users leverage direct access to the Django ORM and manipulate the database with simple APIs like: BalsamJob.objects.filter(state=\"FAILED\").delete() . Obviously, direct database access is not acceptable in a multi-user application. However, in cutting off access to the Django ORM, users would lose the familiar API (arguably one of Balsams' most important features) and have to drop down to writing and decoding JSON data for each request. The client architecture described below provides a solution to this problem with a Django ORM-inspired API. A familiar Python object model of the data, complete with models (e.g. Job ), managers ( Job.objects ), and Querysets ( Job.objects.filter(state=\"FAILED\").delete() ) is available. Instead of accessing a database, execution of these \"queries\" results in a REST API call. Internally, a RESTClient interface encapsulates the HTTP request and authentication logic and contains Resource components that map ordinary Python methods to API methods.","title":"About the API Client"},{"location":"development/contributing/","text":"Contributing to Balsam \u00b6 Installing Balsam for Development \u00b6 For Balsam development and full-stack testing, there are some additional requirements which are not installed with standard pip install -e . Use make install-dev to install Balsam with the necessary dependencies. Direct server dependencies (e.g. FastAPI) are pinned to help with reproducible deployments. git clone https://github.com/argonne-lcf/balsam.git cd balsam # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with pinned deployment and dev dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install Writing code \u00b6 Balsam relies on several tools for consistent code formatting, linting, and type checking. These are automatically run on each git commit when the pre-commit install has been used to install the pre-commit hooks. Otherwise, you should manually perform these actions before pushing new code or making pull requests. Otherwise, the CI is likely to fail: $ make format $ make lint $ make mypy If you have made any changes to the Balsam API schemas (in balsam/schemas/ ) or the REST query parameters (in balsam/server/routers/filters.py ), you must re-generate the client Python library: $ make generate-api This runs the balsam/schemas/api_generator.py to re-generate the balsam/_api/models.py file. You should not edit this file by hand. Testing Locally \u00b6 To run tests locally, be sure that BALSAM_TEST_DB_URL points to an appropriate PostgreSQL testing database (the default value of postgresql://postgres@localhost:5432/balsam-test assumes running a DB named balsam-test on localhost port 5432 as the postgres database user without a password.) The testing commands rely on pytest and can be gleaned from the Makefile. To run all of the tests, simply use: $ make all If you are developing with the Docker container and have a running service, you can simply execute test commands inside the running Balsam web container (named gunicorn by default): $ docker exec -e BALSAM_LOG_DIR = \"/balsam/log\" \\ -e BALSAM_TEST_API_URL = \"http://localhost:8000\" \\ gunicorn make testcov CI Workflows \u00b6 Currently, the following processes run in Github Actions whenever code is pushed to main or on new pull requests: 1. Build mkdocs documentation 2. Run linting, formatting, and mypy type checks 3. Run tests against Python3.7, 3.8, and 3.9 Additionally, when new code is pushed or merged to main , the official Docker container is rebuilt and published on Docker Hub. These Github Actions are defined and maintained within the .github/ directory. Viewing and Writing Documentation \u00b6 To view the docs locally while you edit them, navigate to top-level balsam directory (where mkdocs.yml is located) and run: $ mkdocs serve Follow the link to the documentation. Docs are markdown files in the balsam/docs subdirectory and can be edited on-the-fly. The changes will auto-refresh in the browser window. You can follow mermaid.js examples to create graphs, flowcharts, sequence diagrams, class diagrams, state diagrams, etc... within the Markdown files. For example: graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2] Release checklist \u00b6 Start with code in a state that is passing all CI Tests (green checkmark in Github Actions) Double check make all : all tests passing locally Update the __version__ attribute in balsam/__init__.py Make a new release commit on the main branch with the updated version. Tag the commit: git tag $VERSION Push the release commit: git push Push the tags: git push --tags Update the PyPA build and pip tools: pip install --upgrade build pip twine If it exists, clean up any old distributions: rm -r dist/ Build the latest Balsam distribution: python -m build Check the builds: python -m twine check dist/* and ensure both the .whl and .tar.gz have PASSED Publish to PyPI: python -m twine upload dist/* (This will require having the PyPI credentials stored locally)","title":"Instructions"},{"location":"development/contributing/#contributing-to-balsam","text":"","title":"Contributing to Balsam"},{"location":"development/contributing/#installing-balsam-for-development","text":"For Balsam development and full-stack testing, there are some additional requirements which are not installed with standard pip install -e . Use make install-dev to install Balsam with the necessary dependencies. Direct server dependencies (e.g. FastAPI) are pinned to help with reproducible deployments. git clone https://github.com/argonne-lcf/balsam.git cd balsam # Set up Python3.7+ environment python3.8 -m venv env source env/bin/activate # Install with pinned deployment and dev dependencies: make install-dev # Set up pre-commit linting hooks: pre-commit install","title":"Installing Balsam for Development"},{"location":"development/contributing/#writing-code","text":"Balsam relies on several tools for consistent code formatting, linting, and type checking. These are automatically run on each git commit when the pre-commit install has been used to install the pre-commit hooks. Otherwise, you should manually perform these actions before pushing new code or making pull requests. Otherwise, the CI is likely to fail: $ make format $ make lint $ make mypy If you have made any changes to the Balsam API schemas (in balsam/schemas/ ) or the REST query parameters (in balsam/server/routers/filters.py ), you must re-generate the client Python library: $ make generate-api This runs the balsam/schemas/api_generator.py to re-generate the balsam/_api/models.py file. You should not edit this file by hand.","title":"Writing code"},{"location":"development/contributing/#testing-locally","text":"To run tests locally, be sure that BALSAM_TEST_DB_URL points to an appropriate PostgreSQL testing database (the default value of postgresql://postgres@localhost:5432/balsam-test assumes running a DB named balsam-test on localhost port 5432 as the postgres database user without a password.) The testing commands rely on pytest and can be gleaned from the Makefile. To run all of the tests, simply use: $ make all If you are developing with the Docker container and have a running service, you can simply execute test commands inside the running Balsam web container (named gunicorn by default): $ docker exec -e BALSAM_LOG_DIR = \"/balsam/log\" \\ -e BALSAM_TEST_API_URL = \"http://localhost:8000\" \\ gunicorn make testcov","title":"Testing Locally"},{"location":"development/contributing/#ci-workflows","text":"Currently, the following processes run in Github Actions whenever code is pushed to main or on new pull requests: 1. Build mkdocs documentation 2. Run linting, formatting, and mypy type checks 3. Run tests against Python3.7, 3.8, and 3.9 Additionally, when new code is pushed or merged to main , the official Docker container is rebuilt and published on Docker Hub. These Github Actions are defined and maintained within the .github/ directory.","title":"CI Workflows"},{"location":"development/contributing/#viewing-and-writing-documentation","text":"To view the docs locally while you edit them, navigate to top-level balsam directory (where mkdocs.yml is located) and run: $ mkdocs serve Follow the link to the documentation. Docs are markdown files in the balsam/docs subdirectory and can be edited on-the-fly. The changes will auto-refresh in the browser window. You can follow mermaid.js examples to create graphs, flowcharts, sequence diagrams, class diagrams, state diagrams, etc... within the Markdown files. For example: graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2]","title":"Viewing and Writing Documentation"},{"location":"development/contributing/#release-checklist","text":"Start with code in a state that is passing all CI Tests (green checkmark in Github Actions) Double check make all : all tests passing locally Update the __version__ attribute in balsam/__init__.py Make a new release commit on the main branch with the updated version. Tag the commit: git tag $VERSION Push the release commit: git push Push the tags: git push --tags Update the PyPA build and pip tools: pip install --upgrade build pip twine If it exists, clean up any old distributions: rm -r dist/ Build the latest Balsam distribution: python -m build Check the builds: python -m twine check dist/* and ensure both the .whl and .tar.gz have PASSED Publish to PyPI: python -m twine upload dist/* (This will require having the PyPI credentials stored locally)","title":"Release checklist"},{"location":"development/data-model/","text":"Understanding Balsam \u00b6 Balsam is made up of: A centrally-managed, multi-tenant web application for securely curating HPC applications, authoring workflows, and managing high-throughput job campaigns across one or many computing facilities. Distributed, user-run Balsam Sites that sync with the central API to orchestrate and carry out the workflows defined by users on a given HPC platform. In order to understand how Balsam is organized, one should first consider the server side entities. This graph shows the database schema of the Balsam application. Each node is a table in the database, represented by one of the model classes in the ORM. Each arrow represents a ForeignKey (or many-to-one ) relationship between two tables. The Database Schema \u00b6 A User represents a Balsam user account. All items in the database are linked to a single owner (tenant) , which is reflected in the connectivity of the graph. For example, to get all the jobs belonging to current_user , join the tables via Job.objects.filter(app__site__user=current_user) A Site must have a globally unique name which corresponds to a directory on some machine. One user can own several Balsam sites located across one or several machines. Each site is an independent endpoint where applications are registered, data is transferred in and out, and Job working directories are located. Each Balsam site runs a daemon on behalf of the user that communicates with the central API. If a user has multiple active Balsam Sites, then a separate daemon runs at each of them. The authenticated daemons communicate with the central Balsam API to fetch jobs, orchestrate the workflow locally, and update the database state. An App represents a runnable application at a particular Balsam Site. Every Balsam Site contains an apps/ directory with Python modules containing ApplicationDefinition classes. The set of ApplicationDefinitions determines the applications which may run at the Site. An App instance in the data model is merely a reference to an ApplicationDefinition class, uniquely identified by the Site ID and class path. A Job represents a single run of an App at a particular Site . The Job contains both application-specific data (like command line arguments) and resource requirements (like number of MPI ranks per node) for the run. It is important to note that Job-->App-->Site are non-nullable relations, so a Job is always bound to run at a particular Site from the moment its created . Therefore, the corresponding Balsam service daemon may begin staging-in data as soon as a Job becomes visible, as appropriate. A BatchJob represents a job launch script and resource request submitted by the Site to the local workload manager (e.g. Slurm). Notice that the relation of BatchJob to Site is many-to-one, and that Job to BatchJob is many-to-one. That is, many Jobs run in a single BatchJob , and many BatchJobs are submitted at a Site over time. The Session is an internal model representing an active Balsam launcher session. Jobs have a nullable relationship to Session ; when it is not null, the job is said to be locked by a launcher, and no other launcher should try running it. The Balsam session API is used by launchers acquiring jobs concurrently to avoid race conditions. Sessions contain a heartbeat timestamp that must be periodically ticked to maintain the session. A TransferItem is created for each stage-in or stage-out task associated with a Job . This permits the transfer module of the Balsam service to group transfers according to the remote source or destination, and therefore batch small transfers efficiently. When all the stage-in TransferItems linked to a Job are finished, it is considered \"staged-in\" and moves ahead to preprocessing. A LogEvent contains a timestamp , from_state , to_state , and message for each state transition linked to a Job . The benefit of breaking a Job's state history out into a separate Table is that it becomes easy to query for aggregate throughput, etc... without having to first parse and accumulate timestamps nested inside a Job field. The REST API \u00b6 Refer to the interactive document located under the /docs URL of your Balsam server for detailed information about each endpoint. For instance, launch a local server with docker-compose up and visit localhost:8000/docs . User & a note on Auth \u00b6 Generally, Balsam will need two types of Auth to function: Login auth: This will likely be an pair of views providing an OAuth flow, where Balsam redirects the user to an external auth system, and upon successful authentication, user information is redirected back to a Balsam callback view. For testing purposes, basic password-based login could be used instead. Token auth: After the initial login, Balsam clients need a way to authenticate subsequent requests to the API. This can be performed with Token authentication and a secure setup like Django REST Knox . Upon successful login authentication (step 1), a Token is generated and stored (encrypted) for the User. This token is returned to the client in the login response. The client then stores this token, which has some expiration date, and includes it as a HTTP header on every subsequent request to the API (e.g. Authorization: Token 4789ac8372... ). This is both how Javascript web clients and automated Balsam Site services can communicate with the API. Summary of Endpoints \u00b6 HTTP Method URL Description Example usage GET /sites/ Retrieve the current user's list of sites A user checks their Balsam site statuses on dashboard POST /sites/ Create a new Site balsam init creates a Site and stores new id locally PUT /sites/{id} Update Site information Service daemon syncs backfill_windows periodically DELETE /sites/{id} Delete Site User deletes their Site with balsam rm site ----------- --------------- -------------------- ------------------- GET /apps/ Retrieve the current user's list of Apps balsam ls apps shows Apps across sites POST /apps/ Create a new App balsam app sync creates new Apps from local ApplicationDefinitions PUT /apps/{id} Update App information balsam app sync updates existing Apps with changes from local ApplicationDefinitions DELETE /apps/{id} Delete App User deletes an App ; all related Jobs are deleted ----------- --------------- -------------------- ------------------- GET /jobs/ Get paginated Job lists, filtered by site, state, tags, BatchJob, or App balsam ls POST /jobs/ Bulk-create Jobs Create 1k jobs with single API call PUT /jobs/{id} Update Job information Tweak a single job in web UI DELETE /jobs/{id} Delete Job Delete a single job in web UI PUT /jobs/ Bulk-update Jobs: apply same update to all jobs matching query Restart all jobs at Site X with tag workflow=\"foo\" PATCH /jobs/ Bulk-update Jobs: apply list of patches job-wise Balsam StatusUpdater component sends a list of status updates to API ----------- --------------- -------------------- ------------------- GET /batch-jobs/ Get BatchJobs Web client lists recent BatchJobs POST /batch-jobs/ Create BatchJob Web client or ElasticQueue submits a new BatchJob PUT /batch-jobs/{id} Alter BatchJob by ID Web client alters job runtime while queued DELETE /batch-jobs/{id} Delete BatchJob by ID User deletes job before it was ever submitted PATCH /batch-jobs/ Bulk Update batch jobs by patch list Service syncs BatchJob states ----------- --------------- -------------------- ------------------- GET /sessions Get Sessions List BatchJob Web view shows \"Last Heartbeat\" for each running POST /sessions Create new Session Launcher JobSource initialized POST /sessions/{id}/acquire Acquire Jobs for launcher JobSource acquires new jobs to run PUT /sessions/{id} Tick Session heartbeat JobSource ticks Session periodically DELETE /sessions/{id} Destroy Session and release Jobs Final JobSource release() call ----------- --------------- -------------------- ------------------- GET /transfers/ List TransferItems Transfer module gets list of pending Transfers PUT /transfers/{id} Update TransferItem State Transfer module updates status PATCH /transfers/ Bulk update TransferItems via patch list Transfer module bulk-updates statuses of finished transfers ----------- --------------- -------------------- ------------------- GET /events Fetch EventLogs Web client filters by Job tags and last 24 hours to get a quick view at throughput/utilization for a particular job type Site \u00b6 Field Name Description id Unique Site ID name The unique site name like theta-knl path Absolute POSIX path to the Site directory last_refresh Automatically updated timestamp: last update to Site information creation_date Timestamp when Site was created owner ForeignKey to User model globus_endpoint_id Optional UUID : setting an associated endpoint for data transfer num_nodes Number of compute nodes available at the Site backfill_windows JSONField: array of [queue, num_nodes, wall_time_min] tuples indicating backfill slots queued_jobs JSONField: array of [queue, num_nodes, wall_time_min, state] indicating currently queued and running jobs optional_batch_job_params JSONField used in BatchJob forms/validation {name: default_value} . Taken from site config. allowed_projects JSONField used in BatchJob forms/validation: [ name: str ] allowed_queues JSONField used in BatchJob forms/validation: {name: {max_nodes, max_walltime, max_queued}} transfer_locations JSONField used in Job stage-in/stage-out validation: {alias: {protocol, netloc}} App \u00b6 Field Name Description id Unique App ID site Foreign Key to Site instance containing this App name Short name identifying the app. description Text description (useful in generating Web forms) name Name of ApplicationDefinition class parameters Command line template or function parameters. A dict of dicts with the structure: {name: {required: bool, default: str, help: str}} transfers A dict of stage-in/stage-out slots with the structure: {name: {required: bool, direction: [\"in\"|\"out\"], target_path: str, help: str}} The App model is used to merely index the ApplicationDefinition classes that a user has registered at their Balsam Sites. The parameters field represents \"slots\" for each adjustable command line parameter. For example, an ApplicationDefinition command template of \"echo hello, {{first_name}}!\" would result in an App having the parameters list: [ {name: \"first_name\", required: true, default: \"\", help: \"\"} ] . None of the Balsam site components use App.parameters internally; the purpose of mirroring this field in the database is simply to facilitate Job validation and create App-tailored web forms. Similarly, transfers mirrors data on the ApplicationDefinition for Job input and validation purposes only. For security reasons, the validation of Job input parameters takes place in the site-local ApplicationDefinition module. Even if a malicious user altered the parameters field in the API, they would not be able to successfully run a Job with injected parameters. Job \u00b6 Field Name Description id Unique Job ID workdir Working directory, relative to the Site data/ directory tags JSON {str: str} mappings for tagging and selecting jobs session ForeignKey to Session instance app ForeignKey to App instance parameters JSON {paramName: paramValue} for the App command template parameters batch_job ForeignKey to current or most recent BatchJob instance in which this Job ran state Current state of the Job last_update Timestamp of last modification to Job data Arbitrary JSON data storage return_code Most recent return code of job parents Non-symmetric ManyToMany Parent --> Child relations between Jobs num_nodes Number of compute nodes required (> 1 implies MPI usage) ranks_per_node Number of ranks per node (> 1 implies MPI usage) threads_per_rank Number of logical threads per MPI rank threads_per_core Number of logical threads per hardware core launch_params Optional pass-through parameters to MPI launcher (e.g -cc depth) gpus_per_rank Number of GPUs per MPI rank node_packing_count Maximum number of instances that can run on a single node wall_time_min Lower bound estimate for runtime of the Job (leaving at default 0 is allowed) Let workdir uniqueness be the user's problem. If they put 2 jobs with same workdir, assume it's intentional. We can ensure that \"stdout\" of each job goes into a file named by Job ID, so multiple runs do not collide. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished A user can only access Jobs they own. The related App, BatchJob, and parents are included by ID in the serialized representation. The session is excluded since it is only used internally. Reverse relationships (one-to-many) with transfers and events are also not included in the Job representation, as they can be accessed through separate API endpoints. The related entities are represented in JSON as follows: Field Serialized Deserialized id Primary Key Fetch Job from user-filtered queryset app_id Primary Key Fetch App from user-filtered queryset batch_job_id Primary Key Fetch BatchJob from user-filtered queryset parent_ids Primary Key list Fetch parent jobs from user-filtered queryset transfers N/A Create only: Dict of {transfer_item_name: {location_alias: str, path: str}} events N/A N/A session N/A N/A transfers are nested in the Job for POST only: Job creation is an atomic transaction grouping addition of the Job with its related TransferItems . The API fetches the related App.transfers and Site.transfer_locations to validate each transfer item: transfer_item_name must match one of the keys in App.transfers , which determines the direction and local path The location_alias must match one of the keys in Site.transfer_locations , which determines the protocol and remote_netloc Finally, the remote path is determined by the path key in each Job transfer item BatchJob \u00b6 Field Name Description id Unique ID. Not to be confused with Scheduler ID, which is not necessarily unique across Sites! site ForeignKey to Site where submitted scheduler_id ID assigned by Site's batch scheduler (null if unassigned) project Project/allocation to be charged for the job submission queue Which scheduler queue the batchjob is submitted to num_nodes Number of nodes requested for batchjob wall_time_min Wall time, in minutes, requested job_mode Balsam launcher job mode optional_params Extra pass-through parameters to Job Template filter_tags Restrict launcher to run jobs with matching tags. JSONField dict: {tag_key: tag_val} state Current status of BatchJob status_info JSON: Error or custom data received from scheduler start_time DateTime when BatchJob started running end_time DateTime when BatchJob ended Every workload manager is different and there are numerous job states intentionally not considered in the BatchJob model, including starting , exiting , user_hold , dep_hold , etc. It is the responsibility of the site's Scheduler interface to translate real scheduler states to one of the few coarse-grained Balsam BatchJob states: queued , running , or finished . stateDiagram-v2 pending_submission --> queued pending_submission --> submit_failed queued --> running running --> finished pending_submission --> pending_deletion queued --> pending_deletion running --> pending_deletion pending_deletion --> finished Session \u00b6 Field Name Description id Unique ID heartbeat DateTime of last session tick API call batch_job Non-nullable ForeignKey to BatchJob this Session is running under Session creation only requires providing batch_job_id . Session tick has empty payload Session acquire endpoint uses a special JobAcquireSerializer representation: Field Description states list of states to acquire max_num_acquire limit number of jobs to acquire filter_tags filter Jobs for which job.tags contains all {tag_name: tag_value} pairs node_resources Nested NodeResource representation placing resource constraints on what Jobs may be acquired order_by order returned jobs according to a set of Job fields (may include ascending or descending num_nodes , node_packing_count , wall_time_min ) The nested NodeResource representation is provided as a dict with the structure: { \"max_jobs_per_node\" : 1 , # Determined by Site settings for each Launcher job mode \"max_wall_time_min\" : 60 , \"running_job_counts\" : [ 0 , 1 , 0 ], \"node_occupancies\" : [ 0.0 , 1.0 , 0.0 ], \"idle_cores\" : [ 64 , 63 , 64 ], \"idle_gpus\" : [ 1 , 0 , 1 ], } TransferItem \u00b6 Field Name Description id Unique TransferItem ID job ForeignKey to Job protocol globus or rsync direction in or out . If in , the transfer is from remote_netloc:source_path to Job.workdir/destination_path . If out , the transfer is from Job.workdir/src_path to remote_netloc:dest_path . remote_netloc The Globus endpoint UUID or user@hostname of the remote data location source_path If stage- in : the remote path. If stage- out : the local path destination_path If stage- in : the local path. If stage- out : the remote path. state pending -> active -> done or error task_id Unique identifier of the Transfer task (e.g. Globus Task UUID) transfer_info JSONField for Error messages, average bandwidth, transfer time, etc... There is no create ( POST ) method on the /transfers endpoint, because TransferItem creation is directly linked with Job creation. The related Transfers are nested in the Job representation when POSTING new jobs. The following fields are fixed at creation time: id job protocol direction remote_netloc source_path dest_path For list (GET), the representation includes all fields. job_id represents the Job by primary key. For update (PUT and PATCH), only state , task_id , and transfer_info may be modified. The update of a state to done triggers a check of the related Job 's transfers to determine whether the job can be advanced to STAGED_IN . LogEvent \u00b6 Field Name Description id Unique ID job ForeignKey to Job undergoing event timestamp DateTime of event from_state Job state before transition to_state Job state after transition data JSONField containing {message: str} and other optional data For transitions to or from RUNNING , the data includes nodes as a fractional number of occupied nodes. This enables clients to generate throughput and utilization views without having to fetch entire related Jobs. This is a read only-API with all fields included. The related Job is represented by primary key job_id field.","title":"Understanding the Data Model"},{"location":"development/data-model/#understanding-balsam","text":"Balsam is made up of: A centrally-managed, multi-tenant web application for securely curating HPC applications, authoring workflows, and managing high-throughput job campaigns across one or many computing facilities. Distributed, user-run Balsam Sites that sync with the central API to orchestrate and carry out the workflows defined by users on a given HPC platform. In order to understand how Balsam is organized, one should first consider the server side entities. This graph shows the database schema of the Balsam application. Each node is a table in the database, represented by one of the model classes in the ORM. Each arrow represents a ForeignKey (or many-to-one ) relationship between two tables.","title":"Understanding Balsam"},{"location":"development/data-model/#the-database-schema","text":"A User represents a Balsam user account. All items in the database are linked to a single owner (tenant) , which is reflected in the connectivity of the graph. For example, to get all the jobs belonging to current_user , join the tables via Job.objects.filter(app__site__user=current_user) A Site must have a globally unique name which corresponds to a directory on some machine. One user can own several Balsam sites located across one or several machines. Each site is an independent endpoint where applications are registered, data is transferred in and out, and Job working directories are located. Each Balsam site runs a daemon on behalf of the user that communicates with the central API. If a user has multiple active Balsam Sites, then a separate daemon runs at each of them. The authenticated daemons communicate with the central Balsam API to fetch jobs, orchestrate the workflow locally, and update the database state. An App represents a runnable application at a particular Balsam Site. Every Balsam Site contains an apps/ directory with Python modules containing ApplicationDefinition classes. The set of ApplicationDefinitions determines the applications which may run at the Site. An App instance in the data model is merely a reference to an ApplicationDefinition class, uniquely identified by the Site ID and class path. A Job represents a single run of an App at a particular Site . The Job contains both application-specific data (like command line arguments) and resource requirements (like number of MPI ranks per node) for the run. It is important to note that Job-->App-->Site are non-nullable relations, so a Job is always bound to run at a particular Site from the moment its created . Therefore, the corresponding Balsam service daemon may begin staging-in data as soon as a Job becomes visible, as appropriate. A BatchJob represents a job launch script and resource request submitted by the Site to the local workload manager (e.g. Slurm). Notice that the relation of BatchJob to Site is many-to-one, and that Job to BatchJob is many-to-one. That is, many Jobs run in a single BatchJob , and many BatchJobs are submitted at a Site over time. The Session is an internal model representing an active Balsam launcher session. Jobs have a nullable relationship to Session ; when it is not null, the job is said to be locked by a launcher, and no other launcher should try running it. The Balsam session API is used by launchers acquiring jobs concurrently to avoid race conditions. Sessions contain a heartbeat timestamp that must be periodically ticked to maintain the session. A TransferItem is created for each stage-in or stage-out task associated with a Job . This permits the transfer module of the Balsam service to group transfers according to the remote source or destination, and therefore batch small transfers efficiently. When all the stage-in TransferItems linked to a Job are finished, it is considered \"staged-in\" and moves ahead to preprocessing. A LogEvent contains a timestamp , from_state , to_state , and message for each state transition linked to a Job . The benefit of breaking a Job's state history out into a separate Table is that it becomes easy to query for aggregate throughput, etc... without having to first parse and accumulate timestamps nested inside a Job field.","title":"The Database Schema"},{"location":"development/data-model/#the-rest-api","text":"Refer to the interactive document located under the /docs URL of your Balsam server for detailed information about each endpoint. For instance, launch a local server with docker-compose up and visit localhost:8000/docs .","title":"The REST API"},{"location":"development/data-model/#user-a-note-on-auth","text":"Generally, Balsam will need two types of Auth to function: Login auth: This will likely be an pair of views providing an OAuth flow, where Balsam redirects the user to an external auth system, and upon successful authentication, user information is redirected back to a Balsam callback view. For testing purposes, basic password-based login could be used instead. Token auth: After the initial login, Balsam clients need a way to authenticate subsequent requests to the API. This can be performed with Token authentication and a secure setup like Django REST Knox . Upon successful login authentication (step 1), a Token is generated and stored (encrypted) for the User. This token is returned to the client in the login response. The client then stores this token, which has some expiration date, and includes it as a HTTP header on every subsequent request to the API (e.g. Authorization: Token 4789ac8372... ). This is both how Javascript web clients and automated Balsam Site services can communicate with the API.","title":"User &amp; a note on Auth"},{"location":"development/data-model/#summary-of-endpoints","text":"HTTP Method URL Description Example usage GET /sites/ Retrieve the current user's list of sites A user checks their Balsam site statuses on dashboard POST /sites/ Create a new Site balsam init creates a Site and stores new id locally PUT /sites/{id} Update Site information Service daemon syncs backfill_windows periodically DELETE /sites/{id} Delete Site User deletes their Site with balsam rm site ----------- --------------- -------------------- ------------------- GET /apps/ Retrieve the current user's list of Apps balsam ls apps shows Apps across sites POST /apps/ Create a new App balsam app sync creates new Apps from local ApplicationDefinitions PUT /apps/{id} Update App information balsam app sync updates existing Apps with changes from local ApplicationDefinitions DELETE /apps/{id} Delete App User deletes an App ; all related Jobs are deleted ----------- --------------- -------------------- ------------------- GET /jobs/ Get paginated Job lists, filtered by site, state, tags, BatchJob, or App balsam ls POST /jobs/ Bulk-create Jobs Create 1k jobs with single API call PUT /jobs/{id} Update Job information Tweak a single job in web UI DELETE /jobs/{id} Delete Job Delete a single job in web UI PUT /jobs/ Bulk-update Jobs: apply same update to all jobs matching query Restart all jobs at Site X with tag workflow=\"foo\" PATCH /jobs/ Bulk-update Jobs: apply list of patches job-wise Balsam StatusUpdater component sends a list of status updates to API ----------- --------------- -------------------- ------------------- GET /batch-jobs/ Get BatchJobs Web client lists recent BatchJobs POST /batch-jobs/ Create BatchJob Web client or ElasticQueue submits a new BatchJob PUT /batch-jobs/{id} Alter BatchJob by ID Web client alters job runtime while queued DELETE /batch-jobs/{id} Delete BatchJob by ID User deletes job before it was ever submitted PATCH /batch-jobs/ Bulk Update batch jobs by patch list Service syncs BatchJob states ----------- --------------- -------------------- ------------------- GET /sessions Get Sessions List BatchJob Web view shows \"Last Heartbeat\" for each running POST /sessions Create new Session Launcher JobSource initialized POST /sessions/{id}/acquire Acquire Jobs for launcher JobSource acquires new jobs to run PUT /sessions/{id} Tick Session heartbeat JobSource ticks Session periodically DELETE /sessions/{id} Destroy Session and release Jobs Final JobSource release() call ----------- --------------- -------------------- ------------------- GET /transfers/ List TransferItems Transfer module gets list of pending Transfers PUT /transfers/{id} Update TransferItem State Transfer module updates status PATCH /transfers/ Bulk update TransferItems via patch list Transfer module bulk-updates statuses of finished transfers ----------- --------------- -------------------- ------------------- GET /events Fetch EventLogs Web client filters by Job tags and last 24 hours to get a quick view at throughput/utilization for a particular job type","title":"Summary of Endpoints"},{"location":"development/data-model/#site","text":"Field Name Description id Unique Site ID name The unique site name like theta-knl path Absolute POSIX path to the Site directory last_refresh Automatically updated timestamp: last update to Site information creation_date Timestamp when Site was created owner ForeignKey to User model globus_endpoint_id Optional UUID : setting an associated endpoint for data transfer num_nodes Number of compute nodes available at the Site backfill_windows JSONField: array of [queue, num_nodes, wall_time_min] tuples indicating backfill slots queued_jobs JSONField: array of [queue, num_nodes, wall_time_min, state] indicating currently queued and running jobs optional_batch_job_params JSONField used in BatchJob forms/validation {name: default_value} . Taken from site config. allowed_projects JSONField used in BatchJob forms/validation: [ name: str ] allowed_queues JSONField used in BatchJob forms/validation: {name: {max_nodes, max_walltime, max_queued}} transfer_locations JSONField used in Job stage-in/stage-out validation: {alias: {protocol, netloc}}","title":"Site"},{"location":"development/data-model/#app","text":"Field Name Description id Unique App ID site Foreign Key to Site instance containing this App name Short name identifying the app. description Text description (useful in generating Web forms) name Name of ApplicationDefinition class parameters Command line template or function parameters. A dict of dicts with the structure: {name: {required: bool, default: str, help: str}} transfers A dict of stage-in/stage-out slots with the structure: {name: {required: bool, direction: [\"in\"|\"out\"], target_path: str, help: str}} The App model is used to merely index the ApplicationDefinition classes that a user has registered at their Balsam Sites. The parameters field represents \"slots\" for each adjustable command line parameter. For example, an ApplicationDefinition command template of \"echo hello, {{first_name}}!\" would result in an App having the parameters list: [ {name: \"first_name\", required: true, default: \"\", help: \"\"} ] . None of the Balsam site components use App.parameters internally; the purpose of mirroring this field in the database is simply to facilitate Job validation and create App-tailored web forms. Similarly, transfers mirrors data on the ApplicationDefinition for Job input and validation purposes only. For security reasons, the validation of Job input parameters takes place in the site-local ApplicationDefinition module. Even if a malicious user altered the parameters field in the API, they would not be able to successfully run a Job with injected parameters.","title":"App"},{"location":"development/data-model/#job","text":"Field Name Description id Unique Job ID workdir Working directory, relative to the Site data/ directory tags JSON {str: str} mappings for tagging and selecting jobs session ForeignKey to Session instance app ForeignKey to App instance parameters JSON {paramName: paramValue} for the App command template parameters batch_job ForeignKey to current or most recent BatchJob instance in which this Job ran state Current state of the Job last_update Timestamp of last modification to Job data Arbitrary JSON data storage return_code Most recent return code of job parents Non-symmetric ManyToMany Parent --> Child relations between Jobs num_nodes Number of compute nodes required (> 1 implies MPI usage) ranks_per_node Number of ranks per node (> 1 implies MPI usage) threads_per_rank Number of logical threads per MPI rank threads_per_core Number of logical threads per hardware core launch_params Optional pass-through parameters to MPI launcher (e.g -cc depth) gpus_per_rank Number of GPUs per MPI rank node_packing_count Maximum number of instances that can run on a single node wall_time_min Lower bound estimate for runtime of the Job (leaving at default 0 is allowed) Let workdir uniqueness be the user's problem. If they put 2 jobs with same workdir, assume it's intentional. We can ensure that \"stdout\" of each job goes into a file named by Job ID, so multiple runs do not collide. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished A user can only access Jobs they own. The related App, BatchJob, and parents are included by ID in the serialized representation. The session is excluded since it is only used internally. Reverse relationships (one-to-many) with transfers and events are also not included in the Job representation, as they can be accessed through separate API endpoints. The related entities are represented in JSON as follows: Field Serialized Deserialized id Primary Key Fetch Job from user-filtered queryset app_id Primary Key Fetch App from user-filtered queryset batch_job_id Primary Key Fetch BatchJob from user-filtered queryset parent_ids Primary Key list Fetch parent jobs from user-filtered queryset transfers N/A Create only: Dict of {transfer_item_name: {location_alias: str, path: str}} events N/A N/A session N/A N/A transfers are nested in the Job for POST only: Job creation is an atomic transaction grouping addition of the Job with its related TransferItems . The API fetches the related App.transfers and Site.transfer_locations to validate each transfer item: transfer_item_name must match one of the keys in App.transfers , which determines the direction and local path The location_alias must match one of the keys in Site.transfer_locations , which determines the protocol and remote_netloc Finally, the remote path is determined by the path key in each Job transfer item","title":"Job"},{"location":"development/data-model/#batchjob","text":"Field Name Description id Unique ID. Not to be confused with Scheduler ID, which is not necessarily unique across Sites! site ForeignKey to Site where submitted scheduler_id ID assigned by Site's batch scheduler (null if unassigned) project Project/allocation to be charged for the job submission queue Which scheduler queue the batchjob is submitted to num_nodes Number of nodes requested for batchjob wall_time_min Wall time, in minutes, requested job_mode Balsam launcher job mode optional_params Extra pass-through parameters to Job Template filter_tags Restrict launcher to run jobs with matching tags. JSONField dict: {tag_key: tag_val} state Current status of BatchJob status_info JSON: Error or custom data received from scheduler start_time DateTime when BatchJob started running end_time DateTime when BatchJob ended Every workload manager is different and there are numerous job states intentionally not considered in the BatchJob model, including starting , exiting , user_hold , dep_hold , etc. It is the responsibility of the site's Scheduler interface to translate real scheduler states to one of the few coarse-grained Balsam BatchJob states: queued , running , or finished . stateDiagram-v2 pending_submission --> queued pending_submission --> submit_failed queued --> running running --> finished pending_submission --> pending_deletion queued --> pending_deletion running --> pending_deletion pending_deletion --> finished","title":"BatchJob"},{"location":"development/data-model/#session","text":"Field Name Description id Unique ID heartbeat DateTime of last session tick API call batch_job Non-nullable ForeignKey to BatchJob this Session is running under Session creation only requires providing batch_job_id . Session tick has empty payload Session acquire endpoint uses a special JobAcquireSerializer representation: Field Description states list of states to acquire max_num_acquire limit number of jobs to acquire filter_tags filter Jobs for which job.tags contains all {tag_name: tag_value} pairs node_resources Nested NodeResource representation placing resource constraints on what Jobs may be acquired order_by order returned jobs according to a set of Job fields (may include ascending or descending num_nodes , node_packing_count , wall_time_min ) The nested NodeResource representation is provided as a dict with the structure: { \"max_jobs_per_node\" : 1 , # Determined by Site settings for each Launcher job mode \"max_wall_time_min\" : 60 , \"running_job_counts\" : [ 0 , 1 , 0 ], \"node_occupancies\" : [ 0.0 , 1.0 , 0.0 ], \"idle_cores\" : [ 64 , 63 , 64 ], \"idle_gpus\" : [ 1 , 0 , 1 ], }","title":"Session"},{"location":"development/data-model/#transferitem","text":"Field Name Description id Unique TransferItem ID job ForeignKey to Job protocol globus or rsync direction in or out . If in , the transfer is from remote_netloc:source_path to Job.workdir/destination_path . If out , the transfer is from Job.workdir/src_path to remote_netloc:dest_path . remote_netloc The Globus endpoint UUID or user@hostname of the remote data location source_path If stage- in : the remote path. If stage- out : the local path destination_path If stage- in : the local path. If stage- out : the remote path. state pending -> active -> done or error task_id Unique identifier of the Transfer task (e.g. Globus Task UUID) transfer_info JSONField for Error messages, average bandwidth, transfer time, etc... There is no create ( POST ) method on the /transfers endpoint, because TransferItem creation is directly linked with Job creation. The related Transfers are nested in the Job representation when POSTING new jobs. The following fields are fixed at creation time: id job protocol direction remote_netloc source_path dest_path For list (GET), the representation includes all fields. job_id represents the Job by primary key. For update (PUT and PATCH), only state , task_id , and transfer_info may be modified. The update of a state to done triggers a check of the related Job 's transfers to determine whether the job can be advanced to STAGED_IN .","title":"TransferItem"},{"location":"development/data-model/#logevent","text":"Field Name Description id Unique ID job ForeignKey to Job undergoing event timestamp DateTime of event from_state Job state before transition to_state Job state after transition data JSONField containing {message: str} and other optional data For transitions to or from RUNNING , the data includes nodes as a fractional number of occupied nodes. This enables clients to generate throughput and utilization views without having to fetch entire related Jobs. This is a read only-API with all fields included. The related Job is represented by primary key job_id field.","title":"LogEvent"},{"location":"development/deploy/","text":"Balsam Service Deployment \u00b6 Installation \u00b6 Docker Compose: quick setup (recommended) \u00b6 Docker Compose can be used to manage the PostgreSQL, Redis, and Balsam server containers with a single command: $ cd balsam/ $ git pull $ cp .env.example .env $ vim .env # Configure here $ vim balsam/server/gunicorn.conf.example.py # And here $ docker-compose up --build -d Manual Installation \u00b6 Balsam can be installed into a Python environment in two ways. User-mode installation with pip install --pre balsam-flow or pip install -e . fetches end-user package dependencies with flexible version ranges. This will not suffice for running a Balsam server. To install the server, you must use the second option: pip install -r requirements/deploy.txt This installs all the necessary dependencies with pinned versions for a reproducible deployment environment. Next, you will need to run a PostgreSQL database dedicated to Balsam. If which pg_ctl does not show a Postgres on your system, get the Postgres binaries . You only need to unzip and add the postgres bin/ to your PATH. Follow the PostgreSQL docuemntation to create a new database cluster with initdb , configure and start up the database server, and create an empty database with createdb . The canonical name for the database is balsam but any name can be used. The DSN for Balsam to connect to the database is configured via BALSAM_DATABASE_URL . Refer to the .env.example file and Pydantic documentation for URL parsing . Running Redis is optional and needed only for the Balsam event streaming WebSocket functionality. Inside your virtualenv, run the redis-install.sh script to install Redis (or DIY). This will copy the built Redis binary in your virtualenv bin/ . Run the Redis server and configure the Balsam-Redis connection via the BALSAM_REDIS_PARAMS environment variable, which should be a JSON-formatted string as shown in the .env.example file. Balsam Server Configuration \u00b6 Regardless of installation method, the server can be configured in a few places: Environment variables are read from the .env file in the project root directory. Refer to, copy, and modify the .env.example example file. Gunicorn-specific configuration is contained in the file referenced by GUNICORN_CONFIG_FILE . Refer to the example in balsam/server/gunicorn.conf.example.py Any additional settings listed in balsam/server/conf.py can also be controlled through the .env file or environment variables. The environment variables should be formed by combining the appropriate env_prefix with the setting name. Keeping Balsam and Gunicorn Config Separate Settings internal to the Balsam web server application are defined with Pydantic in balsam/server/conf.py . This includes concerns such as where to find the database, how to perform logging, and how to perform user Authentication. This should not be conflated with outer-level server environment concerns that Balsam itself does not need to know. Examples include which port the server is listening on, or how many copies of the underlying uvicorn web worker are running. These ultimately depend on the deployment method. In the case of Docker Compose with Gunicorn, we break the config into the separate gunicorn.conf.example.py file and load it from within the Dockerfile's entrypoint. Database Migrations \u00b6 Initially, the PostgreSQL database will be empty and have no tables defined. To apply the latest Balsam database schema, you need to run the Alembic migrations: # Make sure Postgres is up and running # Make sure .env has the correct BALSAM_DATABASE_URL $ balsam server migrate # Or when using Docker: $ docker-compose exec gunicorn balsam server migrate Stopping and Starting the Server \u00b6 With Docker Compose, the server and its companion services are started/stopped with the docker-compose subcommands up and down : # Start, detached: $ docker-compose up -d # Stop: $ docker-compose down When running a bare-metal installation, you are responsible for having PostgreSQL (and optionally Redis) started up separately. Then, you may launch the Balsam web application with gunicorn : $ gunicorn -c ./balsam/server/gunicorn.conf.example.py balsam.server.main:app As a quick sanity check that the server is running and reachable, you can try to fetch the FastAPI docs: $ curl localhost:8000/docs Updating the Server Code \u00b6 Run git pull to update the server Python code. Because the source directory is mounted in the container, this can even be used to live-update the server when running with Docker Compose: $ git pull $ docker kill --signal = SIGHUP gunicorn The same applies when running without Docker. Live Update Limitations Restarting gunicorn workers with SIGHUP avoids server downtime, but it will not apply changes to the container environment (i.e. any changes made to docker-compose.yml or .env will not propagate to the workers). This method is only useful for updates to the gunicorn config or Python source code. More generally, when there are changes to the container, Python environment, or configuration, you will want to rebuild the container and restart the service, which entails downtime: $ cd balsam/ $ docker-compose down $ docker-compose build gunicorn $ docker-compose up -d Database Backups \u00b6 The script balsam/deploy/db_snapshot.py can be used with the accompanying service.example file to set up a recurring service for dumping the Postgres database to a file. Copy the Python script to an appropriate location for the service, modify the service.example file accordingly, and follow the instructions at the top of the service.example file. The script uses the basic postgres dump functionality . Backups are performed locally, and should be replicated to a remote system. The easiest way within the CELS GCE environment is to set up a cron job to pull the database backups. 15 * * * * rsync -avz balsam-dev-01.alcf.anl.gov:/home/msalim/db-backups /nfs/gce/projects/balsam/backups/","title":"Admin Guide"},{"location":"development/deploy/#balsam-service-deployment","text":"","title":"Balsam Service Deployment"},{"location":"development/deploy/#installation","text":"","title":"Installation"},{"location":"development/deploy/#docker-compose-quick-setup-recommended","text":"Docker Compose can be used to manage the PostgreSQL, Redis, and Balsam server containers with a single command: $ cd balsam/ $ git pull $ cp .env.example .env $ vim .env # Configure here $ vim balsam/server/gunicorn.conf.example.py # And here $ docker-compose up --build -d","title":"Docker Compose: quick setup (recommended)"},{"location":"development/deploy/#manual-installation","text":"Balsam can be installed into a Python environment in two ways. User-mode installation with pip install --pre balsam-flow or pip install -e . fetches end-user package dependencies with flexible version ranges. This will not suffice for running a Balsam server. To install the server, you must use the second option: pip install -r requirements/deploy.txt This installs all the necessary dependencies with pinned versions for a reproducible deployment environment. Next, you will need to run a PostgreSQL database dedicated to Balsam. If which pg_ctl does not show a Postgres on your system, get the Postgres binaries . You only need to unzip and add the postgres bin/ to your PATH. Follow the PostgreSQL docuemntation to create a new database cluster with initdb , configure and start up the database server, and create an empty database with createdb . The canonical name for the database is balsam but any name can be used. The DSN for Balsam to connect to the database is configured via BALSAM_DATABASE_URL . Refer to the .env.example file and Pydantic documentation for URL parsing . Running Redis is optional and needed only for the Balsam event streaming WebSocket functionality. Inside your virtualenv, run the redis-install.sh script to install Redis (or DIY). This will copy the built Redis binary in your virtualenv bin/ . Run the Redis server and configure the Balsam-Redis connection via the BALSAM_REDIS_PARAMS environment variable, which should be a JSON-formatted string as shown in the .env.example file.","title":"Manual Installation"},{"location":"development/deploy/#balsam-server-configuration","text":"Regardless of installation method, the server can be configured in a few places: Environment variables are read from the .env file in the project root directory. Refer to, copy, and modify the .env.example example file. Gunicorn-specific configuration is contained in the file referenced by GUNICORN_CONFIG_FILE . Refer to the example in balsam/server/gunicorn.conf.example.py Any additional settings listed in balsam/server/conf.py can also be controlled through the .env file or environment variables. The environment variables should be formed by combining the appropriate env_prefix with the setting name. Keeping Balsam and Gunicorn Config Separate Settings internal to the Balsam web server application are defined with Pydantic in balsam/server/conf.py . This includes concerns such as where to find the database, how to perform logging, and how to perform user Authentication. This should not be conflated with outer-level server environment concerns that Balsam itself does not need to know. Examples include which port the server is listening on, or how many copies of the underlying uvicorn web worker are running. These ultimately depend on the deployment method. In the case of Docker Compose with Gunicorn, we break the config into the separate gunicorn.conf.example.py file and load it from within the Dockerfile's entrypoint.","title":"Balsam Server Configuration"},{"location":"development/deploy/#database-migrations","text":"Initially, the PostgreSQL database will be empty and have no tables defined. To apply the latest Balsam database schema, you need to run the Alembic migrations: # Make sure Postgres is up and running # Make sure .env has the correct BALSAM_DATABASE_URL $ balsam server migrate # Or when using Docker: $ docker-compose exec gunicorn balsam server migrate","title":"Database Migrations"},{"location":"development/deploy/#stopping-and-starting-the-server","text":"With Docker Compose, the server and its companion services are started/stopped with the docker-compose subcommands up and down : # Start, detached: $ docker-compose up -d # Stop: $ docker-compose down When running a bare-metal installation, you are responsible for having PostgreSQL (and optionally Redis) started up separately. Then, you may launch the Balsam web application with gunicorn : $ gunicorn -c ./balsam/server/gunicorn.conf.example.py balsam.server.main:app As a quick sanity check that the server is running and reachable, you can try to fetch the FastAPI docs: $ curl localhost:8000/docs","title":"Stopping and Starting the Server"},{"location":"development/deploy/#updating-the-server-code","text":"Run git pull to update the server Python code. Because the source directory is mounted in the container, this can even be used to live-update the server when running with Docker Compose: $ git pull $ docker kill --signal = SIGHUP gunicorn The same applies when running without Docker. Live Update Limitations Restarting gunicorn workers with SIGHUP avoids server downtime, but it will not apply changes to the container environment (i.e. any changes made to docker-compose.yml or .env will not propagate to the workers). This method is only useful for updates to the gunicorn config or Python source code. More generally, when there are changes to the container, Python environment, or configuration, you will want to rebuild the container and restart the service, which entails downtime: $ cd balsam/ $ docker-compose down $ docker-compose build gunicorn $ docker-compose up -d","title":"Updating the Server Code"},{"location":"development/deploy/#database-backups","text":"The script balsam/deploy/db_snapshot.py can be used with the accompanying service.example file to set up a recurring service for dumping the Postgres database to a file. Copy the Python script to an appropriate location for the service, modify the service.example file accordingly, and follow the instructions at the top of the service.example file. The script uses the basic postgres dump functionality . Backups are performed locally, and should be replicated to a remote system. The easiest way within the CELS GCE environment is to set up a cron job to pull the database backups. 15 * * * * rsync -avz balsam-dev-01.alcf.anl.gov:/home/msalim/db-backups /nfs/gce/projects/balsam/backups/","title":"Database Backups"},{"location":"development/layout/","text":"Project Layout \u00b6 Overview \u00b6 This page summarizes Balsam architecture at a high level in terms of the roles that various modules play. Here are the files and folders you'll find right under balsam/ : balsam.schemas : The source of truth on the REST API schema is here. This defines the various resources which are imported by FastAPI code in balsam.server . FastAPI uses this to generate the OpenAPI schema and interactive documentation at /docs . Moreover, the user-facing SDK in balsam/_api/models.py is dynamically-generated from the schemas herein. Running make generate-api (which is invoked during make all ) will re-generate _api/models.py from the schemas. balsam.server the backend REST API server code that is deployed using Gunicorn, alongside PostgreSQL, to host the Balsam web service. Virtually all User or Site-initiated actions via balsam.api will ultimately create an HTTPS request in balsam.client that gets handled by this server. balsam.client : Implementation of the lower-level HTTP clients that are used by the Managers of balsam._api . Auth and retry logic is here. This is what ultimately talks to the server. balsam._api : implementation of a Django ORM-like library for interacting with the Balsam REST API (e.g. look here to understand how Job.objects.filter() is implemented) balsam.api : public re-export of the user-facing resources in _api balsam.analytics : user-facing helper functions to analyze Job statistics balsam.cmdline : The command line interfaces. balsam.config.config : defines the central ClientSettings class, which loads credentials from ~/.balsam/client.yml , as well as the Settings class, which loads Site-local settings from settings.yml . To understand the magic in these classes look at Pydantic Settings management . balsam.config.site_builder : Does the heavy lifting for balsam site init . balsam.config.defaults : The default settings for various HPC platforms that Balsam supports out-of-the-box. Add new defaults here to extend the list of systems that users are prompted to select from balsam site init. balsam.platform : This package defines generic interfaces to compute nodes, MPI app launchers, and schedulers. Concrete subclasses of these implement the platform-specific interfaces (e.g. Theta aprun launcher or Slurm scheduler) balsam.shared_apps : pre-packaged ApplicationDefinitions that users can import, subclass, and deploy to their Sites. balsam.site : The implementation of the Balsam Site Agent and the Launcher pilot jobs that it ultimately dispatches. Thanks to the generic platform interfaces, all the code that does heavy lifting here is totally platform-agnostic : if you find yourself modifying code in here to accomodate new systems, you're doing it wrong! balsam.util : logging, signal handling, datetime parsing, and miscellaneous helpers. balsam.schemas \u00b6 Pydantic is used to define the REST API data structures, (de-)serialize HTTP JSON payloads, and perform validation. The schemas under balsam.schemas are used both by the user-facing balsam._api classes and the backend balsam.server.routers API. Thus when an update to the schema is made, both the client and server-side code inherit the change. The script schemas/api_generator.py is invoked to re-generate the balsam/_api/models.py whenever the schema changes. Thus, users benefit from always-up-to-date docstrings and type annotations across the Balsam SDK, while the implementation is handled by the internal Models, Managers, and Query classes in balsam/_api . balsam.server \u00b6 This is the self-contained codebase for the API server, implemented with FastAPI and SQLAlchemy . We do not expect Balsam users to ever run or touch this code, unless they are interested in standing up their own server. server/conf.py contains the server settings class which loads server settings from the environment using Pydantic. server/main.py imports all of the API routers which define the HTTP endpoints for /auth/ , /jobs , etc... server/auth contains authentication routes and logic. __init__.py::build_auth_router uses the server settings to determine which login methods will be exposed under the API /auth URLs. authorization_code_login.py and device_code_login.py comprise the OAuth2 login capability. db_sessions.py defines the get_admin_session and get_webuser_session functions that manage database connections and connection pooling. The latter can be used to obtain user-level sessions with RLS (row-level security). token.py has the JWT logic which is used on every single request (not just login!) to authenticate the client request prior to invoking the FastAPI route handler. server.main defines the top-level URL routes into views located in balsam.server.routers balsam.server.routers defines the possible API actions. These routes ultimately call into various methods under balsam.server.models.crud , where the business logic is defined. balsam.server.models encapsulates the database and any actions that involve database communication. alembic contains the database migrations crud contains the business logic invoked by the various FastAPI routes tables.py contains the SQLAlchemy model definitions balsam.client \u00b6 This package defines the low-level RESTClient interface used by all the Balsam Python clients. The implementations capture the details of authentication to the Balsam API and performing HTTP requests. balsam._api \u00b6 Whereas the RESTClient provides a lower-level interface (exchanging JSON data over HTTP), the balsam._api defines Django ORM-like Models and Managers to emulate the original Balsam API: from balsam.api import Job finished_jobs = Job . objects . filter ( state = \"JOB_FINISHED\" ) This is the primary user-facing SDK. Under the hood, it uses a RESTClient implementation from the balsam.client subpackage to actually make HTTP requests. For example, the Job model has access via Job.objects to an instance of JobManager which in turn was initialized with an authenticated RESTClient . The Manager is responsible for auto-chunking large requests, lazy-loading queries, handling pagination transparently, and other features inspired by Django ORM that go beyond a typical auto-generated OpenAPI SDK. Rather than emitting SQL, these Models and Managers work together to generate HTTP requests. ApplicationDefinition \u00b6 Users write their own subclasses of ApplicationDefinition (defined in _api/app.py ) to configure the Apps that may run at a particular Balsam Site. Each ApplicationDefinition is serialized and synchronized with the API when users run the sync() method or submit Jobs using the App. balsam.platform \u00b6 The platform subpackage contains all the platform-specific interfaces to various HPC systems. The goal of this architecture is to make porting Balsam to new HPC systems easier: a developer should only have to write minimal interface code under balsam.platform that subclasses and implements well-defined interfaces. Here is a summary of the key Balsam platform interfaces: AppRun \u00b6 This is the application launcher ( mpirun , aprun , jsrun ) interface used by the Balsam launcher (pilot job). It encapsulates the environment, workdir, output streams, compute resource specification (such as MPI ranks and GPUs), and the running process. AppRun implementations may or may not use a subprocess implementation to invoke the run. ComputeNode \u00b6 The Balsam launcher uses this interface to discover available compute resources within a batch job, as well as to enumerate resources (CPU cores, GPUs) on a node and track their occupancy. Scheduler \u00b6 The Balsam Site uses this interface to interact with the local resource manager (e.g. Slurm, Cobalt) to submit new batch jobs, check on job statuses, and inspect other system-wide metrics (e.g. backfill availability). TransferInterface \u00b6 The Balsam Site uses this interface to submit new transfer tasks and poll on their status. A GlobusTransfer interface is implemented for batching Job stage-ins/stage-outs into Globus Transfer tasks. balsam.config \u00b6 A comprehensive configuration is stored in each Balsam Site as settings.yml . This per-site config improves isolation between sites, and enables more flexible configs when multiple sites share a filesystem. The Settings are also described by a Pydantic schema, which is used to validate the YAML file every time it is loaded. The loaded settings are held in a SiteConfig instance that's defined within this subpackage. The SiteConfig is available to all users via: from balsam.api import site_config . This import statement depends on the resolution of the Balsam site path from the local filesystem (i.e. it can only work when cwd is inside of a Balsam site, which is the case wherever a Job is running or pre/post-proccesing.) balsam.site \u00b6 This subpackage contains the real functional core of Balsam: the various components that run on a Site to execute workflows. The Site settings.yml specifies which platform adapters are needed, and these adapters are injected into the Site components, which use them generically. This enables all the code under balsam.site to run across platforms without modification. JobSource \u00b6 Launchers and pre/post-processing modules use this interface to fetch Jobs from the API. The abstraction keeps specific API calls out of the launcher code base, and permits different implementation strategies: FixedDepthJobSource maintains a queue of pre-fetched jobs using a background process SynchronousJobSource performs a blocking API call to fetch jobs according to a specification of available resources. Most importantly, both JobSources use the Balsam Session API to acquire Jobs by performing an HTTP POST /sessions/{session_id} . This prevents race conditions when concurrent launchers acquire Jobs at the same Site, and it ensures that Jobs are not locked in perpetuity if a Session expires. Sessions are ticked with a periodic heartbeat to refresh the lock on long-running jobs. Eventually, Sessions are deleted when the corresponding launcher ends. Details of the job acquisition API are in schemas/sessions.py::SessionAcquire . StatusUpdater \u00b6 The StatusUpdater interface is used to manage job status updates, and also helps to keep API-specific code out of the other Balsam internals. The primary implementation BulkStatusUpdater pools update events that are passed via queue to a background process, and performs bulk API updates to reduce the frequency of API calls. ScriptTemplate \u00b6 The ScriptTemplate is used to generate shell scripts for submission to the local resource manager, using a Site-specific job template file. Launcher \u00b6 The MPI and serial job modes of the Balsam launcher are implemented here. These are standalone, executable Python scripts that carry out the execution of Balsam Jobs (sometimes called a pilot job mechanism). The launchers are invoked from a shell script generated by the ScriptTemplate which is submitted to the local resource manager (via the Scheduler interface). The mpi mode is a simpler implementation that runs a single process on the head node of a batch job which launches each job with the MPI launcher (e.g. aprun , mpiexec ). The serial mode is more involved: a Worker process is first started on each compute node; these workers then receive Jobs and send status updates to a Master process. The advantage of Serial mode is that very large numbers of node-local jobs can be launched without incurring the overhead of a single mpiexec per job. In both mpi and serial modes, the leader process uses a JobSource to acquire jobs and StatusUpdater to send state updates back to the API (see above). Serial mode differs from MPI mode by using ZeroMQ to forward jobs from the leader to the Workers. Moreover, users can pass the --partitions flag to split a single Batch job allocation into multiple leader/worker groups. This allows for scalable launch to hundreds of thousands of simultaneous tasks. For instance on ThetaKNL, one can launch 131,072 simultaneous jobs across 2048 nodes by packing 64 Jobs per node with node_packing_count=64 . The Workers will prefetch Jobs and launch them, thereby hiding the overhead of communication with the leader. To further speed this process, the 2048 node batch job can be split into 16 partitions of 128 nodes each. Thus: 1 central API server fans out to 16 serial mode leaders, each of which fan out to 128 workers, and each worker prefetches Jobs and launches 64 concurrently. Job launch and polling are almost entirely overlapped with communication in this paradigm, and the serial mode leaders use background processes for the JobSource and StatusUpdater , leaving the leader highly available to forward jobs to Workers and route updates back to the StatusUpdater 's queue. balsam.site.service \u00b6 The Balsam Site daemon comprises a group of background processes that run on behalf of the user. The daemon may run on a login node, or on any other resource appropriate for a long-running background process. The only requirements are that: The Site daemon can access the filesystem with the Site directory, and The Site daemon can access the local resource manager (e.g. perform qsub ) The Site daemon can access the Balsam API The Site daemon is organized as a collection of BalsamService classes, each of which describes a particular background process. This setup is highly modular: users can easily configure which service modules are in use, and developers can implement additional services that hook directly into the Site. main \u00b6 The balsam/site/service/main.py is the entry point that ultimately loads settings.yml into a SiteConfig instance, which defines the various services that the Site Agent will run. Each of these services is launched as a background process and monitored here after invoking balsam site start . When terminated (e.g. with balsam site stop ) -- the teardown happens here as well. The following are some of the most common plugins to the Balsam site agent. SchedulerService \u00b6 This BalsamService component syncs with BatchJobs in the Balsam API and uses the Scheduler platform interface to submit new BatchJobs and update the status of existing BatchJobs . It does not automate the process of job submission -- it only serves to keep the API state and local resource manager state synchronized. For example, a user performs balsam queue submit to add a new BatchJob to the REST API. The SchedulerService eventually detects this new BatchJob , generates an appropriate script from the ScriptTemplate and job-template.sh (see above), and submits it to the local Slurm scheduler. ElasticQueueService \u00b6 This BalsamService monitors the backlog of Jobs and locally available compute resources, and it automatically submits new BatchJobs to the API to adapt to realtime workloads. This is a form of automated job submission, which works together with the SchedulerService to fully automate resource allocation and execution. QueueMaintainerService \u00b6 This is another, simpler, form of automated job submission, in which a constant number of fixed-size BatchJobs are maintained at a Site (e.g. keep 5 jobs queued at all times). Intended to get through a long campaign of runs. ProcessingService \u00b6 This service carries out the execution of various workflow steps that are defined on the ApplicationDefinition : preprocess() postprocess() handle_error() handle_timeout() These are meant to be lightweight and IO-bound tasks that run in a process pool on the login node or similar resource. Compute-intensive tasks should be performed in the main body of an App. TransferService \u00b6 This service automates staging in data from remote locations prior to the preprocess() step of a Job, and staging results out to other remote locations after postprocess() . The service batches files and directories that are to be moved between a certain pair of endpoints, and creates batch Transfer tasks via the TransferInterface . balsam.cmdline \u00b6 The command line interfaces to Balsam are written as Python functions decorated with Click","title":"Project Layout"},{"location":"development/layout/#project-layout","text":"","title":"Project Layout"},{"location":"development/layout/#overview","text":"This page summarizes Balsam architecture at a high level in terms of the roles that various modules play. Here are the files and folders you'll find right under balsam/ : balsam.schemas : The source of truth on the REST API schema is here. This defines the various resources which are imported by FastAPI code in balsam.server . FastAPI uses this to generate the OpenAPI schema and interactive documentation at /docs . Moreover, the user-facing SDK in balsam/_api/models.py is dynamically-generated from the schemas herein. Running make generate-api (which is invoked during make all ) will re-generate _api/models.py from the schemas. balsam.server the backend REST API server code that is deployed using Gunicorn, alongside PostgreSQL, to host the Balsam web service. Virtually all User or Site-initiated actions via balsam.api will ultimately create an HTTPS request in balsam.client that gets handled by this server. balsam.client : Implementation of the lower-level HTTP clients that are used by the Managers of balsam._api . Auth and retry logic is here. This is what ultimately talks to the server. balsam._api : implementation of a Django ORM-like library for interacting with the Balsam REST API (e.g. look here to understand how Job.objects.filter() is implemented) balsam.api : public re-export of the user-facing resources in _api balsam.analytics : user-facing helper functions to analyze Job statistics balsam.cmdline : The command line interfaces. balsam.config.config : defines the central ClientSettings class, which loads credentials from ~/.balsam/client.yml , as well as the Settings class, which loads Site-local settings from settings.yml . To understand the magic in these classes look at Pydantic Settings management . balsam.config.site_builder : Does the heavy lifting for balsam site init . balsam.config.defaults : The default settings for various HPC platforms that Balsam supports out-of-the-box. Add new defaults here to extend the list of systems that users are prompted to select from balsam site init. balsam.platform : This package defines generic interfaces to compute nodes, MPI app launchers, and schedulers. Concrete subclasses of these implement the platform-specific interfaces (e.g. Theta aprun launcher or Slurm scheduler) balsam.shared_apps : pre-packaged ApplicationDefinitions that users can import, subclass, and deploy to their Sites. balsam.site : The implementation of the Balsam Site Agent and the Launcher pilot jobs that it ultimately dispatches. Thanks to the generic platform interfaces, all the code that does heavy lifting here is totally platform-agnostic : if you find yourself modifying code in here to accomodate new systems, you're doing it wrong! balsam.util : logging, signal handling, datetime parsing, and miscellaneous helpers.","title":"Overview"},{"location":"development/layout/#balsamschemas","text":"Pydantic is used to define the REST API data structures, (de-)serialize HTTP JSON payloads, and perform validation. The schemas under balsam.schemas are used both by the user-facing balsam._api classes and the backend balsam.server.routers API. Thus when an update to the schema is made, both the client and server-side code inherit the change. The script schemas/api_generator.py is invoked to re-generate the balsam/_api/models.py whenever the schema changes. Thus, users benefit from always-up-to-date docstrings and type annotations across the Balsam SDK, while the implementation is handled by the internal Models, Managers, and Query classes in balsam/_api .","title":"balsam.schemas"},{"location":"development/layout/#balsamserver","text":"This is the self-contained codebase for the API server, implemented with FastAPI and SQLAlchemy . We do not expect Balsam users to ever run or touch this code, unless they are interested in standing up their own server. server/conf.py contains the server settings class which loads server settings from the environment using Pydantic. server/main.py imports all of the API routers which define the HTTP endpoints for /auth/ , /jobs , etc... server/auth contains authentication routes and logic. __init__.py::build_auth_router uses the server settings to determine which login methods will be exposed under the API /auth URLs. authorization_code_login.py and device_code_login.py comprise the OAuth2 login capability. db_sessions.py defines the get_admin_session and get_webuser_session functions that manage database connections and connection pooling. The latter can be used to obtain user-level sessions with RLS (row-level security). token.py has the JWT logic which is used on every single request (not just login!) to authenticate the client request prior to invoking the FastAPI route handler. server.main defines the top-level URL routes into views located in balsam.server.routers balsam.server.routers defines the possible API actions. These routes ultimately call into various methods under balsam.server.models.crud , where the business logic is defined. balsam.server.models encapsulates the database and any actions that involve database communication. alembic contains the database migrations crud contains the business logic invoked by the various FastAPI routes tables.py contains the SQLAlchemy model definitions","title":"balsam.server"},{"location":"development/layout/#balsamclient","text":"This package defines the low-level RESTClient interface used by all the Balsam Python clients. The implementations capture the details of authentication to the Balsam API and performing HTTP requests.","title":"balsam.client"},{"location":"development/layout/#balsam_api","text":"Whereas the RESTClient provides a lower-level interface (exchanging JSON data over HTTP), the balsam._api defines Django ORM-like Models and Managers to emulate the original Balsam API: from balsam.api import Job finished_jobs = Job . objects . filter ( state = \"JOB_FINISHED\" ) This is the primary user-facing SDK. Under the hood, it uses a RESTClient implementation from the balsam.client subpackage to actually make HTTP requests. For example, the Job model has access via Job.objects to an instance of JobManager which in turn was initialized with an authenticated RESTClient . The Manager is responsible for auto-chunking large requests, lazy-loading queries, handling pagination transparently, and other features inspired by Django ORM that go beyond a typical auto-generated OpenAPI SDK. Rather than emitting SQL, these Models and Managers work together to generate HTTP requests.","title":"balsam._api"},{"location":"development/layout/#applicationdefinition","text":"Users write their own subclasses of ApplicationDefinition (defined in _api/app.py ) to configure the Apps that may run at a particular Balsam Site. Each ApplicationDefinition is serialized and synchronized with the API when users run the sync() method or submit Jobs using the App.","title":"ApplicationDefinition"},{"location":"development/layout/#balsamplatform","text":"The platform subpackage contains all the platform-specific interfaces to various HPC systems. The goal of this architecture is to make porting Balsam to new HPC systems easier: a developer should only have to write minimal interface code under balsam.platform that subclasses and implements well-defined interfaces. Here is a summary of the key Balsam platform interfaces:","title":"balsam.platform"},{"location":"development/layout/#apprun","text":"This is the application launcher ( mpirun , aprun , jsrun ) interface used by the Balsam launcher (pilot job). It encapsulates the environment, workdir, output streams, compute resource specification (such as MPI ranks and GPUs), and the running process. AppRun implementations may or may not use a subprocess implementation to invoke the run.","title":"AppRun"},{"location":"development/layout/#computenode","text":"The Balsam launcher uses this interface to discover available compute resources within a batch job, as well as to enumerate resources (CPU cores, GPUs) on a node and track their occupancy.","title":"ComputeNode"},{"location":"development/layout/#scheduler","text":"The Balsam Site uses this interface to interact with the local resource manager (e.g. Slurm, Cobalt) to submit new batch jobs, check on job statuses, and inspect other system-wide metrics (e.g. backfill availability).","title":"Scheduler"},{"location":"development/layout/#transferinterface","text":"The Balsam Site uses this interface to submit new transfer tasks and poll on their status. A GlobusTransfer interface is implemented for batching Job stage-ins/stage-outs into Globus Transfer tasks.","title":"TransferInterface"},{"location":"development/layout/#balsamconfig","text":"A comprehensive configuration is stored in each Balsam Site as settings.yml . This per-site config improves isolation between sites, and enables more flexible configs when multiple sites share a filesystem. The Settings are also described by a Pydantic schema, which is used to validate the YAML file every time it is loaded. The loaded settings are held in a SiteConfig instance that's defined within this subpackage. The SiteConfig is available to all users via: from balsam.api import site_config . This import statement depends on the resolution of the Balsam site path from the local filesystem (i.e. it can only work when cwd is inside of a Balsam site, which is the case wherever a Job is running or pre/post-proccesing.)","title":"balsam.config"},{"location":"development/layout/#balsamsite","text":"This subpackage contains the real functional core of Balsam: the various components that run on a Site to execute workflows. The Site settings.yml specifies which platform adapters are needed, and these adapters are injected into the Site components, which use them generically. This enables all the code under balsam.site to run across platforms without modification.","title":"balsam.site"},{"location":"development/layout/#jobsource","text":"Launchers and pre/post-processing modules use this interface to fetch Jobs from the API. The abstraction keeps specific API calls out of the launcher code base, and permits different implementation strategies: FixedDepthJobSource maintains a queue of pre-fetched jobs using a background process SynchronousJobSource performs a blocking API call to fetch jobs according to a specification of available resources. Most importantly, both JobSources use the Balsam Session API to acquire Jobs by performing an HTTP POST /sessions/{session_id} . This prevents race conditions when concurrent launchers acquire Jobs at the same Site, and it ensures that Jobs are not locked in perpetuity if a Session expires. Sessions are ticked with a periodic heartbeat to refresh the lock on long-running jobs. Eventually, Sessions are deleted when the corresponding launcher ends. Details of the job acquisition API are in schemas/sessions.py::SessionAcquire .","title":"JobSource"},{"location":"development/layout/#statusupdater","text":"The StatusUpdater interface is used to manage job status updates, and also helps to keep API-specific code out of the other Balsam internals. The primary implementation BulkStatusUpdater pools update events that are passed via queue to a background process, and performs bulk API updates to reduce the frequency of API calls.","title":"StatusUpdater"},{"location":"development/layout/#scripttemplate","text":"The ScriptTemplate is used to generate shell scripts for submission to the local resource manager, using a Site-specific job template file.","title":"ScriptTemplate"},{"location":"development/layout/#launcher","text":"The MPI and serial job modes of the Balsam launcher are implemented here. These are standalone, executable Python scripts that carry out the execution of Balsam Jobs (sometimes called a pilot job mechanism). The launchers are invoked from a shell script generated by the ScriptTemplate which is submitted to the local resource manager (via the Scheduler interface). The mpi mode is a simpler implementation that runs a single process on the head node of a batch job which launches each job with the MPI launcher (e.g. aprun , mpiexec ). The serial mode is more involved: a Worker process is first started on each compute node; these workers then receive Jobs and send status updates to a Master process. The advantage of Serial mode is that very large numbers of node-local jobs can be launched without incurring the overhead of a single mpiexec per job. In both mpi and serial modes, the leader process uses a JobSource to acquire jobs and StatusUpdater to send state updates back to the API (see above). Serial mode differs from MPI mode by using ZeroMQ to forward jobs from the leader to the Workers. Moreover, users can pass the --partitions flag to split a single Batch job allocation into multiple leader/worker groups. This allows for scalable launch to hundreds of thousands of simultaneous tasks. For instance on ThetaKNL, one can launch 131,072 simultaneous jobs across 2048 nodes by packing 64 Jobs per node with node_packing_count=64 . The Workers will prefetch Jobs and launch them, thereby hiding the overhead of communication with the leader. To further speed this process, the 2048 node batch job can be split into 16 partitions of 128 nodes each. Thus: 1 central API server fans out to 16 serial mode leaders, each of which fan out to 128 workers, and each worker prefetches Jobs and launches 64 concurrently. Job launch and polling are almost entirely overlapped with communication in this paradigm, and the serial mode leaders use background processes for the JobSource and StatusUpdater , leaving the leader highly available to forward jobs to Workers and route updates back to the StatusUpdater 's queue.","title":"Launcher"},{"location":"development/layout/#balsamsiteservice","text":"The Balsam Site daemon comprises a group of background processes that run on behalf of the user. The daemon may run on a login node, or on any other resource appropriate for a long-running background process. The only requirements are that: The Site daemon can access the filesystem with the Site directory, and The Site daemon can access the local resource manager (e.g. perform qsub ) The Site daemon can access the Balsam API The Site daemon is organized as a collection of BalsamService classes, each of which describes a particular background process. This setup is highly modular: users can easily configure which service modules are in use, and developers can implement additional services that hook directly into the Site.","title":"balsam.site.service"},{"location":"development/layout/#main","text":"The balsam/site/service/main.py is the entry point that ultimately loads settings.yml into a SiteConfig instance, which defines the various services that the Site Agent will run. Each of these services is launched as a background process and monitored here after invoking balsam site start . When terminated (e.g. with balsam site stop ) -- the teardown happens here as well. The following are some of the most common plugins to the Balsam site agent.","title":"main"},{"location":"development/layout/#schedulerservice","text":"This BalsamService component syncs with BatchJobs in the Balsam API and uses the Scheduler platform interface to submit new BatchJobs and update the status of existing BatchJobs . It does not automate the process of job submission -- it only serves to keep the API state and local resource manager state synchronized. For example, a user performs balsam queue submit to add a new BatchJob to the REST API. The SchedulerService eventually detects this new BatchJob , generates an appropriate script from the ScriptTemplate and job-template.sh (see above), and submits it to the local Slurm scheduler.","title":"SchedulerService"},{"location":"development/layout/#elasticqueueservice","text":"This BalsamService monitors the backlog of Jobs and locally available compute resources, and it automatically submits new BatchJobs to the API to adapt to realtime workloads. This is a form of automated job submission, which works together with the SchedulerService to fully automate resource allocation and execution.","title":"ElasticQueueService"},{"location":"development/layout/#queuemaintainerservice","text":"This is another, simpler, form of automated job submission, in which a constant number of fixed-size BatchJobs are maintained at a Site (e.g. keep 5 jobs queued at all times). Intended to get through a long campaign of runs.","title":"QueueMaintainerService"},{"location":"development/layout/#processingservice","text":"This service carries out the execution of various workflow steps that are defined on the ApplicationDefinition : preprocess() postprocess() handle_error() handle_timeout() These are meant to be lightweight and IO-bound tasks that run in a process pool on the login node or similar resource. Compute-intensive tasks should be performed in the main body of an App.","title":"ProcessingService"},{"location":"development/layout/#transferservice","text":"This service automates staging in data from remote locations prior to the preprocess() step of a Job, and staging results out to other remote locations after postprocess() . The service batches files and directories that are to be moved between a certain pair of endpoints, and creates batch Transfer tasks via the TransferInterface .","title":"TransferService"},{"location":"development/layout/#balsamcmdline","text":"The command line interfaces to Balsam are written as Python functions decorated with Click","title":"balsam.cmdline"},{"location":"development/porting/","text":"Porting Balsam to new HPC Sites \u00b6 Porting Balsam to a new system requires minimal (or no) code. We simply need to provide an off-the-shelf default configuration that users of the system can bootstrap new Sites from. Select the Platform Interfaces \u00b6 To port Balsam to a new system, one only needs to select three compatible interfaces: AppRun : The MPI application launcher class ComputeNode : The node resource definition class SchedulerInterface : The HPC resource manager (batch scheduler) class Several interfaces are implemented in the respective platform directories: platform/app_run , platform/compute_node , and platform/scheduler . If the interface to your system is missing, simply add a new subclass that copies the structure of an existing, related implementation. In most cases, the necessary changes are minimal. New interfaces should be included in the appropriate __init__.py for uniform accessibility. Create a Default Configuration \u00b6 Create a new configuration folder for your platform under balsam/config/defaults/ . Inside, you will need to add the following: apps/__init__.py (and other default apps therein) settings.yml (Referencing the platform interfaces added above) job-template.sh Again, the easiest way is to copy an example from one of the existing folders in balsam/config/defaults/ . The job-template.sh is used to generate the shell scripts that are ultimately submitted to the HPC scheduler. This is where any necessary scheduler flags or module load statements can be added.","title":"Porting to New Sites"},{"location":"development/porting/#porting-balsam-to-new-hpc-sites","text":"Porting Balsam to a new system requires minimal (or no) code. We simply need to provide an off-the-shelf default configuration that users of the system can bootstrap new Sites from.","title":"Porting Balsam to new HPC Sites"},{"location":"development/porting/#select-the-platform-interfaces","text":"To port Balsam to a new system, one only needs to select three compatible interfaces: AppRun : The MPI application launcher class ComputeNode : The node resource definition class SchedulerInterface : The HPC resource manager (batch scheduler) class Several interfaces are implemented in the respective platform directories: platform/app_run , platform/compute_node , and platform/scheduler . If the interface to your system is missing, simply add a new subclass that copies the structure of an existing, related implementation. In most cases, the necessary changes are minimal. New interfaces should be included in the appropriate __init__.py for uniform accessibility.","title":"Select the Platform Interfaces"},{"location":"development/porting/#create-a-default-configuration","text":"Create a new configuration folder for your platform under balsam/config/defaults/ . Inside, you will need to add the following: apps/__init__.py (and other default apps therein) settings.yml (Referencing the platform interfaces added above) job-template.sh Again, the easiest way is to copy an example from one of the existing folders in balsam/config/defaults/ . The job-template.sh is used to generate the shell scripts that are ultimately submitted to the HPC scheduler. This is where any necessary scheduler flags or module load statements can be added.","title":"Create a Default Configuration"},{"location":"development/testing/","text":"Integration Testing Balsam on new HPC platforms \u00b6 Registering a new test platform \u00b6 In tests/test_platform.py : Add a supported platform string to the PLATFORMS set: PLATFORMS : Set [ str ] = { \"alcf_theta\" , \"alcf_thetagpu\" , \"alcf_cooley\" , \"generic\" } tests/test_platform.py also references some environment variables that any test runner (CI or manual) needs to set: BALSAM_TEST_DIR : ephemeral test site will be created in this directory; should be somewhere readable from both the test machine and the compute/launch nodes. BALSAM_LOG_DIR : artifacts of the run saved here BALSAM_TEST_API_URL : http:// URL of the server to test against BALSAM_TEST_PLATFORM : the PLATFORM value such as \"alcf_theta\" BALSAM_TEST_DB_URL is ignored if you are testing against an existing API server with BALSAM_TEST_API_URL . Also important in the test_platform.py is the dictionary LAUNCHER_STARTUP_TIMEOUT_SECONDS describing how long the test runner should wait for a launcher to start. This is highly platform dependent and should be set based on the expected queueing time in the test queue. Test Site configuration \u00b6 Next in balsam/tests/default-configs/ there is a default Site config directory for each value of PLATFORMS . You will need to add a config directory with a name matching the platform string added to PLATFORMS . This is basically just a clone of the user-facing default Site config located under: balsam/config/defaults/ . However, the Site should be configured with any Apps or resources needed by the platform tests. For instance, the integration tests specifically require the hello.Hello to exist in every Site. This App can be copied from the balsam/tests/default-configs/generic/apps/hello.py . You may define additional platform specific Apps here. Moreover, the tests run under the first project and queue as defined in settings.yml . Running Tests \u00b6 You should have installed the development dependencies into a virtualenv with make install-dev . To run the integration tests from a login node: cd into the balsam root directory, set the environment variables above, and run make test-site-integ . Writing platform-specific tests \u00b6 The actual tests are detected from files named tests/site_integration/test_*.py . A generic test test_multi_job runs 3 hello.Hello jobs and waits for them to reach JOB_FINISHED, repeating the test with both serial and mpi job modes. By default, new test cases will run on every platform . To define a platform-specific test, simply use the pytest.mark decorator. This indicates that a test case should only run when the BALSAM_TEST_PLATFORM environment variable matches the platform marker name: @pytest . mark . alcf_theta def test_ATLAS_workflow () -> None : assert 1 Using the test fixtures \u00b6 The PyTest fixtures (defined in conftest.py files and the various test files) are responsible for all the test setup and teardown. Some useful fixtures include: balsam_site_config : creates an ephemeral Site and returns the SiteConfig object client returns a Balsam API client authenticated as the test Site owner live_launcher is a fixture parameterized in the job mode, meaning any test using it will be automatically repeated twice with both serial and mpi job modes running on a single node. The fixture blocks until the launcher is actually up and running (this is where the LAUNCHER_STARTUP_TIMEOUT_SECONDS environment variable comes into play.) For instance, the following snippet applies the live_launcher fixture to all test cases in the TestSingleNodeLaunchers class: @pytest . mark . usefixtures ( \"live_launcher\" ) class TestSingleNodeLaunchers : @pytest . mark . parametrize ( \"num_jobs\" , [ 3 ]) def test_multi_job ( self , balsam_site_config : SiteConfig , num_jobs : int , client : RESTClient ) -> None : \"\"\" 3 hello world jobs run to completion \"\"\" ...","title":"Integration Testing"},{"location":"development/testing/#integration-testing-balsam-on-new-hpc-platforms","text":"","title":"Integration Testing Balsam on new HPC platforms"},{"location":"development/testing/#registering-a-new-test-platform","text":"In tests/test_platform.py : Add a supported platform string to the PLATFORMS set: PLATFORMS : Set [ str ] = { \"alcf_theta\" , \"alcf_thetagpu\" , \"alcf_cooley\" , \"generic\" } tests/test_platform.py also references some environment variables that any test runner (CI or manual) needs to set: BALSAM_TEST_DIR : ephemeral test site will be created in this directory; should be somewhere readable from both the test machine and the compute/launch nodes. BALSAM_LOG_DIR : artifacts of the run saved here BALSAM_TEST_API_URL : http:// URL of the server to test against BALSAM_TEST_PLATFORM : the PLATFORM value such as \"alcf_theta\" BALSAM_TEST_DB_URL is ignored if you are testing against an existing API server with BALSAM_TEST_API_URL . Also important in the test_platform.py is the dictionary LAUNCHER_STARTUP_TIMEOUT_SECONDS describing how long the test runner should wait for a launcher to start. This is highly platform dependent and should be set based on the expected queueing time in the test queue.","title":"Registering a new test platform"},{"location":"development/testing/#test-site-configuration","text":"Next in balsam/tests/default-configs/ there is a default Site config directory for each value of PLATFORMS . You will need to add a config directory with a name matching the platform string added to PLATFORMS . This is basically just a clone of the user-facing default Site config located under: balsam/config/defaults/ . However, the Site should be configured with any Apps or resources needed by the platform tests. For instance, the integration tests specifically require the hello.Hello to exist in every Site. This App can be copied from the balsam/tests/default-configs/generic/apps/hello.py . You may define additional platform specific Apps here. Moreover, the tests run under the first project and queue as defined in settings.yml .","title":"Test Site configuration"},{"location":"development/testing/#running-tests","text":"You should have installed the development dependencies into a virtualenv with make install-dev . To run the integration tests from a login node: cd into the balsam root directory, set the environment variables above, and run make test-site-integ .","title":"Running Tests"},{"location":"development/testing/#writing-platform-specific-tests","text":"The actual tests are detected from files named tests/site_integration/test_*.py . A generic test test_multi_job runs 3 hello.Hello jobs and waits for them to reach JOB_FINISHED, repeating the test with both serial and mpi job modes. By default, new test cases will run on every platform . To define a platform-specific test, simply use the pytest.mark decorator. This indicates that a test case should only run when the BALSAM_TEST_PLATFORM environment variable matches the platform marker name: @pytest . mark . alcf_theta def test_ATLAS_workflow () -> None : assert 1","title":"Writing platform-specific tests"},{"location":"development/testing/#using-the-test-fixtures","text":"The PyTest fixtures (defined in conftest.py files and the various test files) are responsible for all the test setup and teardown. Some useful fixtures include: balsam_site_config : creates an ephemeral Site and returns the SiteConfig object client returns a Balsam API client authenticated as the test Site owner live_launcher is a fixture parameterized in the job mode, meaning any test using it will be automatically repeated twice with both serial and mpi job modes running on a single node. The fixture blocks until the launcher is actually up and running (this is where the LAUNCHER_STARTUP_TIMEOUT_SECONDS environment variable comes into play.) For instance, the following snippet applies the live_launcher fixture to all test cases in the TestSingleNodeLaunchers class: @pytest . mark . usefixtures ( \"live_launcher\" ) class TestSingleNodeLaunchers : @pytest . mark . parametrize ( \"num_jobs\" , [ 3 ]) def test_multi_job ( self , balsam_site_config : SiteConfig , num_jobs : int , client : RESTClient ) -> None : \"\"\" 3 hello world jobs run to completion \"\"\" ...","title":"Using the test fixtures"},{"location":"tutorials/theta-quickstart/","text":"Getting started \u00b6 This tutorial gets you up and running with a new Balsam Site quickly. Since Balsam is highly platform-agnostic, you can follow along by choosing from any of the available default site setups: A local MacOS or Linux system Polaris Theta-GPU Theta-KNL Cooley Cori (Haswell or KNL partitions) Perlmutter Summit Install \u00b6 First create a new virtualenv and install Balsam: $ /soft/datascience/create_env.sh my-env # Or DIY $ source my-env/bin/activate $ pip install --pre balsam Log In \u00b6 Now that balsam is installed, you need to log in. Logging in fetches an access token that is used to identify you in all future API interactions, until the token expires and you have to log in again. $ balsam login # Follow the prompt to authenticate Once you are logged in, you can create a Balsam Site for job execution, or send jobs to any of your other existing Sites. Login temporarily restricted Balsam is currently in a pre-release stage and the web service is hosted on limited resources. Consequently, logins are limited to pre-authorized users. Please contact the ALCF Help Desk to request early access membership to the Balsam user group. Create a Balsam Site \u00b6 All Balsam workflows are namespaced under Sites : self-contained project spaces belonging to individual users. You can use Balsam to manage Sites on multiple HPC systems from a single shell or Python program. This is one of the key strengths of Balsam: the usage looks exactly the same whether you're running locally or managing jobs across multiple supercomputers. Let's start by creating a Balsam Site in a folder named ./my-site : $ balsam site init ./my-site # Select the default configuration for Theta-KNL You will be prompted to select a default Site configuration and to enter a unique name for your Site. The directory my-site/ will then be created and preconfigured for the chosen platform. You can list your Sites with the balsam site ls command. $ balsam site ls ID Name Path Active 21 theta-demo .../projects/datascience/my-site No In order to actually run anything at the Site, you have to enter the Site directory and start it with balsam site start . This command launches a persistent background agent which uses your access token to sync with the Balsam service. $ cd my-site $ balsam site start Set up your Apps \u00b6 Every Site has its own collection of Balsam Apps , which define the runnable applications at that Site. A Balsam App is declared by writing an ApplicationDefinition class and running the sync() method from a Python program or interactive session. The simplest ApplicationDefinition is just a template for a bash command, with any workflow variables enclosed in double-curly braces: from balsam.api import ApplicationDefinition class Hello ( ApplicationDefinition ): site = \"theta-demo\" command_template = \"echo Hello, {{ say_hello_to }}!\" Hello . sync () Notice the attribute site = \"theta-demo\" which is required to associate the App \"Hello\" to the Site \"theta-demo\" . In addition to shell command templates, we can define Apps that invoke a Python run() function on a compute node: class VecNorm ( ApplicationDefinition ): site = \"theta-demo\" def run ( self , vec ): return sum ( x ** 2 for x in vec ) ** 0.5 VecNorm . sync () After running the sync() methods for these Apps, they are serialized and shipped into the Balsam cloud service. We can then load and re-use these Apps when submitting Jobs from other Python programs. Add Jobs \u00b6 With these App classes in hand, we can now submit some jobs from the Python SDK. Let's create a Job for both the Hello and VecNorm apps: all we need is to pass a working directory and any necessary parameters for each: hello = Hello . submit ( workdir = \"demo/hello\" , say_hello_to = \"world\" ) norm = VecNorm . submit ( workdir = \"demo/norm\" , vec = [ 3 , 4 ]) Notice how shell command parameters (for Hello ) and Python function parameters (for VecNorm ) are treated on the same footing. We have now created two Jobs that will eventually run on the Site theta-demo , once compute resources are available. These Jobs can be seen by running balsam job ls . Make it run \u00b6 A key concept in Balsam is that Jobs only specify what to run, and you must create BatchJobs to provision the resources that actually execute your jobs. This way, your workflow definitions are neatly separated from the concern of what allocation they run on. You create a collection of Jobs first, and then many of these Jobs can run inside one (or more) BatchJobs. Since BatchJobs dynamically acquire Jobs, Balsam execution is fully elastic (just spin up more nodes by adding another BatchJob) and migratable (a Job that ran out of time in one batch allocation will get picked up in the next BatchJob). BatchJobs will automatically run as many Jobs as they can at their Site. You can simply queue up one or several BatchJobs and let them divide and conquer your workload. BatchJob . objects . create ( site_id = hello_job . site_id , num_nodes = 1 , wall_time_min = 10 , job_mode = \"mpi\" , queue = \"local\" , # Use the appropriate batch queue, or `local` project = \"local\" , # Use the appropriate allocation, or `local` ) After running this command, the Site agent will fetch the new BatchJob and perform the necessary pilot job submission with the local resource manager (e.g. via qsub ). The hello and norm Jobs will run in the workdirs specified above, located relative to the Site's data/ directory. You will find the \"Hello world\" job output in a job.out file therein. For Python run() applications, the created Jobs can be handled like concurrent.futures.Future instances, where the result() method delivers the return value (or re-raises the Exception) from a run() invocation. assert norm . result () == 5.0 # (3**2 + 4**2)**0.5 When creating BatchJobs, you can verify that the allocation was actually created by checking with the local resource manager (e.g. qstat ) or by checking with Balsam: $ balsam queue ls When the BatchJob with job_mode=\"mpi\" starts, an MPI mode launcher pilot job acquires and runs the jobs. You will find helpful logs in the logs/ directory showing what's going on. Follow the Job statuses with balsam job ls : $ balsam job ls ID Site App Workdir State Tags 267280 thetalogin4:my-site Hello test/2 JOB_FINISHED {} 267279 thetalogin4:my-site Hello test/1 JOB_FINISHED {} A complete Python example \u00b6 Now let's combine the Python snippets from above to show a self-contained example of Balsam SDK usage (e.g. something you might run from a Jupyter notebook): from balsam.api import ApplicationDefinition , BatchJob , Job class VecNorm ( ApplicationDefinition ): site = \"my-local-site\" def run ( self , vec ): return sum ( x ** 2 for x in vec ) ** 0.5 jobs = [ VecNorm . submit ( workdir = \"test/1\" , vec = [ 3 , 4 ]), VecNorm . submit ( workdir = \"test/2\" , vec = [ 6 , 8 ]), ] BatchJob . objects . create ( site_id = jobs [ 0 ] . site_id , num_nodes = 1 , wall_time_min = 10 , job_mode = \"mpi\" , queue = \"local\" , project = \"local\" , ) for job in Job . objects . as_completed ( jobs ): print ( job . workdir , job . result ()) Submitting Jobs from the command line \u00b6 You can check the Apps registered at a given Site, or across all Sites, from the command line: $ balsam app ls --site = all ID Name Site 286 Hello laptop 287 VecNorm laptop To create a Balsam Job from the CLI, you must identify the App on the command line: $ balsam job create --site = laptop --app Hello --workdir demo/hello2 --param say_hello_to = \"world2\" There are many additional CLI options in job creation, which can be summarized with balsam job create --help . You will usually create jobs using Python, but the CLI remains useful for monitoring workflow status: $ balsam job ls ID Site App Workdir State Tags 501649 laptop Hello test/1 PREPROCESSED {} 501650 laptop Hello test/2 STAGED_IN {} BatchJobs can be submitted from the CLI, with parameters that mimic a standard scheduler interface: # Substitute -q QUEUE and -A ALLOCATION for your project: $ balsam queue submit -q debug-cache-quad -A datascience -n 1 -t 10 -j mpi","title":"Getting Started"},{"location":"tutorials/theta-quickstart/#getting-started","text":"This tutorial gets you up and running with a new Balsam Site quickly. Since Balsam is highly platform-agnostic, you can follow along by choosing from any of the available default site setups: A local MacOS or Linux system Polaris Theta-GPU Theta-KNL Cooley Cori (Haswell or KNL partitions) Perlmutter Summit","title":"Getting started"},{"location":"tutorials/theta-quickstart/#install","text":"First create a new virtualenv and install Balsam: $ /soft/datascience/create_env.sh my-env # Or DIY $ source my-env/bin/activate $ pip install --pre balsam","title":"Install"},{"location":"tutorials/theta-quickstart/#log-in","text":"Now that balsam is installed, you need to log in. Logging in fetches an access token that is used to identify you in all future API interactions, until the token expires and you have to log in again. $ balsam login # Follow the prompt to authenticate Once you are logged in, you can create a Balsam Site for job execution, or send jobs to any of your other existing Sites. Login temporarily restricted Balsam is currently in a pre-release stage and the web service is hosted on limited resources. Consequently, logins are limited to pre-authorized users. Please contact the ALCF Help Desk to request early access membership to the Balsam user group.","title":"Log In"},{"location":"tutorials/theta-quickstart/#create-a-balsam-site","text":"All Balsam workflows are namespaced under Sites : self-contained project spaces belonging to individual users. You can use Balsam to manage Sites on multiple HPC systems from a single shell or Python program. This is one of the key strengths of Balsam: the usage looks exactly the same whether you're running locally or managing jobs across multiple supercomputers. Let's start by creating a Balsam Site in a folder named ./my-site : $ balsam site init ./my-site # Select the default configuration for Theta-KNL You will be prompted to select a default Site configuration and to enter a unique name for your Site. The directory my-site/ will then be created and preconfigured for the chosen platform. You can list your Sites with the balsam site ls command. $ balsam site ls ID Name Path Active 21 theta-demo .../projects/datascience/my-site No In order to actually run anything at the Site, you have to enter the Site directory and start it with balsam site start . This command launches a persistent background agent which uses your access token to sync with the Balsam service. $ cd my-site $ balsam site start","title":"Create a Balsam Site"},{"location":"tutorials/theta-quickstart/#set-up-your-apps","text":"Every Site has its own collection of Balsam Apps , which define the runnable applications at that Site. A Balsam App is declared by writing an ApplicationDefinition class and running the sync() method from a Python program or interactive session. The simplest ApplicationDefinition is just a template for a bash command, with any workflow variables enclosed in double-curly braces: from balsam.api import ApplicationDefinition class Hello ( ApplicationDefinition ): site = \"theta-demo\" command_template = \"echo Hello, {{ say_hello_to }}!\" Hello . sync () Notice the attribute site = \"theta-demo\" which is required to associate the App \"Hello\" to the Site \"theta-demo\" . In addition to shell command templates, we can define Apps that invoke a Python run() function on a compute node: class VecNorm ( ApplicationDefinition ): site = \"theta-demo\" def run ( self , vec ): return sum ( x ** 2 for x in vec ) ** 0.5 VecNorm . sync () After running the sync() methods for these Apps, they are serialized and shipped into the Balsam cloud service. We can then load and re-use these Apps when submitting Jobs from other Python programs.","title":"Set up your Apps"},{"location":"tutorials/theta-quickstart/#add-jobs","text":"With these App classes in hand, we can now submit some jobs from the Python SDK. Let's create a Job for both the Hello and VecNorm apps: all we need is to pass a working directory and any necessary parameters for each: hello = Hello . submit ( workdir = \"demo/hello\" , say_hello_to = \"world\" ) norm = VecNorm . submit ( workdir = \"demo/norm\" , vec = [ 3 , 4 ]) Notice how shell command parameters (for Hello ) and Python function parameters (for VecNorm ) are treated on the same footing. We have now created two Jobs that will eventually run on the Site theta-demo , once compute resources are available. These Jobs can be seen by running balsam job ls .","title":"Add Jobs"},{"location":"tutorials/theta-quickstart/#make-it-run","text":"A key concept in Balsam is that Jobs only specify what to run, and you must create BatchJobs to provision the resources that actually execute your jobs. This way, your workflow definitions are neatly separated from the concern of what allocation they run on. You create a collection of Jobs first, and then many of these Jobs can run inside one (or more) BatchJobs. Since BatchJobs dynamically acquire Jobs, Balsam execution is fully elastic (just spin up more nodes by adding another BatchJob) and migratable (a Job that ran out of time in one batch allocation will get picked up in the next BatchJob). BatchJobs will automatically run as many Jobs as they can at their Site. You can simply queue up one or several BatchJobs and let them divide and conquer your workload. BatchJob . objects . create ( site_id = hello_job . site_id , num_nodes = 1 , wall_time_min = 10 , job_mode = \"mpi\" , queue = \"local\" , # Use the appropriate batch queue, or `local` project = \"local\" , # Use the appropriate allocation, or `local` ) After running this command, the Site agent will fetch the new BatchJob and perform the necessary pilot job submission with the local resource manager (e.g. via qsub ). The hello and norm Jobs will run in the workdirs specified above, located relative to the Site's data/ directory. You will find the \"Hello world\" job output in a job.out file therein. For Python run() applications, the created Jobs can be handled like concurrent.futures.Future instances, where the result() method delivers the return value (or re-raises the Exception) from a run() invocation. assert norm . result () == 5.0 # (3**2 + 4**2)**0.5 When creating BatchJobs, you can verify that the allocation was actually created by checking with the local resource manager (e.g. qstat ) or by checking with Balsam: $ balsam queue ls When the BatchJob with job_mode=\"mpi\" starts, an MPI mode launcher pilot job acquires and runs the jobs. You will find helpful logs in the logs/ directory showing what's going on. Follow the Job statuses with balsam job ls : $ balsam job ls ID Site App Workdir State Tags 267280 thetalogin4:my-site Hello test/2 JOB_FINISHED {} 267279 thetalogin4:my-site Hello test/1 JOB_FINISHED {}","title":"Make it run"},{"location":"tutorials/theta-quickstart/#a-complete-python-example","text":"Now let's combine the Python snippets from above to show a self-contained example of Balsam SDK usage (e.g. something you might run from a Jupyter notebook): from balsam.api import ApplicationDefinition , BatchJob , Job class VecNorm ( ApplicationDefinition ): site = \"my-local-site\" def run ( self , vec ): return sum ( x ** 2 for x in vec ) ** 0.5 jobs = [ VecNorm . submit ( workdir = \"test/1\" , vec = [ 3 , 4 ]), VecNorm . submit ( workdir = \"test/2\" , vec = [ 6 , 8 ]), ] BatchJob . objects . create ( site_id = jobs [ 0 ] . site_id , num_nodes = 1 , wall_time_min = 10 , job_mode = \"mpi\" , queue = \"local\" , project = \"local\" , ) for job in Job . objects . as_completed ( jobs ): print ( job . workdir , job . result ())","title":"A complete Python example"},{"location":"tutorials/theta-quickstart/#submitting-jobs-from-the-command-line","text":"You can check the Apps registered at a given Site, or across all Sites, from the command line: $ balsam app ls --site = all ID Name Site 286 Hello laptop 287 VecNorm laptop To create a Balsam Job from the CLI, you must identify the App on the command line: $ balsam job create --site = laptop --app Hello --workdir demo/hello2 --param say_hello_to = \"world2\" There are many additional CLI options in job creation, which can be summarized with balsam job create --help . You will usually create jobs using Python, but the CLI remains useful for monitoring workflow status: $ balsam job ls ID Site App Workdir State Tags 501649 laptop Hello test/1 PREPROCESSED {} 501650 laptop Hello test/2 STAGED_IN {} BatchJobs can be submitted from the CLI, with parameters that mimic a standard scheduler interface: # Substitute -q QUEUE and -A ALLOCATION for your project: $ balsam queue submit -q debug-cache-quad -A datascience -n 1 -t 10 -j mpi","title":"Submitting Jobs from the command line"},{"location":"user-guide/api/","text":"The Python API \u00b6 The documentation alludes to the Balsam Python API in several places. For instance, the Managing Jobs section gives real-world examples of API usage in creating, querying, and updating Jobs . In this section, we take a step back and look more generally at the structure and semantics of Balsam's Python API. This is useful because all the Python API resources ( Job , BatchJob , EventLog ) share the same methods and data structures for creating, querying, updating, and deleting resources. The first thing to understand is that the Python API is merely a convenience layer built on top of a standard HTTP requests client. One could bypass balsam.api altogether and interact with the Balsam REST API using another programming language or a command-line tool like curl . Resources are imported at the top-level from balsam.api : from balsam.api import ( Site , ApplicationDefinition , Job , BatchJob , TransferItem , EventLog , ) The following sections use Job as an example but easily generalize to any of the resources imported above. For example, querying your Sites looks just like querying your Jobs : from datetime import datetime , timedelta hour_ago = datetime . utcnow () - timedelta ( hours = 1 ) recently_used_sites = Site . objects . filter ( last_refresh_after = hour_ago ) recently_updated_jobs = Job . objects . filter ( last_update_after = hour_ago ) Once again, the docstrings and type annotations visible in a Python IDE are hugely helpful in discovering the allowed parameters for various resources. Best of all, because the Python API is dynamically generated from the REST API schema, the Python docstrings and type hints stay up to date, even if this user guide lags behind! Model Fields \u00b6 Each of the API resources has a model class (like Job ) which defines a set of Fields that can be exchanged with the REST API. The model fields are type-annotated and can be explored from your IDE or class docstrings. For example, we can start searching for \"param\" and find that job.parameters should be a dictionary of strings: Relationship to Django ORM Previous versions of Balsam used the real Django ORM to communicate with a private user database. This proved to be an effective programming model, and so the new Balsam Python API was written to preserve a subset of the structure and syntax of the former API. If you peek below the surface, the current Balsam Python API is completely different from an ORM. Whereas ORMs lazily execute SQL queries over a database connection, the Balsam API executes HTTPS requests over an Internet connection. It wraps one specific REST API schema and is therefore vastly narrower in scope and capabilities. Creating \u00b6 Each resource has three methods of construction. First, you can create several in-memory instances, and later persist them to the backend: # Create in-memory with the required kwargs... j = Job ( ... ) # New in-memory resources have no ID: assert j . id is None # ...then persist: j . save () assert j . id is not None Second, you can create and persist in a single step: j = Job . objects . create ( ... ) assert j . id is not None Third, you can bulk-create from a collection of in-memory resources. This is far more efficient for creating large numbers of jobs with fewer API round trips: jobs = [ Job ( ** kwargs ) for kwargs in job_specs ] # Capture the return value to get created IDs! jobs = Job . objects . bulk_create ( jobs ) for job in jobs : assert job . id is not None bulk_create does not modify objects in place! When passing a list of items into bulk_create() , you must use the returned value to overwrite the input list with the newly-created items. This is necessary to set the ID on each item as generated by the server. Otherwise, the created items will have id == None and generally behave like objects that have never been saved to the API. Updating \u00b6 If we change some fields on an existing instance, we can update it by calling save() . The Python API is aware that if the resource id is set, you mean to update an existing resource rather than create a new one . job . state = \"RESTART_READY\" job . save () If want to load recent changes to an in-memory resource: # Re-fetch the Job & update fields: job . refresh_from_db () We can apply the same change to every item matching a particular Query : # Select all FAILED jobs, change num_nodes=1, and mark for retry Job . objects . filter ( state = \"FAILED\" ) . update ( num_nodes = 1 , state = \"RESTART_READY\" ) If we need to perform a large list of different updates, we can pass a list of mutated instances: # Mutated jobs to be updated, each in their own way: Job . objects . bulk_update ( modified_jobs ) Deleting \u00b6 We can delete individual resources: job . delete () assert job . id is None Or we can bulk-delete querysets: Job . objects . filter ( state = \"FAILED\" ) . delete () Querying \u00b6 Each class has a manager (for instance Job.objects is a JobManager ) which generates Query objects. All \u00b6 We can start with a query returning all items with all() for job in Job . objects . all (): print ( job ) Filter \u00b6 We can chain queries one-after-another by providing additional sets of filter parameters: # These 3 chained queries... all_jobs = Job . objects . all () foo_jobs = all_jobs . filter ( tags = { \"experiment\" : \"foo\" }) failed_foo_jobs = foo_jobs . filter ( state = \"FAILED\" ) # ...are equivalent to this 1 query: failed_foo_jobs = Job . objects . filter ( tags = { \"experiment\" : \"foo\" }, state = \"FAILED\" ) Order By and Slicing \u00b6 When there are hundreds of thousands of Jobs matching your query, it makes sense to order on some criterion and take chunks with the slicing operator: # 1000 most recent failures Job . objects . filter ( state = \"FAILED\" ) . order_by ( \"-last_update\" )[ 0 : 1000 ] Under the hood, the [0:1000] slice operation adds limit and offset to the HTTP query parameters, generating an efficient request that does not fetch more data than you asked for! Get \u00b6 If our query should return exactly one object, we can use get() instead of filter() to return the object directly. This method raises a model-scoped DoesNotExist error if the query returned 0 items, or MultipleObjectsReturned if more than 1 item was found. We can catch these errors that arise when when exactly one unique object is expected: try : h2o_job = Job . objects . get ( tags = { \"system\" : \"H2O\" }) except Job . DoesNotExist : print ( \"There is no finished H2O job!\" ) except Job . MultipleObjectsReturned : print ( \"There is more than one finished H2O job!\" ) Count \u00b6 We can use count() to fetch the number of items matching a query, without actually fetching the list of items. This can be useful to quickly tally up large numbers of Jobs : for state in [ \"RUNNING\" , \"JOB_FINISHED\" ]: count = Job . objects . filter ( state = state ) . count () print ( state , count ) First \u00b6 To grab the first item from a query we can use either syntax: one = Job . objects . filter ( state = \"RUNNING\" )[ 0 ] same_thing = Job . objects . filter ( state = \"RUNNING\" ) . first () Lazy Query Evaluation \u00b6 We can build queries with filter , chain them together, add order_by clauses, apply [start:end] slices, and store these queries in Python variables. None of these actions fetches data because queries are lazily evaluated. # This doesn't fetch any Jobs yet: failed_jobs = Job . objects . filter ( state = \"FAILED\" ) When a query is evaluated by iteration or some other method, its result is cached, so that repeated iterations over the same query variable do not trigger redundant requests. # This triggers the HTTP request: for job in failed_jobs : print ( job ) # This re-uses the cached result: for job in failed_jobs : print ( \"Again!\" , job ) # We can explicitly force iteration and store the result: running_jobs = list ( Job . objects . filter ( state = \"RUNNING\" )) Length \u00b6 Evaluating the len(query) forces fetching the result set and returns its length. If you only want the count without fetching the items, its more efficient to use the count() method mentioned above. Boolean Value \u00b6 Evaluating the query as a Boolean expression (e.g. in an if statement like if query: ) also triggers evaluation, and the query evaluates to True if there is at least one object in the result set; it's False otherwise.","title":"The Python API"},{"location":"user-guide/api/#the-python-api","text":"The documentation alludes to the Balsam Python API in several places. For instance, the Managing Jobs section gives real-world examples of API usage in creating, querying, and updating Jobs . In this section, we take a step back and look more generally at the structure and semantics of Balsam's Python API. This is useful because all the Python API resources ( Job , BatchJob , EventLog ) share the same methods and data structures for creating, querying, updating, and deleting resources. The first thing to understand is that the Python API is merely a convenience layer built on top of a standard HTTP requests client. One could bypass balsam.api altogether and interact with the Balsam REST API using another programming language or a command-line tool like curl . Resources are imported at the top-level from balsam.api : from balsam.api import ( Site , ApplicationDefinition , Job , BatchJob , TransferItem , EventLog , ) The following sections use Job as an example but easily generalize to any of the resources imported above. For example, querying your Sites looks just like querying your Jobs : from datetime import datetime , timedelta hour_ago = datetime . utcnow () - timedelta ( hours = 1 ) recently_used_sites = Site . objects . filter ( last_refresh_after = hour_ago ) recently_updated_jobs = Job . objects . filter ( last_update_after = hour_ago ) Once again, the docstrings and type annotations visible in a Python IDE are hugely helpful in discovering the allowed parameters for various resources. Best of all, because the Python API is dynamically generated from the REST API schema, the Python docstrings and type hints stay up to date, even if this user guide lags behind!","title":"The Python API"},{"location":"user-guide/api/#model-fields","text":"Each of the API resources has a model class (like Job ) which defines a set of Fields that can be exchanged with the REST API. The model fields are type-annotated and can be explored from your IDE or class docstrings. For example, we can start searching for \"param\" and find that job.parameters should be a dictionary of strings: Relationship to Django ORM Previous versions of Balsam used the real Django ORM to communicate with a private user database. This proved to be an effective programming model, and so the new Balsam Python API was written to preserve a subset of the structure and syntax of the former API. If you peek below the surface, the current Balsam Python API is completely different from an ORM. Whereas ORMs lazily execute SQL queries over a database connection, the Balsam API executes HTTPS requests over an Internet connection. It wraps one specific REST API schema and is therefore vastly narrower in scope and capabilities.","title":"Model Fields"},{"location":"user-guide/api/#creating","text":"Each resource has three methods of construction. First, you can create several in-memory instances, and later persist them to the backend: # Create in-memory with the required kwargs... j = Job ( ... ) # New in-memory resources have no ID: assert j . id is None # ...then persist: j . save () assert j . id is not None Second, you can create and persist in a single step: j = Job . objects . create ( ... ) assert j . id is not None Third, you can bulk-create from a collection of in-memory resources. This is far more efficient for creating large numbers of jobs with fewer API round trips: jobs = [ Job ( ** kwargs ) for kwargs in job_specs ] # Capture the return value to get created IDs! jobs = Job . objects . bulk_create ( jobs ) for job in jobs : assert job . id is not None bulk_create does not modify objects in place! When passing a list of items into bulk_create() , you must use the returned value to overwrite the input list with the newly-created items. This is necessary to set the ID on each item as generated by the server. Otherwise, the created items will have id == None and generally behave like objects that have never been saved to the API.","title":"Creating"},{"location":"user-guide/api/#updating","text":"If we change some fields on an existing instance, we can update it by calling save() . The Python API is aware that if the resource id is set, you mean to update an existing resource rather than create a new one . job . state = \"RESTART_READY\" job . save () If want to load recent changes to an in-memory resource: # Re-fetch the Job & update fields: job . refresh_from_db () We can apply the same change to every item matching a particular Query : # Select all FAILED jobs, change num_nodes=1, and mark for retry Job . objects . filter ( state = \"FAILED\" ) . update ( num_nodes = 1 , state = \"RESTART_READY\" ) If we need to perform a large list of different updates, we can pass a list of mutated instances: # Mutated jobs to be updated, each in their own way: Job . objects . bulk_update ( modified_jobs )","title":"Updating"},{"location":"user-guide/api/#deleting","text":"We can delete individual resources: job . delete () assert job . id is None Or we can bulk-delete querysets: Job . objects . filter ( state = \"FAILED\" ) . delete ()","title":"Deleting"},{"location":"user-guide/api/#querying","text":"Each class has a manager (for instance Job.objects is a JobManager ) which generates Query objects.","title":"Querying"},{"location":"user-guide/api/#all","text":"We can start with a query returning all items with all() for job in Job . objects . all (): print ( job )","title":"All"},{"location":"user-guide/api/#filter","text":"We can chain queries one-after-another by providing additional sets of filter parameters: # These 3 chained queries... all_jobs = Job . objects . all () foo_jobs = all_jobs . filter ( tags = { \"experiment\" : \"foo\" }) failed_foo_jobs = foo_jobs . filter ( state = \"FAILED\" ) # ...are equivalent to this 1 query: failed_foo_jobs = Job . objects . filter ( tags = { \"experiment\" : \"foo\" }, state = \"FAILED\" )","title":"Filter"},{"location":"user-guide/api/#order-by-and-slicing","text":"When there are hundreds of thousands of Jobs matching your query, it makes sense to order on some criterion and take chunks with the slicing operator: # 1000 most recent failures Job . objects . filter ( state = \"FAILED\" ) . order_by ( \"-last_update\" )[ 0 : 1000 ] Under the hood, the [0:1000] slice operation adds limit and offset to the HTTP query parameters, generating an efficient request that does not fetch more data than you asked for!","title":"Order By and Slicing"},{"location":"user-guide/api/#get","text":"If our query should return exactly one object, we can use get() instead of filter() to return the object directly. This method raises a model-scoped DoesNotExist error if the query returned 0 items, or MultipleObjectsReturned if more than 1 item was found. We can catch these errors that arise when when exactly one unique object is expected: try : h2o_job = Job . objects . get ( tags = { \"system\" : \"H2O\" }) except Job . DoesNotExist : print ( \"There is no finished H2O job!\" ) except Job . MultipleObjectsReturned : print ( \"There is more than one finished H2O job!\" )","title":"Get"},{"location":"user-guide/api/#count","text":"We can use count() to fetch the number of items matching a query, without actually fetching the list of items. This can be useful to quickly tally up large numbers of Jobs : for state in [ \"RUNNING\" , \"JOB_FINISHED\" ]: count = Job . objects . filter ( state = state ) . count () print ( state , count )","title":"Count"},{"location":"user-guide/api/#first","text":"To grab the first item from a query we can use either syntax: one = Job . objects . filter ( state = \"RUNNING\" )[ 0 ] same_thing = Job . objects . filter ( state = \"RUNNING\" ) . first ()","title":"First"},{"location":"user-guide/api/#lazy-query-evaluation","text":"We can build queries with filter , chain them together, add order_by clauses, apply [start:end] slices, and store these queries in Python variables. None of these actions fetches data because queries are lazily evaluated. # This doesn't fetch any Jobs yet: failed_jobs = Job . objects . filter ( state = \"FAILED\" ) When a query is evaluated by iteration or some other method, its result is cached, so that repeated iterations over the same query variable do not trigger redundant requests. # This triggers the HTTP request: for job in failed_jobs : print ( job ) # This re-uses the cached result: for job in failed_jobs : print ( \"Again!\" , job ) # We can explicitly force iteration and store the result: running_jobs = list ( Job . objects . filter ( state = \"RUNNING\" ))","title":"Lazy Query Evaluation"},{"location":"user-guide/api/#length","text":"Evaluating the len(query) forces fetching the result set and returns its length. If you only want the count without fetching the items, its more efficient to use the count() method mentioned above.","title":"Length"},{"location":"user-guide/api/#boolean-value","text":"Evaluating the query as a Boolean expression (e.g. in an if statement like if query: ) also triggers evaluation, and the query evaluates to True if there is at least one object in the result set; it's False otherwise.","title":"Boolean Value"},{"location":"user-guide/appdef/","text":"Defining Applications \u00b6 Registering ApplicationDefinitions \u00b6 Once you have a Site, the next step is to define the applications that Balsam may run. Each Site is linked to a set of ApplicationDefinition Python classes: check the Getting Started tutorial to see a quick example of this in action. You can create and register ApplicationDefinition subclasses from any Python session. For instance, Balsam supports interactive workflows in Jupyter notebooks, or workflows driven by any other Python software. At a minimum, ApplicationDefinitions must declare the site and a command_template for a shell command: from balsam.api import ApplicationDefinition class Sleeper ( ApplicationDefinition ): site = \"theta-gpu\" command_template = 'sleep {{ sleeptime }} && echo goodbye' Sleeper . sync () The site attribute must be present on each ApplicationDefinition subclass to identify where the app runs. The site can take any of the following types unambiguously specifying a Site: Site name (uniquely assigned during balsam site init , like \"theta-gpu\" ) Site ID (e.g. 142 ) Site object (fetched from the API ) The ApplicationDefinition is uniquely identified by its class name and site . In the above example, we defined the Sleeper application at the theta-gpu Site. When Sleeper.sync() is called, the Python class and associated metadata is serialized and shipped to the Balsam web API. The Sleeper ApplicationDefinition is thereafter linked to the Site named theta-gpu , and workflows can proceed to submit Sleeper Jobs to theta-gpu from anywhere! ApplicationDefinitions must be named uniquely! If another Python session syncs a different Sleeper class belonging to the same Site, the previous application will be overwritten! This is because apps are uniquely resolved by the pair ( site , class_name ). This is typically the desired behavior: simply running sync() will ensure that Balsam applications stay up-to-date with your source code. However, inadvertent name collisions can cause unexpected results when you overwrite the implementation of an existing ApplicationDefinition . The submit() shortcut \u00b6 To run an application, we then submit a Job to invoke the command with specific resources and parameters. The submit() class method provides a convenient syntax to combine the sync initialization step with job submission in a single call: # Implicitly sync (updates or creates the App) and submit a Job: job = Sleeper . submit ( workdir = \"test/sleep_3\" , sleeptime = 3 ) This shorthand syntax is completely equivalent to the following: from balsam.api import Job Sleeper . sync () job = Job ( workdir = \"test/sleep_3\" , app_id = Sleeper , parameters = { \"sleeptime\" : 3 }) job . save () It's more efficient to use bulk-creation to submit large numbers of Jobs in a single network call. This is possible by passing the special keyword argument save=False : jobs = [ Sleeper . submit ( workdir = f \"test/ { i } \" , sleeptime = 3 , save = False ) for i in range ( 100 ) ] jobs = Job . objects . bulk_create ( jobs ) Besides the special save kwarg, The submit() method has the same signature as the Job() constructor which is covered in-depth on the next page . Python Applications \u00b6 Besides running shell commands, Balsam can run Python applications directly on the compute nodes. This paradigm significantly cuts down boilerplate and reduces the need for creating \"entry point\" scripts. Instead of using command_template , the ApplicationDefinition can simply define a run() method that will be launched using the exact same rules as ordinary shell applications. class Adder ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , x , y ): return x + y job = Adder . submit ( \"test/5plus5\" , x = 5 , y = 5 ) assert job . result () == 10 # This will block until a BatchJob starts run is an instance method and should take self as the first argument. Additional positional or keyword arguments can be supplied as well. When submitting Jobs , the parameters are serialized (under the hood job.parameters = {\"x\": 5, \"y\": 5} is converted to a Base64-encoded byte string with dill and stored as part of the Job in the Balsam web API.) The submitted Jobs behave partially like concurrent.futures.Future objects: namely, the return value or Exception raised by run() will propagate to the result() method. Refer to the Jobs documentation for more details on these Future -like APIs. Python App Capabilities and Limitations \u00b6 Python run() function-based ApplicationDefinitions enjoy all the same lifecycle hooks and flexible resource launching capabilities as ordinary ApplicationDefinitions . For instance, your Balsam apps can directly call into mpi4py code and be launched onto multiple compute nodes: import numpy as np class NumpyPlusMPI ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , * dims ): from mpi4py import MPI if MPI . COMM_WORLD . Get_rank () == 0 : return np . random . rand ( * dims ) job = NumpyPlusMPI . submit ( workdir = \"test/parallel\" , dims = [ 5 ], ranks_per_node = 4 , ) The run function can generally refer to imported modules and objects from other namespaces, as long as they are importable in the Balsam Site's Python environment . This is a crucial constraint to understand when writing ApplicationDefinitions : all import statements are essentially replayed in the Balsam Site environment when the ApplicationDefinition is loaded and de-serialized. This will fail if any any object referenced by the ApplicationDefinition cannot be imported from sys.path ! Import statements will replay on the Balsam Site The issues above are easily avoided by ensuring that your codes and their dependencies are installed in the Python virtual environment where the Balsam Site runs. Just remember that the Balsam app serializer does not recurse and ship other modules over the wire. Virtually any import statement referenced by the ApplicationDefintion must work on both sides. The same constraint holds true when the ApplicationDefinitions themselves are located in an imported module. For example, if your Balsam code is packaged in my_science_package , that module/package must be installed in the Site environment where the ApplicationDefinition is synced to. # This import line must work on theta-gpu... from my_science_package import setup_workflow # If this method syncs apps to theta-gpu: setup_workflow ( site = \"theta-gpu\" ) Technical Details Under the hood, all ApplicationDefinitions , Job parameters, return values, and exceptions are serialized and deserialized using dill with the recurse=True option. Besides the issue of software dependencies, the following additional limitations should be kept in mind when writing ApplicationDefinitions : Do not use any multiprocessing.Process . You can spawn background Thread tasks, but Process workers will attempt to communicate using pickle in a fashion that is incompatible with the serialization scheme used by Balsam. Do not use the super() syntax anywhere in the ApplicationDefinition . This is a known issue with dill . If you are writing your own class hierarchies, you can always get around this by referencing the parent class directly. Avoid referring to balsam.api in the ApplicationDefinition . Instead, you will need to manually initialize the client to fetch resources as follows: from balsam.config import SiteConfig client = SiteConfig () . client # Loads a fresh RESTClient Job = client . Job # Use just like balsam.api.Job ApplicationDefinitions , parameters, return values, and exceptions should be lean. Do not rely on the Balsam API to move large quantities of data (instead, the Transfer API is designed for easy interoperability with out-of-band transfer methods like Globus). Balsam imposes limits on the order of 100KB for each serialized entity. Listing and Refreshing Applications \u00b6 You can check the Apps registered at a site using the CLI: # List apps across all Sites $ balsam app ls --site = all Restart Sites to Reload ApplicationDefinitions A new ApplicationDefinition will be automatically loaded by a running Site agent or launcher BatchJob. However, if you modify an existing app while it is loaded in a running Site or launcher, the change will not propagate! You must remember to run balsam site sync to refresh the loaded apps in a running Site. Loading Existing ApplicationDefinitions \u00b6 You don't need a handle on the ApplicationDefinition source code to submit Jobs with it. Instead, the app_id argument to Job() can take an application by name (string), ID (integer), or reference to a loaded ApplicationDefinition . For example, you can load the set of applications registered at a given Site as follows: apps_by_name = ApplicationDefinition . load_by_site ( \"theta-gpu\" ) Sleeper = apps_by_name [ \"Sleeper\" ] ApplicationDefinitions can then be used from this dictionary as if you had defined them in the current Python session. Writing ApplicationDefinitions \u00b6 At their simplest, ApplicationDefinitions provide a declarative template for a shell command and its adjustable parameters. Alternatively, they define a Python run() function that takes arbitrary inputs. To run an application, we submit a Job that provides values for these parameters. Importantly, we do not specify how the application is launched ( mpiexec ) or its CPU/GPU resources in the ApplicationDefinition . Instead, Balsam takes care of managing resources and building the command lines to efficiently launch our Jobs. Besides the fundamental site , command_template , and run attributes discussed above, ApplicationDefinitions provide other special attributes and methods that we can override to build more complex and useful workflow components. The Class Path \u00b6 Balsam Apps are uniquely identified by: The Site that they belong to Their ApplicationDefinition class __name__ For instance, the Sleeper application we defined above in test.py has a name of Sleeper . We use this to uniquely identify each ApplicationDefinition class later on. The Description \u00b6 The docstring that follows the class statement is captured by Balsam and stored as a description with the REST API. This is purely human-readable text that can be displayed in your App catalog. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" The Site Identifier \u00b6 The site attribute is required on all ApplicationDefinitions and it must unambiguously refer to one of your existing Sites. This class attribute can be a string (site name), integer (site ID), or a Site object loaded from the Balsam SDK. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" Environment Variables \u00b6 The environment_variables attribute should be a Dict[str, str] mapping environment variable names to values. This is useful for constant environment variables that do not vary across runs. This environment is merged with the environment established in the job template. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } Command Template \u00b6 As we have seen, ApplicationDefinitions must contain either a command_template or a run() method. These are mutually exclusive: you must set one or the other. The command_template is interpreted as a Jinja2 template; therefore, parameters must be enclosed in double-curly braces. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\" By default, all app parameters are required parameters: it is an error to omit any parameter named in the template. We can change this behavior below. The run function \u00b6 When the ApplicationDefinition contains a run() method, this function is launched onto compute resources using the parameters set on the corresponding Job . import numpy as np class VecNorm ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , vec ): return np . linalg . norm ( vec ) Python executable \u00b6 When using a run() function, it is important that the execution-side Python environment has the necessary dependencies installed. The optional class attribute python_exe defaults to sys.executable and should not be changed if the app runs in the same environment Balsam is installed in. You should override python_exe if you wish to invoke the run function using a different Python environment from the one in which Balsam is installed. This setting has no effect for command_template apps. import numpy as np class VecNorm ( ApplicationDefinition ): site = \"theta-gpu\" python_exe = \"/path/to/bin/python3.8\" def run ( self , vec ): import numpy as np # Loaded from `python_exe` return np . linalg . norm ( vec ) Parameter Spec \u00b6 Maybe we want to have some optional parameters in the command_template , which take on a default value in the absence of a value specified in the Job. We can do this by providing the parameters dictionary: class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/sim.exe --mode {{ mode }} -inp {{ input_filename }}\" parameters = { \"input_filename\" : { \"required\" : True }, \"mode\" : { \"required\" : False , \"default\" : \"explore\" , \"help\" : \"The simulation mode (default: explore)\" , } } Notice that parameters are either required, in which case it doesn't make sense to have a default value, or not. If a parameter's required value is False , you must provide a default value that is used when the parameter is not passed. The help field is another optional, human-readable field, to assist with App curation in the Web interface. Valid Python Identifiers App parameters can only contain valid Python identifiers, so names with - , for instance, will be rejected when you attempt to run balsam app sync . Transfer Slots \u00b6 A core feature of Balsam, described in more detail in the Data Transfers section , is the ability to write distributed workflows, where data products move between Sites, and Jobs can be triggered when data arrives at its destination. We create this behavior starting at the ApplicationDefinition level, by defining Transfer Slots for data that needs to be staged in before or staged out after execution. You can think of the Job workdir as an ephemeral sandbox where data arrives, computation happens, and then results are staged out to a more accessible location for further analysis. Each ApplicationDefinition may declare a transfers dictionary, where each string key names a Transfer Slot. class MySimulation ( ApplicationDefinition ): transfers = { \"input_file\" : { \"required\" : True , \"direction\" : \"in\" , \"local_path\" : \"input.nw\" , \"description\" : \"Input Deck\" , \"recursive\" : False , }, \"result\" : { \"required\" : True , \"direction\" : \"out\" , \"local_path\" : \"job.out\" , \"description\" : \"Calculation stdout\" , \"recursive\" : False }, }, In order to fill the slots, each Job invoking this application must then provide concrete URIs of the external files: Job . objects . create ( workdir = \"ensemble/1\" , app_name = \"sim.MySimulation\" , transfers = { # Using 'laptop' alias defined in settings.yml \"input_file\" : \"laptop:/path/to/input.dat\" , \"result\" : \"laptop:/path/to/output.json\" , }, ) Transfer slots with required=False are optional when creating Jobs. The direction key must contain the value \"in\" or \"out\" for stage-in and stage-out, respectively. The description is an optional, human-readable parameter to assist in App curation. The recursive flag should be True for directory transfers; otherwise, the transfer is treated as a single file. Finally, local_path must always be given relative to the Job workdir . When direction=in , the local_path refers to the transfer destination . When direction=out , the local_path refers to the transfer source . This local_path behavior encourages a pattern where files in the working directory are always named identically, and only the remote sources and destinations vary. If you need to stage-in remote files without renaming them, a local_path value of . can be used. After running balsam app sync , the command balsam app ls --verbose will show any transfer slots registered for each of your apps. Cleanup Files \u00b6 In long-running data-intensive workflows, a Balsam site may exhaust its HPC storage allocation and trigger disk quota errors. To avoid this problem, valuable data products should be packaged and staged out, while intermediate files are periodically deleted to free storage space. The Site file_cleaner service can be enabled in settings.yml to safely remove files from working directories of finished jobs. Cleanup does not occur until a job reaches the JOB_FINISHED state, after all stage out tasks have completed. By default, the file_cleaner will not delete anything, even when it has been enabled. The ApplicationDefinition must also define a list of glob patterns in the cleanup_files attribute, for which matching files will be removed upon job completion. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\" cleanup_files = [ \"*.hdf\" , \"*.imm\" , \"*.h5\" ] Cleanup occurs once for each finished Job and reads the list of deletion patterns from the cleanup_files attribute in the ApplicationDefinition . Job Lifecycle Hooks \u00b6 The ApplicationDefinition class provides several hooks into stages of the Balsam Job lifecycle , in the form of overridable methods on the class. These methods are called by the Balsam Site as it handles your Jobs, advancing them from CREATED to JOB_FINISHED through a series of state transitions. To be more specific, an instance of the ApplicationDefinition class is created for each Job as it undergoes processing. The hooks are called as ordinary instance methods , where self refers to an ApplicationDefinition object handling a particular Job . The current Job can be accessed via the self.job attribute (see examples below). Of course, you may define any additional methods on the class and access them as usual. ApplicationDefinitions are not persistent! ApplicationDefinition instances are created and torn down after each invocation of a hook for a particular Job. This is because they might execute days or weeks apart on different physical hosts. Therefore, any data that you set on the self object within the hook will not persist. Instead, hooks can persist arbitrary JSON-serializable data on the Job object itself via self.job.data . Hook methods are always executed in the current Job 's working directory with stdout/stderr routed into the file balsam.log . All of the methods described below are optional : the default implementation is essentially a no-op that moves the Job state forward. However, if you do choose to override a lifecycle hook, it is your responsibility to set the Job state appropriately (e.g. you must write self.job.state = \"PREPROCESSED\" in the preprocess() function). The reason for this is that hooks may choose to retry or fail a particular state transition; the ApplicationDefinition should be the explicit source of truth on these possible actions. The Preprocess Hook \u00b6 The preprocess method advances jobs from STAGED_IN to PREPROCESSED . This represents an opportunity to run lightweight or I/O-bound code on the login node after any data for a Job has been staged in, and before the application begins executing. This runs in the processing service on the host where the Site Agent is running. In the following example, preprocess is used to read some user-defined data from the Job object, attempt to generate an input file, and advance the job state only if the generated input was valid. class MySimulation ( ApplicationDefinition ): def preprocess ( self ): # Can read anything from self.job.data coordinates = self . job . data [ \"input_coords\" ] # Run arbitrary methods defined on the class: success = self . generate_input ( coordinates ) # Advance the job state if success : # Ready to run self . job . state = \"PREPROCESSED\" else : # Fail the job and attach searchable data # to the failure event self . job . state = \"FAILED\" self . job . state_data = { \"error\" : \"Preproc got bad coordinates\" } The Shell Preamble \u00b6 The shell_preamble method can return a multi-line string or a list of strings , which are executed in an ephemeral bash shell immediately preceding the application launch command. This hook directly affects the environment of the mpirun (or equivalent) command used to launch each Job; therefore, it is appropriate for loading modules or exporting environment variables in an App- or Job-specific manner. Unlike preprocess , this hook is executed by the launcher (pilot job) on the application launch node. class MySimulation ( ApplicationDefinition ): def shell_preamble ( self ): return f ''' module load conda/tensorflow export FOO= { self . job . data [ \"env_vars\" ][ \"foo\" ] } ''' The Postprocess Hook \u00b6 The postprocess hook is exactly like the preprocess hook, except that it runs after Jobs have successfully executed. In Balsam a \"successful execution\" simply means the application command return code was 0 , and the job is advanced by the launcher from RUNNING to RUN_DONE . Some common patterns in the postprocess hook include: parsing output files summarizing/archiving useful data to be staged out persisting data on the job.data attribute dynamically creating additional Jobs to continue the workflow Upon successful postprocessing, the job state should be advanced to POSTPROCESSED . However, a return code of 0 does not necessarily imply a successful run. The method may therefore choose to set a job as FAILED (to halt further processing) or RESTART_READY (to run again, perhaps after changing some input). class MySimulation ( ApplicationDefinition ): def postprocess ( self ): with open ( \"out.hdf\" ) as fp : # Call your own result parser: results = self . parse_results ( fp ) if self . is_converged ( results ): self . job . state = \"POSTPROCESSED\" else : # Call your own input file fixer: self . fix_input () self . job . state = \"RESTART_READY\" Timeout Handler \u00b6 We have just seen how the postprocess hook handles the return code 0 scenario by moving jobs from RUN_DONE to POSTPROCESSED . There are two less happy scenarios that Balsam handles: The launcher wallclock time expired and the Job was terminated while still running. The launcher marks the job state as RUN_TIMEOUT . The application finished with a nonzero exit code. This is interpreted by the launcher as an error , and the job state is set to RUN_ERROR . The handle_timeout hook gives us an opportunity to manage timed-out jobs in the RUN_TIMEOUT state. The default Balsam action is to immediately mark the timed out job as RESTART_READY : it is simply eligible to run again as soon as resources are available. If you wish to fail the job or tweak inputs before running again, this is the right place to do it. In this example, we choose to mark the timed out job as FAILED but dynamically generate a follow-up job with related parameters. from balsam.api import Job class MySimulation ( ApplicationDefinition ): def handle_timeout ( self ): # Sorry, not retrying slow runs: self . job . state = \"FAILED\" self . job . state_data = { \"reason\" : \"Job Timed out\" } # Create another, faster run: new_job_params = self . next_run_kwargs () Job . objects . create ( ** new_job_params ) Error Handler \u00b6 The handle_error hook handles the second scenario listed in the previous section: when the job terminates with a nonzero exit code. If you can fix the error and try again, set the job state to RESTART_READY ; otherwise, the default implementation simply fails jobs that encountered a RUN_ERROR state. The following example calls some user-defined fix_inputs() to retry a failed run up to three times before declaring the job as FAILED . class MySimulation ( ApplicationDefinition ): def handle_error ( self ): dat = self . job . data retry_count = dat . get ( \"retry_count\" , 0 ) if retry_count <= 3 : self . fix_inputs () self . job . state = \"RESTART_READY\" self . job . data = { ** dat , \"retry_count\" : retry_count + 1 } else : self . job . state = \"FAILED\" self . job . state_data = { \"reason\" : \"Exceeded maximum retries\" } Be careful when updating job.data ! Notice in the example above that we did not simply update self.job.data[\"retry_count\"] , even though that's the only value that changed. Instead, we created a new dictionary merging the existing contents of data with the incremented value for retry_count . If we had attempted the former method, job.data would not have been updated . This is a subtle consequence of the Balsam Python API, which tracks mutated data on the Job object whenever a new value is assigned to one of the object's fields. This works great for immutable values, but unfortunately, updates to mutable fields (like appending to a list or setting a new key:value pair on a dictionary) are not currently intercepted. The Balsam processing service that runs these lifecycle hooks inspects mutations on each Job and propagates efficient bulk-updates to the REST API.","title":"Defining Apps"},{"location":"user-guide/appdef/#defining-applications","text":"","title":"Defining Applications"},{"location":"user-guide/appdef/#registering-applicationdefinitions","text":"Once you have a Site, the next step is to define the applications that Balsam may run. Each Site is linked to a set of ApplicationDefinition Python classes: check the Getting Started tutorial to see a quick example of this in action. You can create and register ApplicationDefinition subclasses from any Python session. For instance, Balsam supports interactive workflows in Jupyter notebooks, or workflows driven by any other Python software. At a minimum, ApplicationDefinitions must declare the site and a command_template for a shell command: from balsam.api import ApplicationDefinition class Sleeper ( ApplicationDefinition ): site = \"theta-gpu\" command_template = 'sleep {{ sleeptime }} && echo goodbye' Sleeper . sync () The site attribute must be present on each ApplicationDefinition subclass to identify where the app runs. The site can take any of the following types unambiguously specifying a Site: Site name (uniquely assigned during balsam site init , like \"theta-gpu\" ) Site ID (e.g. 142 ) Site object (fetched from the API ) The ApplicationDefinition is uniquely identified by its class name and site . In the above example, we defined the Sleeper application at the theta-gpu Site. When Sleeper.sync() is called, the Python class and associated metadata is serialized and shipped to the Balsam web API. The Sleeper ApplicationDefinition is thereafter linked to the Site named theta-gpu , and workflows can proceed to submit Sleeper Jobs to theta-gpu from anywhere! ApplicationDefinitions must be named uniquely! If another Python session syncs a different Sleeper class belonging to the same Site, the previous application will be overwritten! This is because apps are uniquely resolved by the pair ( site , class_name ). This is typically the desired behavior: simply running sync() will ensure that Balsam applications stay up-to-date with your source code. However, inadvertent name collisions can cause unexpected results when you overwrite the implementation of an existing ApplicationDefinition .","title":"Registering ApplicationDefinitions"},{"location":"user-guide/appdef/#the-submit-shortcut","text":"To run an application, we then submit a Job to invoke the command with specific resources and parameters. The submit() class method provides a convenient syntax to combine the sync initialization step with job submission in a single call: # Implicitly sync (updates or creates the App) and submit a Job: job = Sleeper . submit ( workdir = \"test/sleep_3\" , sleeptime = 3 ) This shorthand syntax is completely equivalent to the following: from balsam.api import Job Sleeper . sync () job = Job ( workdir = \"test/sleep_3\" , app_id = Sleeper , parameters = { \"sleeptime\" : 3 }) job . save () It's more efficient to use bulk-creation to submit large numbers of Jobs in a single network call. This is possible by passing the special keyword argument save=False : jobs = [ Sleeper . submit ( workdir = f \"test/ { i } \" , sleeptime = 3 , save = False ) for i in range ( 100 ) ] jobs = Job . objects . bulk_create ( jobs ) Besides the special save kwarg, The submit() method has the same signature as the Job() constructor which is covered in-depth on the next page .","title":"The submit() shortcut"},{"location":"user-guide/appdef/#python-applications","text":"Besides running shell commands, Balsam can run Python applications directly on the compute nodes. This paradigm significantly cuts down boilerplate and reduces the need for creating \"entry point\" scripts. Instead of using command_template , the ApplicationDefinition can simply define a run() method that will be launched using the exact same rules as ordinary shell applications. class Adder ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , x , y ): return x + y job = Adder . submit ( \"test/5plus5\" , x = 5 , y = 5 ) assert job . result () == 10 # This will block until a BatchJob starts run is an instance method and should take self as the first argument. Additional positional or keyword arguments can be supplied as well. When submitting Jobs , the parameters are serialized (under the hood job.parameters = {\"x\": 5, \"y\": 5} is converted to a Base64-encoded byte string with dill and stored as part of the Job in the Balsam web API.) The submitted Jobs behave partially like concurrent.futures.Future objects: namely, the return value or Exception raised by run() will propagate to the result() method. Refer to the Jobs documentation for more details on these Future -like APIs.","title":"Python Applications"},{"location":"user-guide/appdef/#python-app-capabilities-and-limitations","text":"Python run() function-based ApplicationDefinitions enjoy all the same lifecycle hooks and flexible resource launching capabilities as ordinary ApplicationDefinitions . For instance, your Balsam apps can directly call into mpi4py code and be launched onto multiple compute nodes: import numpy as np class NumpyPlusMPI ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , * dims ): from mpi4py import MPI if MPI . COMM_WORLD . Get_rank () == 0 : return np . random . rand ( * dims ) job = NumpyPlusMPI . submit ( workdir = \"test/parallel\" , dims = [ 5 ], ranks_per_node = 4 , ) The run function can generally refer to imported modules and objects from other namespaces, as long as they are importable in the Balsam Site's Python environment . This is a crucial constraint to understand when writing ApplicationDefinitions : all import statements are essentially replayed in the Balsam Site environment when the ApplicationDefinition is loaded and de-serialized. This will fail if any any object referenced by the ApplicationDefinition cannot be imported from sys.path ! Import statements will replay on the Balsam Site The issues above are easily avoided by ensuring that your codes and their dependencies are installed in the Python virtual environment where the Balsam Site runs. Just remember that the Balsam app serializer does not recurse and ship other modules over the wire. Virtually any import statement referenced by the ApplicationDefintion must work on both sides. The same constraint holds true when the ApplicationDefinitions themselves are located in an imported module. For example, if your Balsam code is packaged in my_science_package , that module/package must be installed in the Site environment where the ApplicationDefinition is synced to. # This import line must work on theta-gpu... from my_science_package import setup_workflow # If this method syncs apps to theta-gpu: setup_workflow ( site = \"theta-gpu\" ) Technical Details Under the hood, all ApplicationDefinitions , Job parameters, return values, and exceptions are serialized and deserialized using dill with the recurse=True option. Besides the issue of software dependencies, the following additional limitations should be kept in mind when writing ApplicationDefinitions : Do not use any multiprocessing.Process . You can spawn background Thread tasks, but Process workers will attempt to communicate using pickle in a fashion that is incompatible with the serialization scheme used by Balsam. Do not use the super() syntax anywhere in the ApplicationDefinition . This is a known issue with dill . If you are writing your own class hierarchies, you can always get around this by referencing the parent class directly. Avoid referring to balsam.api in the ApplicationDefinition . Instead, you will need to manually initialize the client to fetch resources as follows: from balsam.config import SiteConfig client = SiteConfig () . client # Loads a fresh RESTClient Job = client . Job # Use just like balsam.api.Job ApplicationDefinitions , parameters, return values, and exceptions should be lean. Do not rely on the Balsam API to move large quantities of data (instead, the Transfer API is designed for easy interoperability with out-of-band transfer methods like Globus). Balsam imposes limits on the order of 100KB for each serialized entity.","title":"Python App Capabilities and Limitations"},{"location":"user-guide/appdef/#listing-and-refreshing-applications","text":"You can check the Apps registered at a site using the CLI: # List apps across all Sites $ balsam app ls --site = all Restart Sites to Reload ApplicationDefinitions A new ApplicationDefinition will be automatically loaded by a running Site agent or launcher BatchJob. However, if you modify an existing app while it is loaded in a running Site or launcher, the change will not propagate! You must remember to run balsam site sync to refresh the loaded apps in a running Site.","title":"Listing and Refreshing Applications"},{"location":"user-guide/appdef/#loading-existing-applicationdefinitions","text":"You don't need a handle on the ApplicationDefinition source code to submit Jobs with it. Instead, the app_id argument to Job() can take an application by name (string), ID (integer), or reference to a loaded ApplicationDefinition . For example, you can load the set of applications registered at a given Site as follows: apps_by_name = ApplicationDefinition . load_by_site ( \"theta-gpu\" ) Sleeper = apps_by_name [ \"Sleeper\" ] ApplicationDefinitions can then be used from this dictionary as if you had defined them in the current Python session.","title":"Loading Existing ApplicationDefinitions"},{"location":"user-guide/appdef/#writing-applicationdefinitions","text":"At their simplest, ApplicationDefinitions provide a declarative template for a shell command and its adjustable parameters. Alternatively, they define a Python run() function that takes arbitrary inputs. To run an application, we submit a Job that provides values for these parameters. Importantly, we do not specify how the application is launched ( mpiexec ) or its CPU/GPU resources in the ApplicationDefinition . Instead, Balsam takes care of managing resources and building the command lines to efficiently launch our Jobs. Besides the fundamental site , command_template , and run attributes discussed above, ApplicationDefinitions provide other special attributes and methods that we can override to build more complex and useful workflow components.","title":"Writing ApplicationDefinitions"},{"location":"user-guide/appdef/#the-class-path","text":"Balsam Apps are uniquely identified by: The Site that they belong to Their ApplicationDefinition class __name__ For instance, the Sleeper application we defined above in test.py has a name of Sleeper . We use this to uniquely identify each ApplicationDefinition class later on.","title":"The Class Path"},{"location":"user-guide/appdef/#the-description","text":"The docstring that follows the class statement is captured by Balsam and stored as a description with the REST API. This is purely human-readable text that can be displayed in your App catalog. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\"","title":"The Description"},{"location":"user-guide/appdef/#the-site-identifier","text":"The site attribute is required on all ApplicationDefinitions and it must unambiguously refer to one of your existing Sites. This class attribute can be a string (site name), integer (site ID), or a Site object loaded from the Balsam SDK. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\"","title":"The Site Identifier"},{"location":"user-guide/appdef/#environment-variables","text":"The environment_variables attribute should be a Dict[str, str] mapping environment variable names to values. This is useful for constant environment variables that do not vary across runs. This environment is merged with the environment established in the job template. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , }","title":"Environment Variables"},{"location":"user-guide/appdef/#command-template","text":"As we have seen, ApplicationDefinitions must contain either a command_template or a run() method. These are mutually exclusive: you must set one or the other. The command_template is interpreted as a Jinja2 template; therefore, parameters must be enclosed in double-curly braces. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\" By default, all app parameters are required parameters: it is an error to omit any parameter named in the template. We can change this behavior below.","title":"Command Template"},{"location":"user-guide/appdef/#the-run-function","text":"When the ApplicationDefinition contains a run() method, this function is launched onto compute resources using the parameters set on the corresponding Job . import numpy as np class VecNorm ( ApplicationDefinition ): site = \"theta-gpu\" def run ( self , vec ): return np . linalg . norm ( vec )","title":"The run function"},{"location":"user-guide/appdef/#python-executable","text":"When using a run() function, it is important that the execution-side Python environment has the necessary dependencies installed. The optional class attribute python_exe defaults to sys.executable and should not be changed if the app runs in the same environment Balsam is installed in. You should override python_exe if you wish to invoke the run function using a different Python environment from the one in which Balsam is installed. This setting has no effect for command_template apps. import numpy as np class VecNorm ( ApplicationDefinition ): site = \"theta-gpu\" python_exe = \"/path/to/bin/python3.8\" def run ( self , vec ): import numpy as np # Loaded from `python_exe` return np . linalg . norm ( vec )","title":"Python executable"},{"location":"user-guide/appdef/#parameter-spec","text":"Maybe we want to have some optional parameters in the command_template , which take on a default value in the absence of a value specified in the Job. We can do this by providing the parameters dictionary: class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/sim.exe --mode {{ mode }} -inp {{ input_filename }}\" parameters = { \"input_filename\" : { \"required\" : True }, \"mode\" : { \"required\" : False , \"default\" : \"explore\" , \"help\" : \"The simulation mode (default: explore)\" , } } Notice that parameters are either required, in which case it doesn't make sense to have a default value, or not. If a parameter's required value is False , you must provide a default value that is used when the parameter is not passed. The help field is another optional, human-readable field, to assist with App curation in the Web interface. Valid Python Identifiers App parameters can only contain valid Python identifiers, so names with - , for instance, will be rejected when you attempt to run balsam app sync .","title":"Parameter Spec"},{"location":"user-guide/appdef/#transfer-slots","text":"A core feature of Balsam, described in more detail in the Data Transfers section , is the ability to write distributed workflows, where data products move between Sites, and Jobs can be triggered when data arrives at its destination. We create this behavior starting at the ApplicationDefinition level, by defining Transfer Slots for data that needs to be staged in before or staged out after execution. You can think of the Job workdir as an ephemeral sandbox where data arrives, computation happens, and then results are staged out to a more accessible location for further analysis. Each ApplicationDefinition may declare a transfers dictionary, where each string key names a Transfer Slot. class MySimulation ( ApplicationDefinition ): transfers = { \"input_file\" : { \"required\" : True , \"direction\" : \"in\" , \"local_path\" : \"input.nw\" , \"description\" : \"Input Deck\" , \"recursive\" : False , }, \"result\" : { \"required\" : True , \"direction\" : \"out\" , \"local_path\" : \"job.out\" , \"description\" : \"Calculation stdout\" , \"recursive\" : False }, }, In order to fill the slots, each Job invoking this application must then provide concrete URIs of the external files: Job . objects . create ( workdir = \"ensemble/1\" , app_name = \"sim.MySimulation\" , transfers = { # Using 'laptop' alias defined in settings.yml \"input_file\" : \"laptop:/path/to/input.dat\" , \"result\" : \"laptop:/path/to/output.json\" , }, ) Transfer slots with required=False are optional when creating Jobs. The direction key must contain the value \"in\" or \"out\" for stage-in and stage-out, respectively. The description is an optional, human-readable parameter to assist in App curation. The recursive flag should be True for directory transfers; otherwise, the transfer is treated as a single file. Finally, local_path must always be given relative to the Job workdir . When direction=in , the local_path refers to the transfer destination . When direction=out , the local_path refers to the transfer source . This local_path behavior encourages a pattern where files in the working directory are always named identically, and only the remote sources and destinations vary. If you need to stage-in remote files without renaming them, a local_path value of . can be used. After running balsam app sync , the command balsam app ls --verbose will show any transfer slots registered for each of your apps.","title":"Transfer Slots"},{"location":"user-guide/appdef/#cleanup-files","text":"In long-running data-intensive workflows, a Balsam site may exhaust its HPC storage allocation and trigger disk quota errors. To avoid this problem, valuable data products should be packaged and staged out, while intermediate files are periodically deleted to free storage space. The Site file_cleaner service can be enabled in settings.yml to safely remove files from working directories of finished jobs. Cleanup does not occur until a job reaches the JOB_FINISHED state, after all stage out tasks have completed. By default, the file_cleaner will not delete anything, even when it has been enabled. The ApplicationDefinition must also define a list of glob patterns in the cleanup_files attribute, for which matching files will be removed upon job completion. class MySimulation ( ApplicationDefinition ): \"\"\" Some description of the app goes here \"\"\" site = \"theta-gpu\" environment_variables = { \"HDF5_USE_FILE_LOCKING\" : \"FALSE\" , } command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\" cleanup_files = [ \"*.hdf\" , \"*.imm\" , \"*.h5\" ] Cleanup occurs once for each finished Job and reads the list of deletion patterns from the cleanup_files attribute in the ApplicationDefinition .","title":"Cleanup Files"},{"location":"user-guide/appdef/#job-lifecycle-hooks","text":"The ApplicationDefinition class provides several hooks into stages of the Balsam Job lifecycle , in the form of overridable methods on the class. These methods are called by the Balsam Site as it handles your Jobs, advancing them from CREATED to JOB_FINISHED through a series of state transitions. To be more specific, an instance of the ApplicationDefinition class is created for each Job as it undergoes processing. The hooks are called as ordinary instance methods , where self refers to an ApplicationDefinition object handling a particular Job . The current Job can be accessed via the self.job attribute (see examples below). Of course, you may define any additional methods on the class and access them as usual. ApplicationDefinitions are not persistent! ApplicationDefinition instances are created and torn down after each invocation of a hook for a particular Job. This is because they might execute days or weeks apart on different physical hosts. Therefore, any data that you set on the self object within the hook will not persist. Instead, hooks can persist arbitrary JSON-serializable data on the Job object itself via self.job.data . Hook methods are always executed in the current Job 's working directory with stdout/stderr routed into the file balsam.log . All of the methods described below are optional : the default implementation is essentially a no-op that moves the Job state forward. However, if you do choose to override a lifecycle hook, it is your responsibility to set the Job state appropriately (e.g. you must write self.job.state = \"PREPROCESSED\" in the preprocess() function). The reason for this is that hooks may choose to retry or fail a particular state transition; the ApplicationDefinition should be the explicit source of truth on these possible actions.","title":"Job Lifecycle Hooks"},{"location":"user-guide/appdef/#the-preprocess-hook","text":"The preprocess method advances jobs from STAGED_IN to PREPROCESSED . This represents an opportunity to run lightweight or I/O-bound code on the login node after any data for a Job has been staged in, and before the application begins executing. This runs in the processing service on the host where the Site Agent is running. In the following example, preprocess is used to read some user-defined data from the Job object, attempt to generate an input file, and advance the job state only if the generated input was valid. class MySimulation ( ApplicationDefinition ): def preprocess ( self ): # Can read anything from self.job.data coordinates = self . job . data [ \"input_coords\" ] # Run arbitrary methods defined on the class: success = self . generate_input ( coordinates ) # Advance the job state if success : # Ready to run self . job . state = \"PREPROCESSED\" else : # Fail the job and attach searchable data # to the failure event self . job . state = \"FAILED\" self . job . state_data = { \"error\" : \"Preproc got bad coordinates\" }","title":"The Preprocess Hook"},{"location":"user-guide/appdef/#the-shell-preamble","text":"The shell_preamble method can return a multi-line string or a list of strings , which are executed in an ephemeral bash shell immediately preceding the application launch command. This hook directly affects the environment of the mpirun (or equivalent) command used to launch each Job; therefore, it is appropriate for loading modules or exporting environment variables in an App- or Job-specific manner. Unlike preprocess , this hook is executed by the launcher (pilot job) on the application launch node. class MySimulation ( ApplicationDefinition ): def shell_preamble ( self ): return f ''' module load conda/tensorflow export FOO= { self . job . data [ \"env_vars\" ][ \"foo\" ] } '''","title":"The Shell Preamble"},{"location":"user-guide/appdef/#the-postprocess-hook","text":"The postprocess hook is exactly like the preprocess hook, except that it runs after Jobs have successfully executed. In Balsam a \"successful execution\" simply means the application command return code was 0 , and the job is advanced by the launcher from RUNNING to RUN_DONE . Some common patterns in the postprocess hook include: parsing output files summarizing/archiving useful data to be staged out persisting data on the job.data attribute dynamically creating additional Jobs to continue the workflow Upon successful postprocessing, the job state should be advanced to POSTPROCESSED . However, a return code of 0 does not necessarily imply a successful run. The method may therefore choose to set a job as FAILED (to halt further processing) or RESTART_READY (to run again, perhaps after changing some input). class MySimulation ( ApplicationDefinition ): def postprocess ( self ): with open ( \"out.hdf\" ) as fp : # Call your own result parser: results = self . parse_results ( fp ) if self . is_converged ( results ): self . job . state = \"POSTPROCESSED\" else : # Call your own input file fixer: self . fix_input () self . job . state = \"RESTART_READY\"","title":"The Postprocess Hook"},{"location":"user-guide/appdef/#timeout-handler","text":"We have just seen how the postprocess hook handles the return code 0 scenario by moving jobs from RUN_DONE to POSTPROCESSED . There are two less happy scenarios that Balsam handles: The launcher wallclock time expired and the Job was terminated while still running. The launcher marks the job state as RUN_TIMEOUT . The application finished with a nonzero exit code. This is interpreted by the launcher as an error , and the job state is set to RUN_ERROR . The handle_timeout hook gives us an opportunity to manage timed-out jobs in the RUN_TIMEOUT state. The default Balsam action is to immediately mark the timed out job as RESTART_READY : it is simply eligible to run again as soon as resources are available. If you wish to fail the job or tweak inputs before running again, this is the right place to do it. In this example, we choose to mark the timed out job as FAILED but dynamically generate a follow-up job with related parameters. from balsam.api import Job class MySimulation ( ApplicationDefinition ): def handle_timeout ( self ): # Sorry, not retrying slow runs: self . job . state = \"FAILED\" self . job . state_data = { \"reason\" : \"Job Timed out\" } # Create another, faster run: new_job_params = self . next_run_kwargs () Job . objects . create ( ** new_job_params )","title":"Timeout Handler"},{"location":"user-guide/appdef/#error-handler","text":"The handle_error hook handles the second scenario listed in the previous section: when the job terminates with a nonzero exit code. If you can fix the error and try again, set the job state to RESTART_READY ; otherwise, the default implementation simply fails jobs that encountered a RUN_ERROR state. The following example calls some user-defined fix_inputs() to retry a failed run up to three times before declaring the job as FAILED . class MySimulation ( ApplicationDefinition ): def handle_error ( self ): dat = self . job . data retry_count = dat . get ( \"retry_count\" , 0 ) if retry_count <= 3 : self . fix_inputs () self . job . state = \"RESTART_READY\" self . job . data = { ** dat , \"retry_count\" : retry_count + 1 } else : self . job . state = \"FAILED\" self . job . state_data = { \"reason\" : \"Exceeded maximum retries\" } Be careful when updating job.data ! Notice in the example above that we did not simply update self.job.data[\"retry_count\"] , even though that's the only value that changed. Instead, we created a new dictionary merging the existing contents of data with the incremented value for retry_count . If we had attempted the former method, job.data would not have been updated . This is a subtle consequence of the Balsam Python API, which tracks mutated data on the Job object whenever a new value is assigned to one of the object's fields. This works great for immutable values, but unfortunately, updates to mutable fields (like appending to a list or setting a new key:value pair on a dictionary) are not currently intercepted. The Balsam processing service that runs these lifecycle hooks inspects mutations on each Job and propagates efficient bulk-updates to the REST API.","title":"Error Handler"},{"location":"user-guide/batchjob/","text":"Scheduling Launchers \u00b6 Unless Auto Scaling is enabled, Jobs do not automatically run in Balsam. After the Site agent completes data staging and preprocessing for a Job, it waits in the PREPROCESSED state until a launcher (pilot job) submitted to the HPC batch scheduler takes over. Launchers run independently of the Site agent, continuously fetching and executing runnable Jobs on the available compute resources. They track occupancy of the allocated CPUs/GPUs while launching Jobs with the requested resources. Because launchers acquire Jobs on-the-fly, you can submit Jobs to any system, and they will execute in real-time on an existing allocation! The Balsam launcher thus handles the compute-intensive core of the Job lifecycle: from PREPROCESSED to RUNNING to RUN_DONE . As users, we need only submit a BatchJob to any one of our Sites. The BatchJob represents an HPC resource allocation as a fixed block of node-hours. Sites will handle new BatchJobs by generating the script to run a launcher and submitting it to the local HPC scheduler. Using the CLI \u00b6 If we are inside a Site directory, we can submit a BatchJob to that Site from the CLI: $ balsam queue submit -q QUEUE -A PROJECT -n 128 -t 70 -j mpi The CLI works remotely, too: we just need to target a specific --site so Balsam knows where you want things to run: $ balsam queue submit --site = my-site -q QUEUE -A PROJECT -n 128 -t 70 -j mpi Balsam will perform the appropriate submission to the underlying HPC scheduler and synchronize your BatchJobs with the local scheduler state. Therefore, instead of checking queues locally (e.g. qstat ), we can check on BatchJobs across all of our Sites with a simple command: $ balsam queue ls --site = all Systems without a batch scheduler Use -n 1 -q local -A local when creating BatchJobs at generic MacOS/Linux Sites without a real batch scheduler or multiple nodes. The OS process manager takes the place of the resource manager, but everything else looks the same! Selecting a Launcher Job Mode \u00b6 All of the queue submit options pass through to the usual scheduler interface (like sbatch or qsub ), except for the -j/--job-mode flag, which may be either mpi or serial . These refer to the launcher job modes , which determines the pilot job implementation that will actually run. mpi mode is the most flexible and should be preferred unless you have a particularly extreme throughput requirement or need to use one of the workarounds offered by serial mode. The mpi launcher runs on the head-node of the BatchJob and executes each job using the system's MPI launch command (e.g. srun or mpirun ). serial mode only handles single-process (non-distributed memory) Jobs that run within a single compute node. Higher throughput of fine-grained tasks (e.g. millions of single-core tasks) is achieved by running a worker process on each compute node and fanning out cached Jobs acquired from the REST API. Both launcher modes can simultaneously execute multiple applications per node, as long as the underlying HPC system provides support. This is not always the case: for example, on ALCF's Theta-KNL system, serial mode is required to pack multiple runs per node. You can submit multiple BatchJobs to a Site Balsam launchers cooperatively divide and conquer the runnable Jobs at a Site. You may therefore choose between queueing up fewer large BatchJobs or several smaller BatchJobs simultaneously. On a busy HPC cluster, smaller BatchJobs can get through the queues faster and improve overall throughput. Using the API \u00b6 A unique capability of the Balsam Python API is that it allows us programmatically manage HPC resources (via BatchJob ) and tasks (via Job ) on equal footing. We can submit and monitor Jobs and BatchJobs at any Site with ease, using a single, consistent programming model. from balsam.api import Job , BatchJob # Create Jobs: job = Job . objects . create ( site_name = \"myProject-theta-gpu\" , app_id = \"SimulationX\" , workdir = \"test-runs/foo/1\" , ) # Or allocate resources: BatchJob . objects . create ( site_id = job . site_id , num_nodes = 1 , wall_time_min = 20 , job_mode = \"mpi\" , project = \"datascience\" , queue = \"full-node\" , ) We can query BatchJobs to track how many resources are currently available or waiting in the queue at each Site: queued_nodes = sum ( batch_job . num_nodes for batch_job in BatchJob . objects . filter ( site_id = 123 , state = \"queued\" ) ) Or we can instruct Balsam to cleanly terminate an allocation: BatchJob . objects . filter ( scheduler_id = 1234 ) . update ( state = \"pending_deletion\" ) Jobs running in that BatchJob will be marked RUN_TIMEOUT and handled by their respective Apps' handle_timeout hooks. Job Templates and specialized parameters \u00b6 Behind the scenes, each BatchJob materializes as a batch job script rendered from the Site's job template . These templates can be customized to support new scheduler flags, load global modules, or perform general pre-execution logic. These templates also accept optional, system-specific parameters that can be passed on the CLI via -x or to the BatchJob optional_params dictionary. Theta-KNL Optional Params \u00b6 On Theta-KNL, we can prime the LDAP cache on each compute node prior to a large-scale ensemble of Singularity jobs. This is necessary to avoid a system error that arises in Singularity startup at scale. With the CLI: $ balsam queue submit -x singularity_prime_cache = yes # ..other args With the Python API: BatchJob . objects . create ( # ...other kwargs optional_params = { \"singularity_prime_cache\" : \"yes\" } ) ThetaGPU \u00b6 On Theta-GPU, we can partition each of the 8 physical A100 GPUs into 2, 3, or 7 Multi-Instance GPU (MIG) resources. This allows us to achieve higher GPU utilization with high-throughput tasks consuming a fraction of the 40 GB device memory. Jobs using a MIG instance should still request a single logical GPU with gpus_per_rank=1 but specify a higher node-packing (e.g. node_packing_count should be 8*3 = 24 for a 3-way MIG partitioning). With the CLI: $ balsam queue submit -x mig_count = 3 # ..other args With the Python API: BatchJob . objects . create ( # ...other kwargs optional_params = { \"mig_count\" : \"3\" } ) Restricting BatchJobs with tags \u00b6 We strongly encourage the use of descriptive tags to facilitate monitoring Jobs. Another major use of tags is to restrict which Jobs can run in a given BatchJob. The default behavior is that a BatchJob will run as many Jobs as possible : Balsam decides what runs in any given allocation. But perhaps we wish to prioritize a certain group of runs, or deliberately run Jobs in separate partitions as part of a scalability study. This is easy with the CLI: # Only run jobs with tags system=H2O and scale=4 $ balsam queue submit -n 4 --tag system = H2O --tag scale = 4 # ...other args Or with the API: BatchJob . objects . create ( # ...other kwargs num_nodes = 4 , filter_tags = { \"system\" : \"H2O\" , \"scale\" : \"4\" } ) Partitioning BatchJobs \u00b6 By default, a single launcher process manages the entire allocation of compute nodes with a single job mode of either mpi or serial . In advanced use-cases, we can actually divide a single queue submission/allocation into multiple launcher partitions. Each of this partitions can have its own number of compute nodes, job mode, and filter tags. This can be useful in different contexts: Dividing an allocation to run a mixed workload of MPI applications and high-throughput sub-node applications. Improving scalability to large node counts by parallelizing the job launcher. We can request that a BatchJob is split into partitions on the CLI. In this example, we split a 128-node allocation into a 2-node MPI launcher (to run some \"leader\" MPI app on 2 nodes), while the remaining 126 nodes are managed by the efficient serial mode launcher for high-throughput. $ balsam queue submit -n 128 -p mpi:2 -p serial:126 # ...other args We could also apply tag restrictions to ensure that the right Jobs run in the right partition: $ balsam queue submit -n 128 -p mpi:2:role = leader -p serial:126:role = worker # ...other args With the Python API, this looks like: BatchJob . objects . create ( # ...other kwargs num_nodes = 128 , partitions = [ { \"job_mode\" : \"mpi\" , \"num_nodes\" : 2 , \"filter_tags\" : { \"role\" : \"leader\" }}, { \"job_mode\" : \"serial\" , \"num_nodes\" : 126 , \"filter_tags\" : { \"role\" : \"worker\" }}, ] )","title":"Executing Jobs"},{"location":"user-guide/batchjob/#scheduling-launchers","text":"Unless Auto Scaling is enabled, Jobs do not automatically run in Balsam. After the Site agent completes data staging and preprocessing for a Job, it waits in the PREPROCESSED state until a launcher (pilot job) submitted to the HPC batch scheduler takes over. Launchers run independently of the Site agent, continuously fetching and executing runnable Jobs on the available compute resources. They track occupancy of the allocated CPUs/GPUs while launching Jobs with the requested resources. Because launchers acquire Jobs on-the-fly, you can submit Jobs to any system, and they will execute in real-time on an existing allocation! The Balsam launcher thus handles the compute-intensive core of the Job lifecycle: from PREPROCESSED to RUNNING to RUN_DONE . As users, we need only submit a BatchJob to any one of our Sites. The BatchJob represents an HPC resource allocation as a fixed block of node-hours. Sites will handle new BatchJobs by generating the script to run a launcher and submitting it to the local HPC scheduler.","title":"Scheduling Launchers"},{"location":"user-guide/batchjob/#using-the-cli","text":"If we are inside a Site directory, we can submit a BatchJob to that Site from the CLI: $ balsam queue submit -q QUEUE -A PROJECT -n 128 -t 70 -j mpi The CLI works remotely, too: we just need to target a specific --site so Balsam knows where you want things to run: $ balsam queue submit --site = my-site -q QUEUE -A PROJECT -n 128 -t 70 -j mpi Balsam will perform the appropriate submission to the underlying HPC scheduler and synchronize your BatchJobs with the local scheduler state. Therefore, instead of checking queues locally (e.g. qstat ), we can check on BatchJobs across all of our Sites with a simple command: $ balsam queue ls --site = all Systems without a batch scheduler Use -n 1 -q local -A local when creating BatchJobs at generic MacOS/Linux Sites without a real batch scheduler or multiple nodes. The OS process manager takes the place of the resource manager, but everything else looks the same!","title":"Using the CLI"},{"location":"user-guide/batchjob/#selecting-a-launcher-job-mode","text":"All of the queue submit options pass through to the usual scheduler interface (like sbatch or qsub ), except for the -j/--job-mode flag, which may be either mpi or serial . These refer to the launcher job modes , which determines the pilot job implementation that will actually run. mpi mode is the most flexible and should be preferred unless you have a particularly extreme throughput requirement or need to use one of the workarounds offered by serial mode. The mpi launcher runs on the head-node of the BatchJob and executes each job using the system's MPI launch command (e.g. srun or mpirun ). serial mode only handles single-process (non-distributed memory) Jobs that run within a single compute node. Higher throughput of fine-grained tasks (e.g. millions of single-core tasks) is achieved by running a worker process on each compute node and fanning out cached Jobs acquired from the REST API. Both launcher modes can simultaneously execute multiple applications per node, as long as the underlying HPC system provides support. This is not always the case: for example, on ALCF's Theta-KNL system, serial mode is required to pack multiple runs per node. You can submit multiple BatchJobs to a Site Balsam launchers cooperatively divide and conquer the runnable Jobs at a Site. You may therefore choose between queueing up fewer large BatchJobs or several smaller BatchJobs simultaneously. On a busy HPC cluster, smaller BatchJobs can get through the queues faster and improve overall throughput.","title":"Selecting a Launcher Job Mode"},{"location":"user-guide/batchjob/#using-the-api","text":"A unique capability of the Balsam Python API is that it allows us programmatically manage HPC resources (via BatchJob ) and tasks (via Job ) on equal footing. We can submit and monitor Jobs and BatchJobs at any Site with ease, using a single, consistent programming model. from balsam.api import Job , BatchJob # Create Jobs: job = Job . objects . create ( site_name = \"myProject-theta-gpu\" , app_id = \"SimulationX\" , workdir = \"test-runs/foo/1\" , ) # Or allocate resources: BatchJob . objects . create ( site_id = job . site_id , num_nodes = 1 , wall_time_min = 20 , job_mode = \"mpi\" , project = \"datascience\" , queue = \"full-node\" , ) We can query BatchJobs to track how many resources are currently available or waiting in the queue at each Site: queued_nodes = sum ( batch_job . num_nodes for batch_job in BatchJob . objects . filter ( site_id = 123 , state = \"queued\" ) ) Or we can instruct Balsam to cleanly terminate an allocation: BatchJob . objects . filter ( scheduler_id = 1234 ) . update ( state = \"pending_deletion\" ) Jobs running in that BatchJob will be marked RUN_TIMEOUT and handled by their respective Apps' handle_timeout hooks.","title":"Using the API"},{"location":"user-guide/batchjob/#job-templates-and-specialized-parameters","text":"Behind the scenes, each BatchJob materializes as a batch job script rendered from the Site's job template . These templates can be customized to support new scheduler flags, load global modules, or perform general pre-execution logic. These templates also accept optional, system-specific parameters that can be passed on the CLI via -x or to the BatchJob optional_params dictionary.","title":"Job Templates and specialized parameters"},{"location":"user-guide/batchjob/#theta-knl-optional-params","text":"On Theta-KNL, we can prime the LDAP cache on each compute node prior to a large-scale ensemble of Singularity jobs. This is necessary to avoid a system error that arises in Singularity startup at scale. With the CLI: $ balsam queue submit -x singularity_prime_cache = yes # ..other args With the Python API: BatchJob . objects . create ( # ...other kwargs optional_params = { \"singularity_prime_cache\" : \"yes\" } )","title":"Theta-KNL Optional Params"},{"location":"user-guide/batchjob/#thetagpu","text":"On Theta-GPU, we can partition each of the 8 physical A100 GPUs into 2, 3, or 7 Multi-Instance GPU (MIG) resources. This allows us to achieve higher GPU utilization with high-throughput tasks consuming a fraction of the 40 GB device memory. Jobs using a MIG instance should still request a single logical GPU with gpus_per_rank=1 but specify a higher node-packing (e.g. node_packing_count should be 8*3 = 24 for a 3-way MIG partitioning). With the CLI: $ balsam queue submit -x mig_count = 3 # ..other args With the Python API: BatchJob . objects . create ( # ...other kwargs optional_params = { \"mig_count\" : \"3\" } )","title":"ThetaGPU"},{"location":"user-guide/batchjob/#restricting-batchjobs-with-tags","text":"We strongly encourage the use of descriptive tags to facilitate monitoring Jobs. Another major use of tags is to restrict which Jobs can run in a given BatchJob. The default behavior is that a BatchJob will run as many Jobs as possible : Balsam decides what runs in any given allocation. But perhaps we wish to prioritize a certain group of runs, or deliberately run Jobs in separate partitions as part of a scalability study. This is easy with the CLI: # Only run jobs with tags system=H2O and scale=4 $ balsam queue submit -n 4 --tag system = H2O --tag scale = 4 # ...other args Or with the API: BatchJob . objects . create ( # ...other kwargs num_nodes = 4 , filter_tags = { \"system\" : \"H2O\" , \"scale\" : \"4\" } )","title":"Restricting BatchJobs with tags"},{"location":"user-guide/batchjob/#partitioning-batchjobs","text":"By default, a single launcher process manages the entire allocation of compute nodes with a single job mode of either mpi or serial . In advanced use-cases, we can actually divide a single queue submission/allocation into multiple launcher partitions. Each of this partitions can have its own number of compute nodes, job mode, and filter tags. This can be useful in different contexts: Dividing an allocation to run a mixed workload of MPI applications and high-throughput sub-node applications. Improving scalability to large node counts by parallelizing the job launcher. We can request that a BatchJob is split into partitions on the CLI. In this example, we split a 128-node allocation into a 2-node MPI launcher (to run some \"leader\" MPI app on 2 nodes), while the remaining 126 nodes are managed by the efficient serial mode launcher for high-throughput. $ balsam queue submit -n 128 -p mpi:2 -p serial:126 # ...other args We could also apply tag restrictions to ensure that the right Jobs run in the right partition: $ balsam queue submit -n 128 -p mpi:2:role = leader -p serial:126:role = worker # ...other args With the Python API, this looks like: BatchJob . objects . create ( # ...other kwargs num_nodes = 128 , partitions = [ { \"job_mode\" : \"mpi\" , \"num_nodes\" : 2 , \"filter_tags\" : { \"role\" : \"leader\" }}, { \"job_mode\" : \"serial\" , \"num_nodes\" : 126 , \"filter_tags\" : { \"role\" : \"worker\" }}, ] )","title":"Partitioning BatchJobs"},{"location":"user-guide/cli/","text":"The Command Line Interface (CLI) \u00b6 When installed, Balsam provides a balsam command line tool for convenient, shell-based management of your Sites, Apps, Jobs, and BatchJobs. The CLI comprises recursively-documented subcommands: use the --help flag at any level to view example usage: # See all commands: $ balsam --help # See all job-related commands: $ balsam job --help # See details of CLI job creation: $ balsam job create --help The Site Selector \u00b6 A key feature of the CLI (and underlying API) is that it works across sites : you might, for example, query and submit jobs to three HPC facilities from a single shell running on your laptop. This raises a namespacing concern: how should the CLI commands target different Sites? To answer this question, the Balsam CLI is somewhat context-aware. When you are logged into the machine and inside of a Site directory, the current site is automatically inferred. Thus commands like balsam job ls will filter the visible Jobs to only those in the current Site. Likewise, commands like balsam job create or balsam queue submit will infer the ID of the Site for which you are trying to add a Job or BatchJob. If you are outside of a Site directory, the CLI instead limits queries to all currently active Sites. For example, balsam job ls will list Jobs across all your Sites that have an Agent process currently running. We can override this context-dependent behavior by explicitly passing the --site selector argument as follows: $ balsam job ls --site = all # Show jobs at ALL sites $ balsam job ls --site = this # Only show jobs at the current site $ balsam job ls --site = active # Show jobs at active sites only Moreover, the --site selector can provide a comma-separated list of Site IDs or Site Path fragments: $ balsam job ls --site = 123 ,125 $ balsam job ls --site = myFolder/siteX,siteY,123 When you are creating a Job or enqueueing a BatchJob, the --site selector must unambiguously narrow down to a single Site. In this case, use a single numeric Site ID (as shown in balsam site ls ) or a unique substring of the Site path. $ balsam queue submit --site = 123 # ... $ balsam queue submit --site = unique_name # ...","title":"The Command Line"},{"location":"user-guide/cli/#the-command-line-interface-cli","text":"When installed, Balsam provides a balsam command line tool for convenient, shell-based management of your Sites, Apps, Jobs, and BatchJobs. The CLI comprises recursively-documented subcommands: use the --help flag at any level to view example usage: # See all commands: $ balsam --help # See all job-related commands: $ balsam job --help # See details of CLI job creation: $ balsam job create --help","title":"The Command Line Interface (CLI)"},{"location":"user-guide/cli/#the-site-selector","text":"A key feature of the CLI (and underlying API) is that it works across sites : you might, for example, query and submit jobs to three HPC facilities from a single shell running on your laptop. This raises a namespacing concern: how should the CLI commands target different Sites? To answer this question, the Balsam CLI is somewhat context-aware. When you are logged into the machine and inside of a Site directory, the current site is automatically inferred. Thus commands like balsam job ls will filter the visible Jobs to only those in the current Site. Likewise, commands like balsam job create or balsam queue submit will infer the ID of the Site for which you are trying to add a Job or BatchJob. If you are outside of a Site directory, the CLI instead limits queries to all currently active Sites. For example, balsam job ls will list Jobs across all your Sites that have an Agent process currently running. We can override this context-dependent behavior by explicitly passing the --site selector argument as follows: $ balsam job ls --site = all # Show jobs at ALL sites $ balsam job ls --site = this # Only show jobs at the current site $ balsam job ls --site = active # Show jobs at active sites only Moreover, the --site selector can provide a comma-separated list of Site IDs or Site Path fragments: $ balsam job ls --site = 123 ,125 $ balsam job ls --site = myFolder/siteX,siteY,123 When you are creating a Job or enqueueing a BatchJob, the --site selector must unambiguously narrow down to a single Site. In this case, use a single numeric Site ID (as shown in balsam site ls ) or a unique substring of the Site path. $ balsam queue submit --site = 123 # ... $ balsam queue submit --site = unique_name # ...","title":"The Site Selector"},{"location":"user-guide/elastic/","text":"Auto-scaling Resources with Balsam Elastic Queue \u00b6 A Balsam Site does not automatically use your computing allocation by default. Instead, we must use the BatchJob API or balsam queue CLI to explicitly request compute nodes at a given Balsam Site. This follows the principle of least surprise: as the User, you explicitly decide when and how resources are spent. This is not a limitation since we can remotely submit BatchJobs to any Site from a computer where Balsam is installed! However, we can opt-in to enable the elastic_queue plugin that creates BatchJob submissions on our behalf. This can be a useful service in bursty or real-time workloads: instead of micro-managing the queues, we simply submit a stream of Jobs and allow the Site to provision resources as needed over time. Enabling the Elastic Queue Plugin \u00b6 Auto-scaling is enabled at a Site by setting the elastic_queue configuration appropriately inside of the settings.yml file. You should find this line uncommented by default: elastic_queue : null Following this line is a commented-block showing an example elastic_queue configuration. We want to comment out the elastic_queue: null line and uncomment the configuration; setting each of the parameters appropriately for our use case. Once the Plugin has been configured properly (see below), we must restart the Balsam Site Agent to load it: $ balsam site sync # Or if the Site isn't already running: $ balsam site start Disabling the Elastic Queue Plugin \u00b6 To disable, comment out (or delete) the current elastic_queue configuration in settings.yml and replace it with the line: elastic_queue : null Then restart the Balsam Site to run without the elastic queue plugin: $ balsam site sync Configuring the Elastic Queue \u00b6 The configuration is fairly flexible to enable a wide range of use cases. This section explains the YAML configuration in chunks. Project, Queue, and Submit Frequency \u00b6 Firstly, service_period controls the waiting period (in seconds) between cycles in which a new BatchJob might be submitted to the queue. The submit_project , submit_queue , and job_mode are directly passed through to the new BatchJob . The max_queue_wait_time_min determines how long a submitted BatchJob should be enqueued before the elastic queue deletes it and tries re-submitting. When using backfill to grab idle nodes (see next section), it makes sense to set a relatively short waiting time of 5-10 minutes. Otherwise, this duration should be increased to a reasonable upper threshold to avoid deleting BatchJobs that have accrued priority in the queues. The elastic queue will maintain up to max_queued_jobs in the queue at any given time. This should be set to the maximum desired (or allowed) number of simultaneously queued/running BatchJobs at the Site. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20 Wall Time and Backfilling \u00b6 Many HPC systems use backfilling schedulers, which attempt to place small Jobs while draining nodes for larger Jobs to start up at a determined future time. By opportunistically sizing jobs to fit into these idle node-hour windows, Balsam effectively \"fills the gaps\" in unused resources. We enable this dynamic sizing with use_backfill: True . The interpretation of min_wall_time_min and max_wall_time_min depends on whether or not use_backfill is enabled: When use_backfill is False : min_wall_time_min is ignored and BatchJobs are submitted for a constant wallclock time limit of max_wall_time_min . When use_backfill is True : Balsam selects backfill windows that are at least as long as min_wall_time_min (this is to avoid futile 5 minute submissions when all Jobs take at least 30 minutes). The wallclock time limit is then the lesser of the scheduler's backfill duration and max_wall_time_min . Finally, a \"padding\" value of wall_time_pad_min is subtracted from the final wallclock time in all BatchJob submissions. This should be set to a couple minutes when use_backfill is True and 0 otherwise. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20 Node Count \u00b6 Finally, the min_num_nodes and max_num_nodes determine the permissible range of node counts in submitted BatchJobs . When operating with the use_backfill=True constraint, backfill windows smaller than min_num_nodes will be ignored. Otherwise, BatchJob submissions use min_num_nodes as a lower bound. Likewise, max_num_nodes gives an upper bound on the BatchJob's node count. The actual submitted BatchJob node count falls somewhere in this range. It is determined from the difference between how many nodes are currently requested (queued or running BatchJobs) and the aggregate node footprint of all runnable Jobs. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20 Therefore, the elastic queue automatically controls the size and number of requested BatchJobs as the workload grows. We can think of each BatchJob as a flexibly-sized block of resources, and the elastic queue creates multiple blocks (one per service_period ) while choosing their sizes. If one BatchJob does not accommodate the incoming volume of tasks, then multiple BatchJobs of the maximum size are submitted at each iteration. When the incoming Jobs slow down and the backlog falls inside the (min_num_nodes, max_num_nodes) range, the BatchJobs reduce down to a single, smaller allocation of resources. As utilization decreases and launchers become idle, the nodes are released according to the launcher's idle_ttl_sec configuration (also in settings.yml ).","title":"Auto Scaling"},{"location":"user-guide/elastic/#auto-scaling-resources-with-balsam-elastic-queue","text":"A Balsam Site does not automatically use your computing allocation by default. Instead, we must use the BatchJob API or balsam queue CLI to explicitly request compute nodes at a given Balsam Site. This follows the principle of least surprise: as the User, you explicitly decide when and how resources are spent. This is not a limitation since we can remotely submit BatchJobs to any Site from a computer where Balsam is installed! However, we can opt-in to enable the elastic_queue plugin that creates BatchJob submissions on our behalf. This can be a useful service in bursty or real-time workloads: instead of micro-managing the queues, we simply submit a stream of Jobs and allow the Site to provision resources as needed over time.","title":"Auto-scaling Resources with Balsam Elastic Queue"},{"location":"user-guide/elastic/#enabling-the-elastic-queue-plugin","text":"Auto-scaling is enabled at a Site by setting the elastic_queue configuration appropriately inside of the settings.yml file. You should find this line uncommented by default: elastic_queue : null Following this line is a commented-block showing an example elastic_queue configuration. We want to comment out the elastic_queue: null line and uncomment the configuration; setting each of the parameters appropriately for our use case. Once the Plugin has been configured properly (see below), we must restart the Balsam Site Agent to load it: $ balsam site sync # Or if the Site isn't already running: $ balsam site start","title":"Enabling the Elastic Queue Plugin"},{"location":"user-guide/elastic/#disabling-the-elastic-queue-plugin","text":"To disable, comment out (or delete) the current elastic_queue configuration in settings.yml and replace it with the line: elastic_queue : null Then restart the Balsam Site to run without the elastic queue plugin: $ balsam site sync","title":"Disabling the Elastic Queue Plugin"},{"location":"user-guide/elastic/#configuring-the-elastic-queue","text":"The configuration is fairly flexible to enable a wide range of use cases. This section explains the YAML configuration in chunks.","title":"Configuring the Elastic Queue"},{"location":"user-guide/elastic/#project-queue-and-submit-frequency","text":"Firstly, service_period controls the waiting period (in seconds) between cycles in which a new BatchJob might be submitted to the queue. The submit_project , submit_queue , and job_mode are directly passed through to the new BatchJob . The max_queue_wait_time_min determines how long a submitted BatchJob should be enqueued before the elastic queue deletes it and tries re-submitting. When using backfill to grab idle nodes (see next section), it makes sense to set a relatively short waiting time of 5-10 minutes. Otherwise, this duration should be increased to a reasonable upper threshold to avoid deleting BatchJobs that have accrued priority in the queues. The elastic queue will maintain up to max_queued_jobs in the queue at any given time. This should be set to the maximum desired (or allowed) number of simultaneously queued/running BatchJobs at the Site. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20","title":"Project, Queue, and Submit Frequency"},{"location":"user-guide/elastic/#wall-time-and-backfilling","text":"Many HPC systems use backfilling schedulers, which attempt to place small Jobs while draining nodes for larger Jobs to start up at a determined future time. By opportunistically sizing jobs to fit into these idle node-hour windows, Balsam effectively \"fills the gaps\" in unused resources. We enable this dynamic sizing with use_backfill: True . The interpretation of min_wall_time_min and max_wall_time_min depends on whether or not use_backfill is enabled: When use_backfill is False : min_wall_time_min is ignored and BatchJobs are submitted for a constant wallclock time limit of max_wall_time_min . When use_backfill is True : Balsam selects backfill windows that are at least as long as min_wall_time_min (this is to avoid futile 5 minute submissions when all Jobs take at least 30 minutes). The wallclock time limit is then the lesser of the scheduler's backfill duration and max_wall_time_min . Finally, a \"padding\" value of wall_time_pad_min is subtracted from the final wallclock time in all BatchJob submissions. This should be set to a couple minutes when use_backfill is True and 0 otherwise. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20","title":"Wall Time and Backfilling"},{"location":"user-guide/elastic/#node-count","text":"Finally, the min_num_nodes and max_num_nodes determine the permissible range of node counts in submitted BatchJobs . When operating with the use_backfill=True constraint, backfill windows smaller than min_num_nodes will be ignored. Otherwise, BatchJob submissions use min_num_nodes as a lower bound. Likewise, max_num_nodes gives an upper bound on the BatchJob's node count. The actual submitted BatchJob node count falls somewhere in this range. It is determined from the difference between how many nodes are currently requested (queued or running BatchJobs) and the aggregate node footprint of all runnable Jobs. elastic_queue : service_period : 60 submit_project : \"datascience\" submit_queue : \"balsam\" job_mode : \"mpi\" use_backfill : True min_wall_time_min : 35 max_wall_time_min : 360 wall_time_pad_min : 5 min_num_nodes : 20 max_num_nodes : 127 max_queue_wait_time_min : 10 max_queued_jobs : 20 Therefore, the elastic queue automatically controls the size and number of requested BatchJobs as the workload grows. We can think of each BatchJob as a flexibly-sized block of resources, and the elastic queue creates multiple blocks (one per service_period ) while choosing their sizes. If one BatchJob does not accommodate the incoming volume of tasks, then multiple BatchJobs of the maximum size are submitted at each iteration. When the incoming Jobs slow down and the backlog falls inside the (min_num_nodes, max_num_nodes) range, the BatchJobs reduce down to a single, smaller allocation of resources. As utilization decreases and launchers become idle, the nodes are released according to the launcher's idle_ttl_sec configuration (also in settings.yml ).","title":"Node Count"},{"location":"user-guide/installation/","text":"Installation \u00b6 Balsam requires Python3.7+ and is tested on Linux and MacOS. Within any suitable Python environment, Balsam can be installed using pip : # Use --pre to get the Balsam pre-release $ pip install --pre balsam-flow Balsam developers or service administrators should instead follow the developer installation instructions . Supported Sites \u00b6 Balsam is easily extensible to new HPC systems. Default configurations are available for the following systems: Facility System Configuration Included? ALCF Theta (KNL) ALCF Theta (GPU) ALCF Cooley NERSC Cori OLCF Summit --- Mac OS Summit (OLCF) \u00b6 The cryptography sub-dependency of globus-sdk can be troublesome on non-x86 environments, where pip may attempt to build it from source. One workaround is to create a conda environment with the cryptography dependency pre-satisfied, from which pip install works smoothly: $ module load gcc/10.2.0 $ module load python/3.7.0-anaconda3-5.3.0 $ conda init bash $ source ~/.bashrc $ conda create -p ./b2env \"cryptography>=1.8.1,<3.4.0\" -y $ conda activate ./b2env $ pip install --pre balsam-flow","title":"Installation"},{"location":"user-guide/installation/#installation","text":"Balsam requires Python3.7+ and is tested on Linux and MacOS. Within any suitable Python environment, Balsam can be installed using pip : # Use --pre to get the Balsam pre-release $ pip install --pre balsam-flow Balsam developers or service administrators should instead follow the developer installation instructions .","title":"Installation"},{"location":"user-guide/installation/#supported-sites","text":"Balsam is easily extensible to new HPC systems. Default configurations are available for the following systems: Facility System Configuration Included? ALCF Theta (KNL) ALCF Theta (GPU) ALCF Cooley NERSC Cori OLCF Summit --- Mac OS","title":"Supported Sites"},{"location":"user-guide/installation/#summit-olcf","text":"The cryptography sub-dependency of globus-sdk can be troublesome on non-x86 environments, where pip may attempt to build it from source. One workaround is to create a conda environment with the cryptography dependency pre-satisfied, from which pip install works smoothly: $ module load gcc/10.2.0 $ module load python/3.7.0-anaconda3-5.3.0 $ conda init bash $ source ~/.bashrc $ conda create -p ./b2env \"cryptography>=1.8.1,<3.4.0\" -y $ conda activate ./b2env $ pip install --pre balsam-flow","title":"Summit (OLCF)"},{"location":"user-guide/jobs/","text":"Balsam Jobs \u00b6 Key Concept: The Job Lifecycle \u00b6 The Balsam Job represents a single invocation of an App on some specified computing resources. Each Job is a stateful object that advances through a lifecycle of states (from CREATED to JOB_FINISHED in a successful flow). After defining the requisite Apps , we create a collection of Jobs . Each Job specifies any data transfer or inter-job dependencies. The collection of Jobs represents our workflow, which is then executed by Balsam over time. Jobs do not automatically run! As mentioned in the quickstart , Jobs only specify the CPU/GPU resources needed for each task. In order to run Jobs, we then request a block of node-hours by submitting a BatchJob . This section goes into detail on managing Jobs , which is a separate concern in Balsam. You will find your jobs waiting in the PREPROCESSED state until a BatchJob begins running. In the normal (successful) flow of execution, a Job moves through the following sequence of states. The table below defines each state as well as the action performed by Balsam to move the job toward the next state. State Meaning Next Balsam Action CREATED Job initially submitted. Check the parent Job and data transfer dependencies AWAITING_PARENTS Pending parent job dependencies. Advance to READY when all parents finished READY All parent jobs have finished. Submit any stage-in transfer tasks STAGED_IN All data dependencies staged in. Call the preprocess() hook PREPROCESSED The preprocess hook completed. Acquire and launch the executable on a compute node RUNNING Started executing on a compute node. Poll the executing Job process's return code RUN_DONE Execution finished with return 0 . Call the postprocess() hook POSTPROCESSED The postprocess hook completed. Submit any stage-out transfer tasks STAGED_OUT All stage-out transfers completed. Mark the job JOB_FINISHED JOB_FINISHED The job has completed processing. Nothing (end state) Additionally, Balsam defines the following exceptional states for handling jobs that encounter errors or early termination: State Meaning Next Balsam Action RUN_ERROR Execution finished with nonzero returncode. Call the handle_error() hook RUN_TIMEOUT Execution was terminated mid-run. Call the handle_timeout() hook RESTART_READY Job is ready to run again. Acquire and launch the executable on a compute node FAILED Completed processing (unsuccessfully). Nothing (end state) Hopefully, it's clear from these state flows that a Job can be thought of as the workflow surrounding a single App run . You should check out specific examples of the Balsam hooks that can be used to build interesting workflows at the ApplicationDefinition level. If we don't define any special hooks or data transfers, most of the steps listed above are no-ops and the Job simplifies down to a simple run of an App command. Of course, we can also build DAGs or ensembles of many application runs by creating multiple Jobs , potentially specifying inter- Job dependencies. We will show effective methods for creating large batches of Jobs later on. To conclude this section, the state diagram below summarizes the Job lifecycle by illustrating the common state flows. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished Creating Jobs \u00b6 To create a Job , we need to supply arguments via the Python API's Job() constructor or the balsam job create CLI. Most fields are optional and take sensible default values. At a minimum, we must always supply: app_id or app_name and site_path : reference to the specific App workdir : the working directory, relative to the Site's data/ path Any parameters required by the ApplicationDefinition 's command template Any transfers items required by the ApplicationDefinition We can also create Jobs using the ApplicationDefinition.submit() shorthand : this removes the need for app_id because the value is inferred from the application class itself. CLI Job Creation \u00b6 The quickstart tutorial showed an example of CLI job creation: $ balsam job create --site = laptop --app Hello --workdir demo/hello2 --param say_hello_to = \"world2\" If -a/--app doesn't uniquely specify an App by its class path , you can provide the numeric app ID (revealed by balsam app ls ) or target a specific site using the --site selector . Since you can target any App defined at any Site, the process of submitting Jobs locally or between systems is seamless and unified. By passing test/1 to the -w/--workdir option, we declare that the job should run in the data/test/1/ subdirectory of the Site. This folder will be created automatically. Finally, multiple command template parameters can be passed by repeated -p/--param arguments. In the example above we have only one parameter called name and provide a value of \"world\" . Multiple Arguments Run balsam job create --help to list the various options and example usage. For any option that takes multiple arguments, they should be provided by repeating the flag. For instance, balsam job create --tag foo=xyz --tag experiment=initial will create a job with two tags. API Job Creation \u00b6 You will usually prefer to leverage the flexibility of Python to populate a large number of Jobs programmatically. For example, a common pattern in Balsam is to write a quick one-off script to crawl a directory of input files and generate a Job for each one. Our entrypoint to creating Jobs from Python is the Balsam Job API: from balsam.api import Job Take advantage of the docstrings and type annotations! We strongly recommend using the Balsam APIs in an interactive development environment such as a Jupyter Notebook or Python IDE of choice. Each Balsam model defined under balsam.api includes detailed docstrings and type annotations for the possible methods. We can construct an in-memory Job object by populating the required fields, and then we submit it to the web service by calling job.save() . This is the Python equivalent of the previous CLI example: job = Job ( app_id = 123 , workdir = \"test/1\" , parameters = { \"name\" : \"world!\" }) job . save () If you don't want to lookup and hard-code the app_id , you can provide the app name in its place. If you're using the same app name at multiple Sites, you will also have to provide the site_name to disambiguate which app you really mean to create: job = Job ( app_id = \"Hello\" , site_name = \"theta-gpu\" , workdir = \"test/1\" , parameters = { \"name\" : \"world!\" } ) job . save () A shortcut for creating and saving the Job in one step is provide the same exact arguments to Job.objects.create : job = Job . objects . create ( app_id = 123 , ... ) # don't need to call job.save() The real advantage of the API is to create many related Jobs programmatically. We can still call job.save() one-by-one, but it's more efficient to bulk-create the jobs with a single network round trip: jobs = [ Job ( app_id = 123 , workdir = f \"test/ { n } \" , parameters = { \"name\" : f \"world { n } !\" }) for n in range ( 10 ) ] # Capture `jobs` as return value! jobs = Job . objects . bulk_create ( jobs ) bulk_create does not modify objects in place! When passing a list of Jobs into bulk_create() , you must use the returned value to overwrite the input list with the newly-created Jobs. This is necessary to set the ID on each item as generated by the server. Otherwise, the created Jobs will have id == None and generally behave like objects that have never been saved to the API. Finally, ApplicationDefinitions provide a convenient shorthand to create Jobs from the same file that the application is defined in: job = Hello . submit ( workdir = \"test/123\" , name = \"world!\" ) When using the ApplicationDefinition.submit() syntax, the app_id is automatically inferred, and any unrecognized keyword arguments are passed through into job.parameters . This allows for a very concise Job creation. To use submit with bulk-creation, pass save=False to avoid saving each Job to the API one a time: jobs = [ Hello . submit ( workdir = f \"test/ { n } \" , say_hello_to = f \"world { n } !\" , save = False ) for n in range ( 10 ) ] jobs = Job . objects . bulk_create ( jobs ) # efficient creation Tagging Jobs \u00b6 When creating many Jobs to run the same App , we need a way of keeping things organized and searchable. Jobs should be organized into hierarchical working directories in a scheme that makes sense for your workflow. Jobs can then be queried by working directory substrings, which facilitates monitoring groups *of Jobs having some common path fragment. However, organizing by workdir quickly becomes limited, so Balsam provides a more flexible system for tagging Jobs with arbitrary key-value string pairs. You can assign Jobs any tag names and values (keeping in mind that even numerical tags are treated as strings), and then easily query or manipulate Jobs according to their tags. # Create tagged jobs... $ balsam job create --tag experiment = foo --tag system = H2O --tag run = 5 # ...other args # ...so that you can fetch jobs with certain tags later! $ balsam job ls --tag experiment = foo The idea is much the same with the Python API: Job . objects . create ( # ...other kwargs here tags = { \"experiment\" : \"foo\" , \"system\" : \"H2O\" , \"run\" : \"5\" } ) for job in Job . objects . filter ( tags = { \"experiment\" : \"foo\" }): if job . state == \"JOB_FINISHED\" : print ( \"Finished:\" , job . workdir ) Associating Data with Jobs \u00b6 The Balsam service is not designed to store large volumes of data directly; instead, Balsam interfaces with external transfer providers such as Globus to orchestrate out-of-band data transfers efficiently. Nevertheless, each Balsam Job contains a data attribute that can store a dictionary of arbitrary, JSON-serialized data. This can be particularly useful to attach some user-defined, persistent state to Jobs that can be leveraged by the lifecycle hooks . # Creating Jobs with some initial data Job . objects . create ( # ...other kwargs here data = { \"retry_count\" : 0 , \"input_coords\" : coords } ) Note that in order to update job.data on an existing Job, we need to assign a new dictionary to the job.data attribute, rather than setting an individual key: job = Job . objects . get ( tags = { \"experiment\" : \"foo\" }, workdir__contains = \"foo/20\" ) dat = job . data retry_count = dat [ \"retry_count\" ] + 1 # Merge the old job.data with an incremented value: job . data = { ** dat , \"retry_count\" : retry_count + 1 } job . save () This is a consequence of the descriptor protocol used to track mutations to Job fields. The Python API currently sends only fields which have been expclitly set ( job.FIELD = VALUE ) in updates to the backend. If you modify an existing mutable field (appending to a list or setting a new key on the data dictionary), the change cannot yet be detected by the Balsam client API layer. Defining Compute Resources \u00b6 The default Job arguments assume the application will execute as one single-threaded process occupying a full compute node. This is often not the case, and the Job constructor provides several options to specify precise resource requirements. As usual, these parameters can be specified via the Python API or CLI when creating new Jobs. num_nodes : number of compute nodes needed in a multi-node MPI application ranks_per_node : number of processes (MPI ranks) per compute node threads_per_rank : number of threads per rank threads_per_core : number of threads per physical CPU core launch_params : optional pass-through parameters to MPI launcher gpus_per_rank : number of GPU accelerators per rank node_packing_count : maximum number of Jobs that may run simultaneously on the same compute node wall_time_min : optional Job execution time estimate, in minutes Balsam dynamically schedules Jobs onto the available compute resources over the course of each launcher (pilot batch job). Each Job is considered to fully occupy a whole number of CPU cores and GPU devices while it runs. For each compute node in a batch allocation, Balsam tracks the list of busy CPUs, busy GPUs, and a node occupancy metric. The occupancy is a floating-point value between 0.0 (idle) and 1.0 (busy) calculated as the sum of 1 / job.node_packing_count over all jobs running on a node. When Balsam places a sub-node job, it simultaneously honors the constraints: The node must have enough idle GPUs ( job.ranks_per_node * job.gpus_per_rank ) The node must have enough idle CPUs ( job.ranks_per_node * job.threads_per_rank // job.threads_per_core ) The node must have low enough occupancy to accommodate the job without exceeding an occupancy of 1.0. Job Placement Examples Consider a 2-node allocation on a system with 64 CPU cores and 8 GPUs per node. If there are 16 single-process jobs with node_packing_count=8 and gpus_per_rank=1 , then all 16 runs will execute concurrently. With node_packing_count=4 and gpus_per_rank=1 , only 8 jobs will run at a time (4 per node, constrained by the node occupancy). If node_packing_count=8 and gpus_per_rank=8 , only 2 jobs will run at a time (one job per node, constrained by the lack of idle GPUs). Parent Job Dependencies \u00b6 By including parent Job IDs in a Job constructor, we create dependencies : every Job waits to begin executing until all of its parents reach the JOB_FINISHED state. This can be used to build workflows comprising several Apps . Moreover, since Jobs can be created in lifecycle hooks, we can leverage this ability to dynamically change the workflow graph as Jobs are executed. # Create a Job that depends on job1 and job2: Job . objects . create ( # ...other kwargs parent_ids = [ job1 . id , job2 . id ] ) Data Transfer \u00b6 In the App Transfer Slots section , we explained how ApplicationDefinition classes define requirements for remote data stage in before execution (or stage out after execution). This scheme has two advantages: The inputs and outputs for an App are explicitly defined and consistently named alongside the other ingredients of the ApplicationDefinition class. This pattern facilitates writing command templates and lifecycle hooks that are decoupled from details like external file paths. The Balsam Site Agent automatically groups transfers from endpoint A to B across many Jobs. It can then submit batched transfer tasks to the underlying transfer service provider, and those transfers are monitored until completion. As soon as input data arrives, waiting jobs transition from READY to STAGED_IN and begin preprocessing. The end-to-end lifecycle (await parent dependencies, stage in datasets, preprocess, schedule compute nodes, launch application) is fully managed by the Site Agent. Since the ApplicationDefinition decides how files are named in local working directories, we only need to fill the Transfer Slots by providing remote data locations. The Job transfers argument is a dictionary of the form {\"slot_name\": \"location_alias:absolute_path\"} . slot_name must match one of the App's transfer keys. location_alias must match one of the keys in the transfer_locations dictionary in settings.yml . absolute_path is the fully-resolved path to the source file or directory for stage-ins. For stage-outs, it is the path of the destination to be written. Adding Location Aliases The transfer_locations dictionary in settings.yml maps location aliases to values of the form protocol://network_location . A useful example would be a Globus Connect Personal endpoint running on your laptop. The corresponding list item under transfer_locations in settings.yml would look like this: transfer_locations : laptop : globus://9d6d99eb-6d04-11e5-ba46-22000b92c6ec In this example, we create a Job that runs on a supercomputer but copies the input_file from our laptop and eventually writes the result back to it. Job . objects . create ( # ...other kwargs transfers = { # Using 'laptop' alias defined in settings.yml \"input_file\" : \"laptop:/path/to/input.dat\" , \"result\" : \"laptop:/path/to/output.json\" , }, ) Querying Jobs \u00b6 Once many Jobs are added to Balsam, there are several effective ways of searching and manipulating those jobs from the CLI or Python API. The Balsam query API has a regular structure that's the same for all resources ( Site , App , Job , BatchJob , etc...) and loosely based on the Django ORM . Refer to the next section on the Balsam API for general details that apply to all resources. In the following, we focus on Job specific examples, because those are the most common and useful queries you'll be performing with Balsam. The CLI is just a wrapper of the API While the examples focus on the Python API, the CLI is merely a thin wrapper of the API, and most CLI queries can be inferred from the balsam job ls --help menu. For example, this Python query: Job . objects . filter ( state = \"FAILED\" , tags = { \"experiment\" : \"foo\" }) is equivalent to this command line: $ balsam job ls --state FAILED --tag experiment = foo Filtering Examples \u00b6 Job.objects is a Manager class that talks to the underlying REST API over HTTPS and builds Job objects from JSON data transferred over the Web. We can start to build a query with one or many filter kwargs passed to Job.objects.filter(). The filter docstrings are again very useful here in listing the supported query parameters within your IDE. Queries are chainable and lazily-evaluated : from balsam.api import Site , Job # This hits the network and immediately returns ONE Site object: theta_site = Site . objects . get ( name = \"theta\" , path = \"my-site\" ) # This doesn't hit the network yet: foo_jobs = Job . objects . filter ( site_id = theta_site . id , tags = { \"experiment\" : \"foo\" }, ) # We chain the query and iterate, triggering HTTPS request: failed_workdirs = [ j . workdir for j in foo_jobs . filter ( state = \"FAILED\" ) ] We can generate a query that returns all Jobs across Balsam: all_jobs = Job . objects . all () Queries support slicing operations: some_jobs = all_jobs [ 5 : 15 ] We can count the number of Jobs that satisfy some query: Job . objects . filter ( state = \"RUNNING\" ) . count () We can order the Jobs according to some criterion (prefix with - for descending order): ten_most_recent_finished = Job . objects . filter ( state = \"JOB_FINISHED\" ) . order_by ( \"-last_update\" )[: 10 ] We can build up queries based on numerous criteria, including but not limited to: workdir__contains : path fragment tags app_id state parameters id (single or list of Job IDs) parent_id (single or list of Parent Job IDs) The iterable queries return Job instances that can be inspected or modified and updated by calling job.save() . Again, refer to the docstrings or use help(Job) to see a detailed listing of the Job fields. failed = Job . objects . filter ( workdir__contains = \"production-X\" , state = \"FAILED\" ) for job in failed : if job . return_code == 1 : job . num_nodes = 16 job . state = \"RESTART_READY\" job . save () Resolving the Workdir \u00b6 The Job.workdir attribute is given and stored relative to the Site data/ directory. Sometimes it is useful to resolve an absolute path to a job's working directory: # From a given Site and Job... site = Site . objects . get ( id = 123 ) job = Job . objects . get ( id = 456 ) # The workdir can be constructed with Path join operators: abs_workdir = site . path / \"data\" / job . workdir # Or with the helper method: abs_workdir = job . resolve_workdir ( site . path / \"data\" ) If you are running code from inside a Site, you can access the current Site configuration and the resolved data/ path: from balsam.api import site_config abs_workdir = job . resolve_workdir ( site_config . data_path ) Accessing Parents \u00b6 Jobs contain the special method parent_query() which returns a iterable query over the job's parents. For example, we can combine this with resolve_workdir to list the parents' working directories. This pattern can be particularly useful in the preprocessing app hook, where a Job needs to read some data from its parents before executing: from balsam.api import site_config from balsam.api import ApplicationDefinition class MyApp ( ApplicationDefinition ): # ...other class attrs def preprocess ( self ): parent_workdirs = [ j . resolve_workdir ( site_config . data_path ) for j in self . job . parent_query () ] Updating Jobs \u00b6 In addition to the examples where Jobs were modified and updated by calling save() , we can efficiently apply the same update to all jobs matching a particular query. This can be significantly faster than calling job.save() in a loop, which repeatedly sends small HTTP update requests over the wire. # Run all the Failed Jobs again: Job . objects . filter ( state = \"FAILED\" ) . update ( state = \"RESTART_READY\" ) Deleting Jobs \u00b6 Just as we can apply updates to individual jobs or job sets selected by a query, we can also delete Jobs : # Delete a single job: job . delete () # Delete a collection of Jobs: Job . objects . filter ( state = \"FAILED\" , tags = { \"run\" : \"H2O\" }) . delete () Python App Futures \u00b6 When using the run() function in your ApplicationDefinitions , you can treat the resultant Job objects like standard Python Futures in a few useful ways. Accessing Results \u00b6 The Job.result(timeout=None) method will block until the job is completed, and return the propagated return value of the run() function. If the function raised an Exception, the Exception is re-raised: job = Adder . submit ( \"test/123\" , x = 3 , y = 7 ) assert job . result () == 3 + 7 result() optionally takes a floating point seconds timeout value. If the Job does not complete within the timeout period, it will raise concurrent.futures.TimeoutError . Similarly, result_nowait() will return the result in a non-blocking fashion and raise Job.NoResult if a result is not immediately available. Polling a single job on completion \u00b6 Job.done() polls the API and returns True if the Job is either in the JOB_FINISHED or FAILED state. Polling many jobs on completion \u00b6 The Job.objects.wait() function takes a list of Jobs and behaves otherwise analogously to concurrent.futures.wait . This can be used to efficiently poll on a large collection of Jobs with a timeout and sort the results by completed/in-progress Jobs: wait_result = Job . objects . wait ( active_jobs , return_when = \"FIRST_COMPLETED\" , timeout = 60 , poll_interval = 10 ) print ( f \" { len ( wait_result . done ) } jobs completed\" ) print ( f \" { len ( wait_result . not_done ) } active jobs\" ) Iterating over Jobs as they complete \u00b6 The Job.objects.as_completed() function behaves analogously to concurrent.futures.as_completed . The method returns an generator over the input Jobs , which yields Jobs one a time as they are completed. for job in Job . objects . as_completed ( active_jobs , timeout = 60 ): print ( f \"Job { job . workdir } returned: { job . result () } \" )","title":"Managing Jobs"},{"location":"user-guide/jobs/#balsam-jobs","text":"","title":"Balsam Jobs"},{"location":"user-guide/jobs/#key-concept-the-job-lifecycle","text":"The Balsam Job represents a single invocation of an App on some specified computing resources. Each Job is a stateful object that advances through a lifecycle of states (from CREATED to JOB_FINISHED in a successful flow). After defining the requisite Apps , we create a collection of Jobs . Each Job specifies any data transfer or inter-job dependencies. The collection of Jobs represents our workflow, which is then executed by Balsam over time. Jobs do not automatically run! As mentioned in the quickstart , Jobs only specify the CPU/GPU resources needed for each task. In order to run Jobs, we then request a block of node-hours by submitting a BatchJob . This section goes into detail on managing Jobs , which is a separate concern in Balsam. You will find your jobs waiting in the PREPROCESSED state until a BatchJob begins running. In the normal (successful) flow of execution, a Job moves through the following sequence of states. The table below defines each state as well as the action performed by Balsam to move the job toward the next state. State Meaning Next Balsam Action CREATED Job initially submitted. Check the parent Job and data transfer dependencies AWAITING_PARENTS Pending parent job dependencies. Advance to READY when all parents finished READY All parent jobs have finished. Submit any stage-in transfer tasks STAGED_IN All data dependencies staged in. Call the preprocess() hook PREPROCESSED The preprocess hook completed. Acquire and launch the executable on a compute node RUNNING Started executing on a compute node. Poll the executing Job process's return code RUN_DONE Execution finished with return 0 . Call the postprocess() hook POSTPROCESSED The postprocess hook completed. Submit any stage-out transfer tasks STAGED_OUT All stage-out transfers completed. Mark the job JOB_FINISHED JOB_FINISHED The job has completed processing. Nothing (end state) Additionally, Balsam defines the following exceptional states for handling jobs that encounter errors or early termination: State Meaning Next Balsam Action RUN_ERROR Execution finished with nonzero returncode. Call the handle_error() hook RUN_TIMEOUT Execution was terminated mid-run. Call the handle_timeout() hook RESTART_READY Job is ready to run again. Acquire and launch the executable on a compute node FAILED Completed processing (unsuccessfully). Nothing (end state) Hopefully, it's clear from these state flows that a Job can be thought of as the workflow surrounding a single App run . You should check out specific examples of the Balsam hooks that can be used to build interesting workflows at the ApplicationDefinition level. If we don't define any special hooks or data transfers, most of the steps listed above are no-ops and the Job simplifies down to a simple run of an App command. Of course, we can also build DAGs or ensembles of many application runs by creating multiple Jobs , potentially specifying inter- Job dependencies. We will show effective methods for creating large batches of Jobs later on. To conclude this section, the state diagram below summarizes the Job lifecycle by illustrating the common state flows. stateDiagram-v2 created: Created awaiting_parents: Awaiting Parents ready: Ready staged_in: Staged In preprocessed: Preprocessed restart_ready: Restart Ready running: Running run_done: Run Done postprocessed: Postprocessed staged_out: Staged Out finished: Job Finished run_error: Run Error run_timeout: Run Timeout failed: Failed created --> ready: No parents created --> awaiting_parents: Pending dependencies awaiting_parents --> ready: Dependencies finished ready --> staged_in: Transfer external data in staged_in --> preprocessed: Run preprocess script preprocessed --> running: Launch job running --> run_done: Return code 0 running --> run_error: Nonzero return running --> run_timeout: Early termination run_timeout --> restart_ready: Auto retry run_error --> restart_ready: Run error handler run_error --> failed: No error handler restart_ready --> running: Launch job run_done --> postprocessed: Run postprocess script postprocessed --> staged_out: Transfer data out staged_out --> finished: Job Finished","title":"Key Concept: The Job Lifecycle"},{"location":"user-guide/jobs/#creating-jobs","text":"To create a Job , we need to supply arguments via the Python API's Job() constructor or the balsam job create CLI. Most fields are optional and take sensible default values. At a minimum, we must always supply: app_id or app_name and site_path : reference to the specific App workdir : the working directory, relative to the Site's data/ path Any parameters required by the ApplicationDefinition 's command template Any transfers items required by the ApplicationDefinition We can also create Jobs using the ApplicationDefinition.submit() shorthand : this removes the need for app_id because the value is inferred from the application class itself.","title":"Creating Jobs"},{"location":"user-guide/jobs/#cli-job-creation","text":"The quickstart tutorial showed an example of CLI job creation: $ balsam job create --site = laptop --app Hello --workdir demo/hello2 --param say_hello_to = \"world2\" If -a/--app doesn't uniquely specify an App by its class path , you can provide the numeric app ID (revealed by balsam app ls ) or target a specific site using the --site selector . Since you can target any App defined at any Site, the process of submitting Jobs locally or between systems is seamless and unified. By passing test/1 to the -w/--workdir option, we declare that the job should run in the data/test/1/ subdirectory of the Site. This folder will be created automatically. Finally, multiple command template parameters can be passed by repeated -p/--param arguments. In the example above we have only one parameter called name and provide a value of \"world\" . Multiple Arguments Run balsam job create --help to list the various options and example usage. For any option that takes multiple arguments, they should be provided by repeating the flag. For instance, balsam job create --tag foo=xyz --tag experiment=initial will create a job with two tags.","title":"CLI Job Creation"},{"location":"user-guide/jobs/#api-job-creation","text":"You will usually prefer to leverage the flexibility of Python to populate a large number of Jobs programmatically. For example, a common pattern in Balsam is to write a quick one-off script to crawl a directory of input files and generate a Job for each one. Our entrypoint to creating Jobs from Python is the Balsam Job API: from balsam.api import Job Take advantage of the docstrings and type annotations! We strongly recommend using the Balsam APIs in an interactive development environment such as a Jupyter Notebook or Python IDE of choice. Each Balsam model defined under balsam.api includes detailed docstrings and type annotations for the possible methods. We can construct an in-memory Job object by populating the required fields, and then we submit it to the web service by calling job.save() . This is the Python equivalent of the previous CLI example: job = Job ( app_id = 123 , workdir = \"test/1\" , parameters = { \"name\" : \"world!\" }) job . save () If you don't want to lookup and hard-code the app_id , you can provide the app name in its place. If you're using the same app name at multiple Sites, you will also have to provide the site_name to disambiguate which app you really mean to create: job = Job ( app_id = \"Hello\" , site_name = \"theta-gpu\" , workdir = \"test/1\" , parameters = { \"name\" : \"world!\" } ) job . save () A shortcut for creating and saving the Job in one step is provide the same exact arguments to Job.objects.create : job = Job . objects . create ( app_id = 123 , ... ) # don't need to call job.save() The real advantage of the API is to create many related Jobs programmatically. We can still call job.save() one-by-one, but it's more efficient to bulk-create the jobs with a single network round trip: jobs = [ Job ( app_id = 123 , workdir = f \"test/ { n } \" , parameters = { \"name\" : f \"world { n } !\" }) for n in range ( 10 ) ] # Capture `jobs` as return value! jobs = Job . objects . bulk_create ( jobs ) bulk_create does not modify objects in place! When passing a list of Jobs into bulk_create() , you must use the returned value to overwrite the input list with the newly-created Jobs. This is necessary to set the ID on each item as generated by the server. Otherwise, the created Jobs will have id == None and generally behave like objects that have never been saved to the API. Finally, ApplicationDefinitions provide a convenient shorthand to create Jobs from the same file that the application is defined in: job = Hello . submit ( workdir = \"test/123\" , name = \"world!\" ) When using the ApplicationDefinition.submit() syntax, the app_id is automatically inferred, and any unrecognized keyword arguments are passed through into job.parameters . This allows for a very concise Job creation. To use submit with bulk-creation, pass save=False to avoid saving each Job to the API one a time: jobs = [ Hello . submit ( workdir = f \"test/ { n } \" , say_hello_to = f \"world { n } !\" , save = False ) for n in range ( 10 ) ] jobs = Job . objects . bulk_create ( jobs ) # efficient creation","title":"API Job Creation"},{"location":"user-guide/jobs/#tagging-jobs","text":"When creating many Jobs to run the same App , we need a way of keeping things organized and searchable. Jobs should be organized into hierarchical working directories in a scheme that makes sense for your workflow. Jobs can then be queried by working directory substrings, which facilitates monitoring groups *of Jobs having some common path fragment. However, organizing by workdir quickly becomes limited, so Balsam provides a more flexible system for tagging Jobs with arbitrary key-value string pairs. You can assign Jobs any tag names and values (keeping in mind that even numerical tags are treated as strings), and then easily query or manipulate Jobs according to their tags. # Create tagged jobs... $ balsam job create --tag experiment = foo --tag system = H2O --tag run = 5 # ...other args # ...so that you can fetch jobs with certain tags later! $ balsam job ls --tag experiment = foo The idea is much the same with the Python API: Job . objects . create ( # ...other kwargs here tags = { \"experiment\" : \"foo\" , \"system\" : \"H2O\" , \"run\" : \"5\" } ) for job in Job . objects . filter ( tags = { \"experiment\" : \"foo\" }): if job . state == \"JOB_FINISHED\" : print ( \"Finished:\" , job . workdir )","title":"Tagging Jobs"},{"location":"user-guide/jobs/#associating-data-with-jobs","text":"The Balsam service is not designed to store large volumes of data directly; instead, Balsam interfaces with external transfer providers such as Globus to orchestrate out-of-band data transfers efficiently. Nevertheless, each Balsam Job contains a data attribute that can store a dictionary of arbitrary, JSON-serialized data. This can be particularly useful to attach some user-defined, persistent state to Jobs that can be leveraged by the lifecycle hooks . # Creating Jobs with some initial data Job . objects . create ( # ...other kwargs here data = { \"retry_count\" : 0 , \"input_coords\" : coords } ) Note that in order to update job.data on an existing Job, we need to assign a new dictionary to the job.data attribute, rather than setting an individual key: job = Job . objects . get ( tags = { \"experiment\" : \"foo\" }, workdir__contains = \"foo/20\" ) dat = job . data retry_count = dat [ \"retry_count\" ] + 1 # Merge the old job.data with an incremented value: job . data = { ** dat , \"retry_count\" : retry_count + 1 } job . save () This is a consequence of the descriptor protocol used to track mutations to Job fields. The Python API currently sends only fields which have been expclitly set ( job.FIELD = VALUE ) in updates to the backend. If you modify an existing mutable field (appending to a list or setting a new key on the data dictionary), the change cannot yet be detected by the Balsam client API layer.","title":"Associating Data with Jobs"},{"location":"user-guide/jobs/#defining-compute-resources","text":"The default Job arguments assume the application will execute as one single-threaded process occupying a full compute node. This is often not the case, and the Job constructor provides several options to specify precise resource requirements. As usual, these parameters can be specified via the Python API or CLI when creating new Jobs. num_nodes : number of compute nodes needed in a multi-node MPI application ranks_per_node : number of processes (MPI ranks) per compute node threads_per_rank : number of threads per rank threads_per_core : number of threads per physical CPU core launch_params : optional pass-through parameters to MPI launcher gpus_per_rank : number of GPU accelerators per rank node_packing_count : maximum number of Jobs that may run simultaneously on the same compute node wall_time_min : optional Job execution time estimate, in minutes Balsam dynamically schedules Jobs onto the available compute resources over the course of each launcher (pilot batch job). Each Job is considered to fully occupy a whole number of CPU cores and GPU devices while it runs. For each compute node in a batch allocation, Balsam tracks the list of busy CPUs, busy GPUs, and a node occupancy metric. The occupancy is a floating-point value between 0.0 (idle) and 1.0 (busy) calculated as the sum of 1 / job.node_packing_count over all jobs running on a node. When Balsam places a sub-node job, it simultaneously honors the constraints: The node must have enough idle GPUs ( job.ranks_per_node * job.gpus_per_rank ) The node must have enough idle CPUs ( job.ranks_per_node * job.threads_per_rank // job.threads_per_core ) The node must have low enough occupancy to accommodate the job without exceeding an occupancy of 1.0. Job Placement Examples Consider a 2-node allocation on a system with 64 CPU cores and 8 GPUs per node. If there are 16 single-process jobs with node_packing_count=8 and gpus_per_rank=1 , then all 16 runs will execute concurrently. With node_packing_count=4 and gpus_per_rank=1 , only 8 jobs will run at a time (4 per node, constrained by the node occupancy). If node_packing_count=8 and gpus_per_rank=8 , only 2 jobs will run at a time (one job per node, constrained by the lack of idle GPUs).","title":"Defining Compute Resources"},{"location":"user-guide/jobs/#parent-job-dependencies","text":"By including parent Job IDs in a Job constructor, we create dependencies : every Job waits to begin executing until all of its parents reach the JOB_FINISHED state. This can be used to build workflows comprising several Apps . Moreover, since Jobs can be created in lifecycle hooks, we can leverage this ability to dynamically change the workflow graph as Jobs are executed. # Create a Job that depends on job1 and job2: Job . objects . create ( # ...other kwargs parent_ids = [ job1 . id , job2 . id ] )","title":"Parent Job Dependencies"},{"location":"user-guide/jobs/#data-transfer","text":"In the App Transfer Slots section , we explained how ApplicationDefinition classes define requirements for remote data stage in before execution (or stage out after execution). This scheme has two advantages: The inputs and outputs for an App are explicitly defined and consistently named alongside the other ingredients of the ApplicationDefinition class. This pattern facilitates writing command templates and lifecycle hooks that are decoupled from details like external file paths. The Balsam Site Agent automatically groups transfers from endpoint A to B across many Jobs. It can then submit batched transfer tasks to the underlying transfer service provider, and those transfers are monitored until completion. As soon as input data arrives, waiting jobs transition from READY to STAGED_IN and begin preprocessing. The end-to-end lifecycle (await parent dependencies, stage in datasets, preprocess, schedule compute nodes, launch application) is fully managed by the Site Agent. Since the ApplicationDefinition decides how files are named in local working directories, we only need to fill the Transfer Slots by providing remote data locations. The Job transfers argument is a dictionary of the form {\"slot_name\": \"location_alias:absolute_path\"} . slot_name must match one of the App's transfer keys. location_alias must match one of the keys in the transfer_locations dictionary in settings.yml . absolute_path is the fully-resolved path to the source file or directory for stage-ins. For stage-outs, it is the path of the destination to be written. Adding Location Aliases The transfer_locations dictionary in settings.yml maps location aliases to values of the form protocol://network_location . A useful example would be a Globus Connect Personal endpoint running on your laptop. The corresponding list item under transfer_locations in settings.yml would look like this: transfer_locations : laptop : globus://9d6d99eb-6d04-11e5-ba46-22000b92c6ec In this example, we create a Job that runs on a supercomputer but copies the input_file from our laptop and eventually writes the result back to it. Job . objects . create ( # ...other kwargs transfers = { # Using 'laptop' alias defined in settings.yml \"input_file\" : \"laptop:/path/to/input.dat\" , \"result\" : \"laptop:/path/to/output.json\" , }, )","title":"Data Transfer"},{"location":"user-guide/jobs/#querying-jobs","text":"Once many Jobs are added to Balsam, there are several effective ways of searching and manipulating those jobs from the CLI or Python API. The Balsam query API has a regular structure that's the same for all resources ( Site , App , Job , BatchJob , etc...) and loosely based on the Django ORM . Refer to the next section on the Balsam API for general details that apply to all resources. In the following, we focus on Job specific examples, because those are the most common and useful queries you'll be performing with Balsam. The CLI is just a wrapper of the API While the examples focus on the Python API, the CLI is merely a thin wrapper of the API, and most CLI queries can be inferred from the balsam job ls --help menu. For example, this Python query: Job . objects . filter ( state = \"FAILED\" , tags = { \"experiment\" : \"foo\" }) is equivalent to this command line: $ balsam job ls --state FAILED --tag experiment = foo","title":"Querying Jobs"},{"location":"user-guide/jobs/#filtering-examples","text":"Job.objects is a Manager class that talks to the underlying REST API over HTTPS and builds Job objects from JSON data transferred over the Web. We can start to build a query with one or many filter kwargs passed to Job.objects.filter(). The filter docstrings are again very useful here in listing the supported query parameters within your IDE. Queries are chainable and lazily-evaluated : from balsam.api import Site , Job # This hits the network and immediately returns ONE Site object: theta_site = Site . objects . get ( name = \"theta\" , path = \"my-site\" ) # This doesn't hit the network yet: foo_jobs = Job . objects . filter ( site_id = theta_site . id , tags = { \"experiment\" : \"foo\" }, ) # We chain the query and iterate, triggering HTTPS request: failed_workdirs = [ j . workdir for j in foo_jobs . filter ( state = \"FAILED\" ) ] We can generate a query that returns all Jobs across Balsam: all_jobs = Job . objects . all () Queries support slicing operations: some_jobs = all_jobs [ 5 : 15 ] We can count the number of Jobs that satisfy some query: Job . objects . filter ( state = \"RUNNING\" ) . count () We can order the Jobs according to some criterion (prefix with - for descending order): ten_most_recent_finished = Job . objects . filter ( state = \"JOB_FINISHED\" ) . order_by ( \"-last_update\" )[: 10 ] We can build up queries based on numerous criteria, including but not limited to: workdir__contains : path fragment tags app_id state parameters id (single or list of Job IDs) parent_id (single or list of Parent Job IDs) The iterable queries return Job instances that can be inspected or modified and updated by calling job.save() . Again, refer to the docstrings or use help(Job) to see a detailed listing of the Job fields. failed = Job . objects . filter ( workdir__contains = \"production-X\" , state = \"FAILED\" ) for job in failed : if job . return_code == 1 : job . num_nodes = 16 job . state = \"RESTART_READY\" job . save ()","title":"Filtering Examples"},{"location":"user-guide/jobs/#resolving-the-workdir","text":"The Job.workdir attribute is given and stored relative to the Site data/ directory. Sometimes it is useful to resolve an absolute path to a job's working directory: # From a given Site and Job... site = Site . objects . get ( id = 123 ) job = Job . objects . get ( id = 456 ) # The workdir can be constructed with Path join operators: abs_workdir = site . path / \"data\" / job . workdir # Or with the helper method: abs_workdir = job . resolve_workdir ( site . path / \"data\" ) If you are running code from inside a Site, you can access the current Site configuration and the resolved data/ path: from balsam.api import site_config abs_workdir = job . resolve_workdir ( site_config . data_path )","title":"Resolving the Workdir"},{"location":"user-guide/jobs/#accessing-parents","text":"Jobs contain the special method parent_query() which returns a iterable query over the job's parents. For example, we can combine this with resolve_workdir to list the parents' working directories. This pattern can be particularly useful in the preprocessing app hook, where a Job needs to read some data from its parents before executing: from balsam.api import site_config from balsam.api import ApplicationDefinition class MyApp ( ApplicationDefinition ): # ...other class attrs def preprocess ( self ): parent_workdirs = [ j . resolve_workdir ( site_config . data_path ) for j in self . job . parent_query () ]","title":"Accessing Parents"},{"location":"user-guide/jobs/#updating-jobs","text":"In addition to the examples where Jobs were modified and updated by calling save() , we can efficiently apply the same update to all jobs matching a particular query. This can be significantly faster than calling job.save() in a loop, which repeatedly sends small HTTP update requests over the wire. # Run all the Failed Jobs again: Job . objects . filter ( state = \"FAILED\" ) . update ( state = \"RESTART_READY\" )","title":"Updating Jobs"},{"location":"user-guide/jobs/#deleting-jobs","text":"Just as we can apply updates to individual jobs or job sets selected by a query, we can also delete Jobs : # Delete a single job: job . delete () # Delete a collection of Jobs: Job . objects . filter ( state = \"FAILED\" , tags = { \"run\" : \"H2O\" }) . delete ()","title":"Deleting Jobs"},{"location":"user-guide/jobs/#python-app-futures","text":"When using the run() function in your ApplicationDefinitions , you can treat the resultant Job objects like standard Python Futures in a few useful ways.","title":"Python App Futures"},{"location":"user-guide/jobs/#accessing-results","text":"The Job.result(timeout=None) method will block until the job is completed, and return the propagated return value of the run() function. If the function raised an Exception, the Exception is re-raised: job = Adder . submit ( \"test/123\" , x = 3 , y = 7 ) assert job . result () == 3 + 7 result() optionally takes a floating point seconds timeout value. If the Job does not complete within the timeout period, it will raise concurrent.futures.TimeoutError . Similarly, result_nowait() will return the result in a non-blocking fashion and raise Job.NoResult if a result is not immediately available.","title":"Accessing Results"},{"location":"user-guide/jobs/#polling-a-single-job-on-completion","text":"Job.done() polls the API and returns True if the Job is either in the JOB_FINISHED or FAILED state.","title":"Polling a single job on completion"},{"location":"user-guide/jobs/#polling-many-jobs-on-completion","text":"The Job.objects.wait() function takes a list of Jobs and behaves otherwise analogously to concurrent.futures.wait . This can be used to efficiently poll on a large collection of Jobs with a timeout and sort the results by completed/in-progress Jobs: wait_result = Job . objects . wait ( active_jobs , return_when = \"FIRST_COMPLETED\" , timeout = 60 , poll_interval = 10 ) print ( f \" { len ( wait_result . done ) } jobs completed\" ) print ( f \" { len ( wait_result . not_done ) } active jobs\" )","title":"Polling many jobs on completion"},{"location":"user-guide/jobs/#iterating-over-jobs-as-they-complete","text":"The Job.objects.as_completed() function behaves analogously to concurrent.futures.as_completed . The method returns an generator over the input Jobs , which yields Jobs one a time as they are completed. for job in Job . objects . as_completed ( active_jobs , timeout = 60 ): print ( f \"Job { job . workdir } returned: { job . result () } \" )","title":"Iterating over Jobs as they complete"},{"location":"user-guide/monitoring/","text":"Debugging, Monitoring, and Analytics \u00b6 Methods \u00b6 There are a few levels at which we can follow the workflows running in Balsam, diagnose errors, and gain insight into recent progress or efficiency. We can always use the Python API or CLI to perform flexible queries: tracking Job states with balsam job ls tracking BatchJob states with balsam queue ls We can also simply read the files generated at the Balsam site. Job-level standard output and error streams will appear in the job.out file of each Job's working directory. We can also search the Balsam logs for informative messages (e.g. when a Job is launched or an uncaught exception is thrown): reading the output of Jobs in the data/ directory reading pre-/post-processing logs generated by the Balsam site agent ( logs/service*.log ) reading Job execution logs generated by the Balsam launcher ( logs/mpi_mode*.log or logs/serial_mode*.log for the respective job modes) Finally, Balsam provides an EventLog API that we can use to flexibly query Job events (state transitions) and visualize metrics like throughput or utilization within a particular subset of Jobs or BatchJobs: querying recent events with EventLog generating visual reports using balsam.analytics The EventLog API \u00b6 The timestamp of each Job state transition (e.g. PREPROCESSED --> RUNNING ) is recorded in the Balsam EventLog API. We can leverage standard Balsam API queries to obtain a relevant set of Events to answer many interesting questions. Here, we print all the event details for each Job tagged with experiment=\"foo\" : from balsam.api import EventLog for evt in EventLog . objects . filter ( tags = { \"experiment\" : \"foo\" }): print ( evt . job_id ) # Job ID print ( evt . timestamp ) # Time of state change (UTC) print ( evt . from_state ) # From which state the job transitioned print ( evt . to_state ) # To which state print ( evt . data ) # optional payload Example Queries \u00b6 Refer to the EventLog.objects.filter docstrings for a comprehensive listing of the possible query parameters. Here, we list just a few of the most useful queries. Get all EventLogs for a particular set of Job IDs: EventLog . objects . filter ( job_id = [ 123 , 124 , 125 ]) Get EventLogs for all the Jobs that ran in a particular BatchJob : EventLog . objects . filter ( batch_job_id = 123 ) # Using Balsam's intrinsic BatchJob ID (revealed by balsam queue ls --history) EventLog . objects . filter ( scheduler_id = 456 ) # Using the HPC scheduler's own ID (e.g. Cobalt ID) Get EventLogs for all the Jobs having a set of matching tags: EventLog . objects . filter ( tags = { \"experiment\" : \"ffn-1\" , \"scale_nodes\" : \"512\" }) Get EventLogs for a particular state transition: EventLog . objects . filter ( to_state = \"RUNNING\" ) # all Job startup events EventLog . objects . filter ( from_state = \"RESTART_READY\" , to_state = \"RUNNING\" ) # Only *restart* Events Get EventLogs that occured within a certain UTC time range: from datetime import datetime , timedelta yesterday = datetime . utcnow () - timedelta ( days = 1 ) EventLog . objects . filter ( timestamp_after = yesterday ) # Only events more recent than 1 day The Analytics API \u00b6 We can certainly process the EventLog queries above however we want (e.g. to count how many Jobs started in a certain experiment or a certain time interval). However, the balsam.analytics provides convenience methods for the most common Balsam Event processing tasks. In particular, we can use this module to visualize: Throughput: the finished Job count as a function of time Utilization: how many Jobs were running as a function of time Node availability: how many compute nodes were actively running a Balsam launcher as a function of time To perform these analyses, we simply call one of the following methods to generate time-series data: throughput_report(eventlog_query, to_state=\"JOB_FINISHED\") utilization_report(eventlog_query, node_weighting=True) available_nodes(batchjob_query) Each of these methods returns a 2-tuple of (X, Y) data that fits seamlessly within a matplotlib visualization workflow. We show some specific examples below. Throughput \u00b6 In this example, we visualize the Job throughput versus elapsed minutes in one particular launcher run: from balsam.api import EventLog from balsam.analytics import throughput_report from matplotlib import pyplot as plt events = EventLog . objects . filter ( scheduler_id = 123 ) times , done_counts = throughput_report ( events , to_state = \"RUN_DONE\" ) t0 = min ( times ) elapsed_minutes = [( t - t0 ) . total_seconds () / 60 for t in times ] plt . step ( elapsed_minutes , done_counts , where = \"post\" ) Utilization \u00b6 We can look at how many nodes were actively running a Job at any given time using the same EventLog query from above. In this example, keeping the default node_weighting=True kwarg ensures that each Job is weighted by its resource requirements (so that 8 simultaneous jobs with node_packing_count=8 contribute 8/8=1.0 to the overall utilization): from balsam.api import EventLog from balsam.analytics import utilization_report from matplotlib import pyplot as plt events = EventLog . objects . filter ( scheduler_id = 123 ) times , util = utilization_report ( events , node_weighting = True ) t0 = min ( times ) elapsed_minutes = [( t - t0 ) . total_seconds () / 60 for t in times ] plt . step ( elapsed_minutes , util , where = \"post\" ) Available Nodes \u00b6 It's often most interesting to super-impose the utilization with the total number of available compute nodes. For one BatchJob this is really easy: we just plot a horizontal line at the node count. If we are using auto scaling or have several BatchJob allocations overlapping in time, the available node count becomes a more complex step function in time. We can generate this from a BatchJob query in Balsam: from balsam.api import BatchJob from balsam.analytics import available_nodes foo_batchjobs = BatchJob . objects . filter ( filter_tags = { \"experiment\" : \"foo\" } ) times , node_counts = available_nodes ( foo_batchjobs ) By overlaying this timeline with the Job utilization from above, we get an intuitive visual representation of how efficiently Balsam is using the available resources. An example is shown below, where the thick, gray trace shows the available_nodes , while the thin blue trace shows the utilization_report .","title":"Workflow Monitoring and Analytics"},{"location":"user-guide/monitoring/#debugging-monitoring-and-analytics","text":"","title":"Debugging, Monitoring, and Analytics"},{"location":"user-guide/monitoring/#methods","text":"There are a few levels at which we can follow the workflows running in Balsam, diagnose errors, and gain insight into recent progress or efficiency. We can always use the Python API or CLI to perform flexible queries: tracking Job states with balsam job ls tracking BatchJob states with balsam queue ls We can also simply read the files generated at the Balsam site. Job-level standard output and error streams will appear in the job.out file of each Job's working directory. We can also search the Balsam logs for informative messages (e.g. when a Job is launched or an uncaught exception is thrown): reading the output of Jobs in the data/ directory reading pre-/post-processing logs generated by the Balsam site agent ( logs/service*.log ) reading Job execution logs generated by the Balsam launcher ( logs/mpi_mode*.log or logs/serial_mode*.log for the respective job modes) Finally, Balsam provides an EventLog API that we can use to flexibly query Job events (state transitions) and visualize metrics like throughput or utilization within a particular subset of Jobs or BatchJobs: querying recent events with EventLog generating visual reports using balsam.analytics","title":"Methods"},{"location":"user-guide/monitoring/#the-eventlog-api","text":"The timestamp of each Job state transition (e.g. PREPROCESSED --> RUNNING ) is recorded in the Balsam EventLog API. We can leverage standard Balsam API queries to obtain a relevant set of Events to answer many interesting questions. Here, we print all the event details for each Job tagged with experiment=\"foo\" : from balsam.api import EventLog for evt in EventLog . objects . filter ( tags = { \"experiment\" : \"foo\" }): print ( evt . job_id ) # Job ID print ( evt . timestamp ) # Time of state change (UTC) print ( evt . from_state ) # From which state the job transitioned print ( evt . to_state ) # To which state print ( evt . data ) # optional payload","title":"The EventLog API"},{"location":"user-guide/monitoring/#example-queries","text":"Refer to the EventLog.objects.filter docstrings for a comprehensive listing of the possible query parameters. Here, we list just a few of the most useful queries. Get all EventLogs for a particular set of Job IDs: EventLog . objects . filter ( job_id = [ 123 , 124 , 125 ]) Get EventLogs for all the Jobs that ran in a particular BatchJob : EventLog . objects . filter ( batch_job_id = 123 ) # Using Balsam's intrinsic BatchJob ID (revealed by balsam queue ls --history) EventLog . objects . filter ( scheduler_id = 456 ) # Using the HPC scheduler's own ID (e.g. Cobalt ID) Get EventLogs for all the Jobs having a set of matching tags: EventLog . objects . filter ( tags = { \"experiment\" : \"ffn-1\" , \"scale_nodes\" : \"512\" }) Get EventLogs for a particular state transition: EventLog . objects . filter ( to_state = \"RUNNING\" ) # all Job startup events EventLog . objects . filter ( from_state = \"RESTART_READY\" , to_state = \"RUNNING\" ) # Only *restart* Events Get EventLogs that occured within a certain UTC time range: from datetime import datetime , timedelta yesterday = datetime . utcnow () - timedelta ( days = 1 ) EventLog . objects . filter ( timestamp_after = yesterday ) # Only events more recent than 1 day","title":"Example Queries"},{"location":"user-guide/monitoring/#the-analytics-api","text":"We can certainly process the EventLog queries above however we want (e.g. to count how many Jobs started in a certain experiment or a certain time interval). However, the balsam.analytics provides convenience methods for the most common Balsam Event processing tasks. In particular, we can use this module to visualize: Throughput: the finished Job count as a function of time Utilization: how many Jobs were running as a function of time Node availability: how many compute nodes were actively running a Balsam launcher as a function of time To perform these analyses, we simply call one of the following methods to generate time-series data: throughput_report(eventlog_query, to_state=\"JOB_FINISHED\") utilization_report(eventlog_query, node_weighting=True) available_nodes(batchjob_query) Each of these methods returns a 2-tuple of (X, Y) data that fits seamlessly within a matplotlib visualization workflow. We show some specific examples below.","title":"The Analytics API"},{"location":"user-guide/monitoring/#throughput","text":"In this example, we visualize the Job throughput versus elapsed minutes in one particular launcher run: from balsam.api import EventLog from balsam.analytics import throughput_report from matplotlib import pyplot as plt events = EventLog . objects . filter ( scheduler_id = 123 ) times , done_counts = throughput_report ( events , to_state = \"RUN_DONE\" ) t0 = min ( times ) elapsed_minutes = [( t - t0 ) . total_seconds () / 60 for t in times ] plt . step ( elapsed_minutes , done_counts , where = \"post\" )","title":"Throughput"},{"location":"user-guide/monitoring/#utilization","text":"We can look at how many nodes were actively running a Job at any given time using the same EventLog query from above. In this example, keeping the default node_weighting=True kwarg ensures that each Job is weighted by its resource requirements (so that 8 simultaneous jobs with node_packing_count=8 contribute 8/8=1.0 to the overall utilization): from balsam.api import EventLog from balsam.analytics import utilization_report from matplotlib import pyplot as plt events = EventLog . objects . filter ( scheduler_id = 123 ) times , util = utilization_report ( events , node_weighting = True ) t0 = min ( times ) elapsed_minutes = [( t - t0 ) . total_seconds () / 60 for t in times ] plt . step ( elapsed_minutes , util , where = \"post\" )","title":"Utilization"},{"location":"user-guide/monitoring/#available-nodes","text":"It's often most interesting to super-impose the utilization with the total number of available compute nodes. For one BatchJob this is really easy: we just plot a horizontal line at the node count. If we are using auto scaling or have several BatchJob allocations overlapping in time, the available node count becomes a more complex step function in time. We can generate this from a BatchJob query in Balsam: from balsam.api import BatchJob from balsam.analytics import available_nodes foo_batchjobs = BatchJob . objects . filter ( filter_tags = { \"experiment\" : \"foo\" } ) times , node_counts = available_nodes ( foo_batchjobs ) By overlaying this timeline with the Job utilization from above, we get an intuitive visual representation of how efficiently Balsam is using the available resources. An example is shown below, where the thick, gray trace shows the available_nodes , while the thin blue trace shows the utilization_report .","title":"Available Nodes"},{"location":"user-guide/site-config/","text":"Balsam Sites \u00b6 Sites have a single owner \u00b6 All Balsam workflows are namespaced under Sites , which are self-contained project directories managed by an autonomous user agent. There is no concept of sharing Balsam Sites with other users: each Site has exactly one owner , who is the sole user able to see the Site. Therefore, to create a new Site, you must be authenticated with Balsam: $ balsam login Public logins temporarily are restricted The central Balsam service is currently in a pre-release phase and login is limited to pre-authorized ALCF users. For early access to Balsam, please send a request to the ALCF Help Desk . Creating a site \u00b6 To initialize a Balsam site, use the CLI to select an appropriate default configuration for the current system. Balsam creates a new Site directory and registers it with the REST API. In order to use the Site, we must also start the agent process with balsam site start . $ balsam site init SITE-PATH $ cd SITE-PATH $ balsam site start The Site is populated with several folders and a bootstrapped configuration file settings.yml . The name that you choose for the Site must be unique across all of your Sites. This name is used to identify and target Jobs to specific Sites. You may need to restart the Site when the system is rebooted or otherwise goes down for maintenance. You can always stop and restart the Site yourself: $ balsam site stop $ balsam site start The Site directory \u00b6 Each Balsam site has a regular structure comprising certain files and directories: data/ : Each Balsam Job runs in a subdirectory of data/ . The Job working directories are specified relative to this folder via job.workdir . log/ : The Site user agent and launcher pilot jobs send diagnostic messages here. Checking the logs is a great way to track exactly what is happening or what went wrong. qsubmit/ : Balsam runs your apps inside of launchers which are submitted to the HPC batch scheduler via a shell script. This directory contains each of the materialized scripts that was actually submitted to the batch queue. job-template.sh : This is the template for the qsubmit/ scripts submitted to the HPC batch scheduler. settings.yml : This is where the Balsam Site is configured. The file is populated with sensible defaults for the chosen platform, and it is commented for you to read and modify. Customizing the Job Template \u00b6 You can adapt the job-template.sh file to add custom scheduler flags, or to run code on your allocation before the pilot job starts. Some examples of job template customization include: Adding scheduler-specific directives to the header of the shell script Configuring hardware (e.g. setting up MIG mode on GPUs) Staging data by copying files to node-local SSD's Loading modules or exporting global environment variables Feel free to add logic to the job-template.sh file as needed. You can also maintain templates with different filenames, and point to the currently active job template by changing the job_template_path parameter in settings.yml . Keep Template Variables Intact Any values enclosed in double-curly braces ( {{launcher_cmd}} ) are variables provided to the template from Balsam. Be sure to leave these names intact when modifying the template! Whenever you change the job template or Site settings file, it is necessary to reload the Site if it's already running: $ balsam site sync Warning The job-template.sh and settings.yml are first loaded when the Site starts and stay in memory! Therefore, any changes will not apply until you stop and restart the Site, using balsam site stop and balsam site start or balsam site sync . Optional Template Variables \u00b6 Because templates are generated with Jinja2 , you can write custom templates leveraging variables, if-statements, for-loops, etc... referring to the Jinja2 template syntax. You can define optional parameters exposed to the template by using the optional_batch_job_params key of settings.yml . This expects a dictionary mapping parameter names to default string values . These \"pass-through\" parameters can then be provided as extras on the command line via -x/--extra-param flags. For instance, the ALCF-Theta job template supports a singularity_prime_cache option that allows you to enable this feature using -x singularity_prime_cache=yes on the balsam queue submit command. Here's how that option is implemented: # In settings.yml scheduler : optional_batch_job_params : singularity_prime_cache : 'no' # In job-template.sh { % if optional_params.get ( \"singularity_prime_cache\" ) == 'yes' % } # Prime LDAP for large-scale Singularity runs aprun -N 1 -n $COBALT_JOBSIZE /soft/tools/prime-cache sleep 10 { % endif % } Users of ThetaGPU at the ALCF can similarly leverage the MIG partitioning capability of A100 GPUs by passing the optional parameter -x mig_count=N (three possible split values of N equal to 2 , 3 , and 7 are currently supported). Customizing the Settings \u00b6 There are numerous adjustable parameters in settings.yml that control how the Site runs and processes your workflows. The default values are designed to work on the chosen platform as-is , but users are encouraged to read the comments and modify settings.yml to suit their own needs. Note Be sure to run balsam site sync to apply any changes to the Site agent! We highlight just a few of the important settings you may want to adjust: logging.level : Change the verbosity to get more or less diagnostics from Balsam in your log/ directory. launcher.idle_ttl_sec : controls how long the pilot job should stay alive before quitting when nothing is running. You might turn this up if you are debugging and want to hold on to resources. scheduler.allowed_projects : lists the projects/allocations that the Site may submit to. You need to update this to manage what allocations the Site may use. scheduler.allowed_queues : defines the queueing policies per-queue name. If a special reservation or partition is created for your project or a workshop, you will need to define that here. processing.num_workers : controls the number of simultaneous processes handling your Jobs' pre/post-processing workload. If I/O-intensive preprocessing is a bottleneck, you can turn this value up. transfers.transfer_locations : lets Balsam know about remote Globus endpoints that the Site may stage data in/out from elastic_queue : controls automated queue submissions by defining the granularity and flexibility of resource requests. This is disabled by default and must be configured on an as-needed basis (see the Auto Scaling page for more information). The Site CLI \u00b6 Starting, Stopping, and Restarting Sites \u00b6 In order for workflows to actually run at a Site, the agent must be started as a background process on a login (or gateway) node. # To start and stop the Site agent: $ balsam site start $ balsam site stop # Restart the Site agent and push settings changes to the API: $ balsam site sync Site Agent Resources The Balsam site agent runs as persistent daemon. It can be started on any node with Internet access, access to the parallel filesystems, and access to the HPC resource manager. This is typically a \"login\" or \"gateway\" node in a multi-user environment. The site agent runs a collection of plug-in modules responsible for various facets of the workflow. These plug-ins can be configured, enabled, and disabled in the settings.yml file by adjusting data under these keys: scheduler : interfaces with the HPC resource manager to submit and query batch resource allocations processing : runs pre- and post- job execution lifecycle hooks to advance the workflow. transfers : manages batch transfer tasks for stage in and stage out of job data elastic_queue : automates batch job submissions to auto-scale resources to the runnable backlog file_cleaner : clears unused data from working directories of finished jobs Launchers will still run if the agent is stopped Once submitted to the HPC queue, the Balsam launchers (pilot jobs) operate on the compute nodes independently of the Site agent. As long as they have a valid access token, they will work regardless of whether the Site is running. However, you typically want the Site to continue running so that new Jobs can be preprocessed for execution. Listing Sites \u00b6 The CLI is useful to get a quick listing of all the Sites you own: # Summary of Sites: $ balsam site ls The Active column indicates whether Sites have recently communicated with the API and are most likely up and running. You can obtain much more detailed Site information (such as a listing of currently idle backfill windows at each Site) by including the -v/--verbose flag: # Detailed Site data: $ balsam site ls -v Moving or Deleting Sites \u00b6 To permanently delete a Site and all associated workflows inside: $ balsam site rm SITE-PATH To rename a site or move it to a new directory: $ balsam site mv SITE-PATH DESTINATION Why is there a delay in queue submission? \u00b6 Any Balsam CLI or Python API interaction (like running balsam queue submit ), does not affect the HPC system directly. In fact, you're only manipulating resources in the REST API (creating a BatchJob ). Eventually, the Site agent that runs in the background fetches state from the backend and performs local actions (run sbatch or qsub ) to synchronize your Site with the central state. This decoupled design, with all control flow routed through the central REST API, makes Balsam interactions completely uniform, whether you're running commands locally or remotely. For instance, you can provision resources on a remote supercomputer simply by specifying the --site on the command line: $ balsam queue submit --site = my-site -n 1 -t 15 -q debug -A Project -j mpi sequenceDiagram User->>API: `balsam queue submit` API-->>User: OK Site->>API: Have any new BatchJobs for me? API-->>Site: Yes, here is one Site->>Cobalt: `qsub` this BatchJob Cobalt-->>Site: New Cobalt Job ID Site->>API: Update BatchJob with Cobalt Job ID API-->>Site: OK User->>API: `balsam queue ls` API-->>User: Updated BatchJob","title":"Creating Balsam Sites"},{"location":"user-guide/site-config/#balsam-sites","text":"","title":"Balsam Sites"},{"location":"user-guide/site-config/#sites-have-a-single-owner","text":"All Balsam workflows are namespaced under Sites , which are self-contained project directories managed by an autonomous user agent. There is no concept of sharing Balsam Sites with other users: each Site has exactly one owner , who is the sole user able to see the Site. Therefore, to create a new Site, you must be authenticated with Balsam: $ balsam login Public logins temporarily are restricted The central Balsam service is currently in a pre-release phase and login is limited to pre-authorized ALCF users. For early access to Balsam, please send a request to the ALCF Help Desk .","title":"Sites have a single owner"},{"location":"user-guide/site-config/#creating-a-site","text":"To initialize a Balsam site, use the CLI to select an appropriate default configuration for the current system. Balsam creates a new Site directory and registers it with the REST API. In order to use the Site, we must also start the agent process with balsam site start . $ balsam site init SITE-PATH $ cd SITE-PATH $ balsam site start The Site is populated with several folders and a bootstrapped configuration file settings.yml . The name that you choose for the Site must be unique across all of your Sites. This name is used to identify and target Jobs to specific Sites. You may need to restart the Site when the system is rebooted or otherwise goes down for maintenance. You can always stop and restart the Site yourself: $ balsam site stop $ balsam site start","title":"Creating a site"},{"location":"user-guide/site-config/#the-site-directory","text":"Each Balsam site has a regular structure comprising certain files and directories: data/ : Each Balsam Job runs in a subdirectory of data/ . The Job working directories are specified relative to this folder via job.workdir . log/ : The Site user agent and launcher pilot jobs send diagnostic messages here. Checking the logs is a great way to track exactly what is happening or what went wrong. qsubmit/ : Balsam runs your apps inside of launchers which are submitted to the HPC batch scheduler via a shell script. This directory contains each of the materialized scripts that was actually submitted to the batch queue. job-template.sh : This is the template for the qsubmit/ scripts submitted to the HPC batch scheduler. settings.yml : This is where the Balsam Site is configured. The file is populated with sensible defaults for the chosen platform, and it is commented for you to read and modify.","title":"The Site directory"},{"location":"user-guide/site-config/#customizing-the-job-template","text":"You can adapt the job-template.sh file to add custom scheduler flags, or to run code on your allocation before the pilot job starts. Some examples of job template customization include: Adding scheduler-specific directives to the header of the shell script Configuring hardware (e.g. setting up MIG mode on GPUs) Staging data by copying files to node-local SSD's Loading modules or exporting global environment variables Feel free to add logic to the job-template.sh file as needed. You can also maintain templates with different filenames, and point to the currently active job template by changing the job_template_path parameter in settings.yml . Keep Template Variables Intact Any values enclosed in double-curly braces ( {{launcher_cmd}} ) are variables provided to the template from Balsam. Be sure to leave these names intact when modifying the template! Whenever you change the job template or Site settings file, it is necessary to reload the Site if it's already running: $ balsam site sync Warning The job-template.sh and settings.yml are first loaded when the Site starts and stay in memory! Therefore, any changes will not apply until you stop and restart the Site, using balsam site stop and balsam site start or balsam site sync .","title":"Customizing the Job Template"},{"location":"user-guide/site-config/#optional-template-variables","text":"Because templates are generated with Jinja2 , you can write custom templates leveraging variables, if-statements, for-loops, etc... referring to the Jinja2 template syntax. You can define optional parameters exposed to the template by using the optional_batch_job_params key of settings.yml . This expects a dictionary mapping parameter names to default string values . These \"pass-through\" parameters can then be provided as extras on the command line via -x/--extra-param flags. For instance, the ALCF-Theta job template supports a singularity_prime_cache option that allows you to enable this feature using -x singularity_prime_cache=yes on the balsam queue submit command. Here's how that option is implemented: # In settings.yml scheduler : optional_batch_job_params : singularity_prime_cache : 'no' # In job-template.sh { % if optional_params.get ( \"singularity_prime_cache\" ) == 'yes' % } # Prime LDAP for large-scale Singularity runs aprun -N 1 -n $COBALT_JOBSIZE /soft/tools/prime-cache sleep 10 { % endif % } Users of ThetaGPU at the ALCF can similarly leverage the MIG partitioning capability of A100 GPUs by passing the optional parameter -x mig_count=N (three possible split values of N equal to 2 , 3 , and 7 are currently supported).","title":"Optional Template Variables"},{"location":"user-guide/site-config/#customizing-the-settings","text":"There are numerous adjustable parameters in settings.yml that control how the Site runs and processes your workflows. The default values are designed to work on the chosen platform as-is , but users are encouraged to read the comments and modify settings.yml to suit their own needs. Note Be sure to run balsam site sync to apply any changes to the Site agent! We highlight just a few of the important settings you may want to adjust: logging.level : Change the verbosity to get more or less diagnostics from Balsam in your log/ directory. launcher.idle_ttl_sec : controls how long the pilot job should stay alive before quitting when nothing is running. You might turn this up if you are debugging and want to hold on to resources. scheduler.allowed_projects : lists the projects/allocations that the Site may submit to. You need to update this to manage what allocations the Site may use. scheduler.allowed_queues : defines the queueing policies per-queue name. If a special reservation or partition is created for your project or a workshop, you will need to define that here. processing.num_workers : controls the number of simultaneous processes handling your Jobs' pre/post-processing workload. If I/O-intensive preprocessing is a bottleneck, you can turn this value up. transfers.transfer_locations : lets Balsam know about remote Globus endpoints that the Site may stage data in/out from elastic_queue : controls automated queue submissions by defining the granularity and flexibility of resource requests. This is disabled by default and must be configured on an as-needed basis (see the Auto Scaling page for more information).","title":"Customizing the Settings"},{"location":"user-guide/site-config/#the-site-cli","text":"","title":"The Site CLI"},{"location":"user-guide/site-config/#starting-stopping-and-restarting-sites","text":"In order for workflows to actually run at a Site, the agent must be started as a background process on a login (or gateway) node. # To start and stop the Site agent: $ balsam site start $ balsam site stop # Restart the Site agent and push settings changes to the API: $ balsam site sync Site Agent Resources The Balsam site agent runs as persistent daemon. It can be started on any node with Internet access, access to the parallel filesystems, and access to the HPC resource manager. This is typically a \"login\" or \"gateway\" node in a multi-user environment. The site agent runs a collection of plug-in modules responsible for various facets of the workflow. These plug-ins can be configured, enabled, and disabled in the settings.yml file by adjusting data under these keys: scheduler : interfaces with the HPC resource manager to submit and query batch resource allocations processing : runs pre- and post- job execution lifecycle hooks to advance the workflow. transfers : manages batch transfer tasks for stage in and stage out of job data elastic_queue : automates batch job submissions to auto-scale resources to the runnable backlog file_cleaner : clears unused data from working directories of finished jobs Launchers will still run if the agent is stopped Once submitted to the HPC queue, the Balsam launchers (pilot jobs) operate on the compute nodes independently of the Site agent. As long as they have a valid access token, they will work regardless of whether the Site is running. However, you typically want the Site to continue running so that new Jobs can be preprocessed for execution.","title":"Starting, Stopping, and Restarting Sites"},{"location":"user-guide/site-config/#listing-sites","text":"The CLI is useful to get a quick listing of all the Sites you own: # Summary of Sites: $ balsam site ls The Active column indicates whether Sites have recently communicated with the API and are most likely up and running. You can obtain much more detailed Site information (such as a listing of currently idle backfill windows at each Site) by including the -v/--verbose flag: # Detailed Site data: $ balsam site ls -v","title":"Listing Sites"},{"location":"user-guide/site-config/#moving-or-deleting-sites","text":"To permanently delete a Site and all associated workflows inside: $ balsam site rm SITE-PATH To rename a site or move it to a new directory: $ balsam site mv SITE-PATH DESTINATION","title":"Moving or Deleting Sites"},{"location":"user-guide/site-config/#why-is-there-a-delay-in-queue-submission","text":"Any Balsam CLI or Python API interaction (like running balsam queue submit ), does not affect the HPC system directly. In fact, you're only manipulating resources in the REST API (creating a BatchJob ). Eventually, the Site agent that runs in the background fetches state from the backend and performs local actions (run sbatch or qsub ) to synchronize your Site with the central state. This decoupled design, with all control flow routed through the central REST API, makes Balsam interactions completely uniform, whether you're running commands locally or remotely. For instance, you can provision resources on a remote supercomputer simply by specifying the --site on the command line: $ balsam queue submit --site = my-site -n 1 -t 15 -q debug -A Project -j mpi sequenceDiagram User->>API: `balsam queue submit` API-->>User: OK Site->>API: Have any new BatchJobs for me? API-->>Site: Yes, here is one Site->>Cobalt: `qsub` this BatchJob Cobalt-->>Site: New Cobalt Job ID Site->>API: Update BatchJob with Cobalt Job ID API-->>Site: OK User->>API: `balsam queue ls` API-->>User: Updated BatchJob","title":"Why is there a delay in queue submission?"},{"location":"user-guide/transfer/","text":"Data Transfers \u00b6 Background \u00b6 Each Balsam Job may require data to be staged in prior to execution or staged out after execution. A core feature of Balsam is to interface with services such as Globus Transfer and automatically submit and monitor batched transfer tasks between endpoints. This enables distributed workflows where large numbers of Jobs with relatively small datasets are submitted in real-time : the Site manages the details of efficient batch transfers and marks individual jobs as STAGED_IN as the requisite data arrives. To use this functionality, the first step is to define the Transfer Slots for a given Balsam App. We can then submit Jobs with transfer items that fill the required transfer slots. Be sure to read these two sections in the user guide for more information. The only other requirement is to configure the transfer plugin at the Balsam Site and authenticate with Globus, which we explain below. Configuring Transfers \u00b6 When using the Globus transfer interface, Balsam needs an access token to communicate with the Globus Transfer API. You may already have an access token stored from a Globus CLI installation on your machine: check globus whoami to see if this is the case. Otherwise, Balsam ships with the necessary tooling and you can follow the same Globus authentication flow by running: $ balsam site globus-login Next, we configure the transfers section of settings.yml : transfer_locations should be set to a dictionary of trusted location aliases . If you need to add Globus endpoints, they can be inserted here. globus_endpoint_id should refer to the endpoint ID of the local Site. On public subscriber endpoints at large HPC facilities, this value will be set correctly in the default Balsam site configuration. max_concurrent_transfers determines the maximum number of in-flight transfer tasks, where each task manages a batch of files for many Jobs. transfer_batch_size determines the maximum number of transfer items per transfer task. This should be tuned depending on your workload (a higher number makes sense to utilize available bandwidth for smaller files). num_items_query_limit determines the maximum number of transfer items considered in any single transfer task submission. service_period determines the interval (in seconds) between transfer task submissions. Once settings.yml has been configured appropriately, be sure to restart the Balsam Site: $ balsam site sync The Site will start issuing stage in and stage out tasks immediately and advancing Jobs as needed. We can track the state of transfers using the Python API : from balsam.api import TransferItem for item in TransferItem . objects . filter ( direction = \"in\" , state = \"active\" ): print ( f \"File { item . remote_path } is currently staging in via task ID: { item . task_id } \" )","title":"Data Transfers"},{"location":"user-guide/transfer/#data-transfers","text":"","title":"Data Transfers"},{"location":"user-guide/transfer/#background","text":"Each Balsam Job may require data to be staged in prior to execution or staged out after execution. A core feature of Balsam is to interface with services such as Globus Transfer and automatically submit and monitor batched transfer tasks between endpoints. This enables distributed workflows where large numbers of Jobs with relatively small datasets are submitted in real-time : the Site manages the details of efficient batch transfers and marks individual jobs as STAGED_IN as the requisite data arrives. To use this functionality, the first step is to define the Transfer Slots for a given Balsam App. We can then submit Jobs with transfer items that fill the required transfer slots. Be sure to read these two sections in the user guide for more information. The only other requirement is to configure the transfer plugin at the Balsam Site and authenticate with Globus, which we explain below.","title":"Background"},{"location":"user-guide/transfer/#configuring-transfers","text":"When using the Globus transfer interface, Balsam needs an access token to communicate with the Globus Transfer API. You may already have an access token stored from a Globus CLI installation on your machine: check globus whoami to see if this is the case. Otherwise, Balsam ships with the necessary tooling and you can follow the same Globus authentication flow by running: $ balsam site globus-login Next, we configure the transfers section of settings.yml : transfer_locations should be set to a dictionary of trusted location aliases . If you need to add Globus endpoints, they can be inserted here. globus_endpoint_id should refer to the endpoint ID of the local Site. On public subscriber endpoints at large HPC facilities, this value will be set correctly in the default Balsam site configuration. max_concurrent_transfers determines the maximum number of in-flight transfer tasks, where each task manages a batch of files for many Jobs. transfer_batch_size determines the maximum number of transfer items per transfer task. This should be tuned depending on your workload (a higher number makes sense to utilize available bandwidth for smaller files). num_items_query_limit determines the maximum number of transfer items considered in any single transfer task submission. service_period determines the interval (in seconds) between transfer task submissions. Once settings.yml has been configured appropriately, be sure to restart the Balsam Site: $ balsam site sync The Site will start issuing stage in and stage out tasks immediately and advancing Jobs as needed. We can track the state of transfers using the Python API : from balsam.api import TransferItem for item in TransferItem . objects . filter ( direction = \"in\" , state = \"active\" ): print ( f \"File { item . remote_path } is currently staging in via task ID: { item . task_id } \" )","title":"Configuring Transfers"}]}